{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2YxgoRvcGk7T"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras import backend\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import math\n",
        "from math import log\n",
        "import numpy as np \n",
        "from numpy.linalg import norm \n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "from itertools import product\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8NeeWD-G5rP"
      },
      "source": [
        "Generate a Cantor Set, defined as for set $A_i$, $x=\\sum\\limits_{k=1}^\\infty{a_\\text{k}*(2^n)^\\text{-k}}$  where $a_\\text{k}\\in \\{ 0,2^\\text{i-1} \\}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcUxHginG3wQ",
        "outputId": "0c60fd58-ca06-4e07-c2fa-7acf2282b27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0078125, 0.03125, 0.0390625, 0.125, 0.1328125, 0.15625, 0.1640625, 0.5, 0.5078125, 0.53125, 0.5390625, 0.625, 0.6328125, 0.65625, 0.6640625]\n"
          ]
        }
      ],
      "source": [
        "x=[]\n",
        "kgap=4  # determines how many time the iteration will go, will generate 2^kgap numbers of points\n",
        "def xinAi(k,coor,power,dimension):  # for the case x in Ai, dimension is the dim of the cantor set, i=dimension, power starts from 1\n",
        "    for an in [0, 2**(dimension-1)]:\n",
        "        coor = coor + an*((2**dimension)**(-power))\n",
        "\n",
        "        if power < k:\n",
        "            xinAi(k, coor, power + 1, dimension)\n",
        "\n",
        "        else :\n",
        "            x.append(coor)\n",
        "\n",
        "xinAi(kgap,0,1,2)\n",
        "\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNMk8kO2OPR1"
      },
      "source": [
        "Several possible learning tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Dw7gwpPqOOyx"
      },
      "outputs": [],
      "source": [
        "def Distencefunction(HighDimVar):  \n",
        "  HighDimVar=np.array(HighDimVar)\n",
        "  sum=0\n",
        "  #i,j = product\n",
        "  #return math.sqrt(i*i+j*j)\n",
        "  for i in range(len(HighDimVar)):\n",
        "    sum = sum + HighDimVar[i]**2\n",
        "  return math.sqrt(sum)\n",
        "\n",
        "\n",
        "def SqrtAllComp(HighDimVar):    \n",
        "  HighDimVar=np.array(HighDimVar)\n",
        "  product=1\n",
        "\n",
        "  for i in range(len(HighDimVar)):\n",
        "    product=product * HighDimVar[i]\n",
        "  \n",
        "  return math.sqrt(product)\n",
        "\n",
        "def IdentitySum(HighDimVar):\n",
        "  HighDimVar = np.array(HighDimVar)\n",
        "  sum = 0\n",
        "  for i in range(len(HighDimVar)):\n",
        "      sum = sum + HighDimVar[i]\n",
        "  return sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Fd2xwpNd2s"
      },
      "source": [
        "n Cantor sets (named as $A_1, A_2, A_3, A_4, A_5, ... A_n$ ) on [0,1] can be constructed so that their Cartesian product is reducible to a 1 dimensional set if $A_1$ only contains numbers with the digits 0 or 1, $A_2$ only contains numbers with the digits 0 or 2, $A_3$ only contains numbers with the digits 0 or 4,..... $A_n$ only contains numbers with the digits 0 or $2^\\text{n-1}$ in base $2^n$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0eeXEOyNfSk",
        "outputId": "b548eb2b-7b5e-4185-cda8-bed28a2d6def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.0078125, 0.03125, 0.0390625, 0.125, 0.1328125, 0.15625, 0.1640625, 0.5, 0.5078125, 0.53125, 0.5390625, 0.625, 0.6328125, 0.65625, 0.6640625], [0.0, 0.0009765625, 0.0078125, 0.0087890625, 0.0625, 0.0634765625, 0.0703125, 0.0712890625, 0.5, 0.5009765625, 0.5078125, 0.5087890625, 0.5625, 0.5634765625, 0.5703125, 0.5712890625], [0.0, 0.0001220703125, 0.001953125, 0.0020751953125, 0.03125, 0.0313720703125, 0.033203125, 0.0333251953125, 0.5, 0.5001220703125, 0.501953125, 0.5020751953125, 0.53125, 0.5313720703125, 0.533203125, 0.5333251953125], [0.0, 1.52587890625e-05, 0.00048828125, 0.0005035400390625, 0.015625, 0.0156402587890625, 0.01611328125, 0.0161285400390625, 0.5, 0.5000152587890625, 0.50048828125, 0.5005035400390625, 0.515625, 0.5156402587890625, 0.51611328125, 0.5161285400390625]]\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "x=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def HighDimCantorSet(CantorDimension):\n",
        "  for i in range(2,CantorDimension+2):\n",
        "    \n",
        "    xinAi(kgap,0,1,i)\n",
        "    xcopy=x.copy()\n",
        "    #print(x)\n",
        "    CantorList.append(xcopy)\n",
        "\n",
        "    x.clear()\n",
        "    \n",
        "\n",
        "HighDimCantorSet(4)\n",
        "\n",
        "print(CantorList)\n",
        "\n",
        "\n",
        "CantorList=[]\n",
        "def HighDim(NumofCarProdut,function):  #NumofCarProdut, it makes sense for training purpose for a value larger than 2\n",
        "\n",
        "  #CantorList.clear()\n",
        "\n",
        "  #CantorListCopy=CantorList.copy()\n",
        "  HighDimCantorSet(NumofCarProdut)\n",
        "  CarProdut=list(itertools.product(*CantorList))\n",
        "\n",
        "  #CantorList.clear()\n",
        "  \n",
        "  for i in range(len(CarProdut)):    # HighDimList list of high dim points on the cantor set x1,x2,x3,....xn in R^n\n",
        "      HighDimList.append(CarProdut[i])\n",
        "      #print(CarProdut)\n",
        "\n",
        "\n",
        "  for i in range(len(CarProdut)):    # sum of each point's coordinate and return a one dim list\n",
        "        #print(sum(CarProdut[i]))\n",
        "        oneDimlist.append(sum(CarProdut[i]))\n",
        "  \n",
        "  for i in range(len(HighDimList)):   # the function which input is each point in the HighDimlist, the function we will test is identity sum, distance and sqrt\n",
        "      outputlist.append(function(HighDimList[i]))\n",
        "\n",
        "HighDim(3,IdentitySum) #two layer sin(a/100)/100 sin(a/1)/100\n",
        "\n",
        "#print(HighDimList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acd0BcoqW6sn"
      },
      "source": [
        "# Higher Dim Case(>=2)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baBkOTjHbo-Z"
      },
      "source": [
        " ##  x,y to mutivariate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK5ciEr5W7mH",
        "outputId": "e1344a71-662a-4cac-8b8c-d8501c174ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.0, 0.0), (0.0, 0.0009765625), (0.0, 0.0078125), (0.0, 0.0087890625), (0.0, 0.0625), (0.0, 0.0634765625), (0.0, 0.0703125), (0.0, 0.0712890625), (0.0, 0.5), (0.0, 0.5009765625), (0.0, 0.5078125), (0.0, 0.5087890625), (0.0, 0.5625), (0.0, 0.5634765625), (0.0, 0.5703125), (0.0, 0.5712890625), (0.0078125, 0.0), (0.0078125, 0.0009765625), (0.0078125, 0.0078125), (0.0078125, 0.0087890625), (0.0078125, 0.0625), (0.0078125, 0.0634765625), (0.0078125, 0.0703125), (0.0078125, 0.0712890625), (0.0078125, 0.5), (0.0078125, 0.5009765625), (0.0078125, 0.5078125), (0.0078125, 0.5087890625), (0.0078125, 0.5625), (0.0078125, 0.5634765625), (0.0078125, 0.5703125), (0.0078125, 0.5712890625), (0.03125, 0.0), (0.03125, 0.0009765625), (0.03125, 0.0078125), (0.03125, 0.0087890625), (0.03125, 0.0625), (0.03125, 0.0634765625), (0.03125, 0.0703125), (0.03125, 0.0712890625), (0.03125, 0.5), (0.03125, 0.5009765625), (0.03125, 0.5078125), (0.03125, 0.5087890625), (0.03125, 0.5625), (0.03125, 0.5634765625), (0.03125, 0.5703125), (0.03125, 0.5712890625), (0.0390625, 0.0), (0.0390625, 0.0009765625), (0.0390625, 0.0078125), (0.0390625, 0.0087890625), (0.0390625, 0.0625), (0.0390625, 0.0634765625), (0.0390625, 0.0703125), (0.0390625, 0.0712890625), (0.0390625, 0.5), (0.0390625, 0.5009765625), (0.0390625, 0.5078125), (0.0390625, 0.5087890625), (0.0390625, 0.5625), (0.0390625, 0.5634765625), (0.0390625, 0.5703125), (0.0390625, 0.5712890625), (0.125, 0.0), (0.125, 0.0009765625), (0.125, 0.0078125), (0.125, 0.0087890625), (0.125, 0.0625), (0.125, 0.0634765625), (0.125, 0.0703125), (0.125, 0.0712890625), (0.125, 0.5), (0.125, 0.5009765625), (0.125, 0.5078125), (0.125, 0.5087890625), (0.125, 0.5625), (0.125, 0.5634765625), (0.125, 0.5703125), (0.125, 0.5712890625), (0.1328125, 0.0), (0.1328125, 0.0009765625), (0.1328125, 0.0078125), (0.1328125, 0.0087890625), (0.1328125, 0.0625), (0.1328125, 0.0634765625), (0.1328125, 0.0703125), (0.1328125, 0.0712890625), (0.1328125, 0.5), (0.1328125, 0.5009765625), (0.1328125, 0.5078125), (0.1328125, 0.5087890625), (0.1328125, 0.5625), (0.1328125, 0.5634765625), (0.1328125, 0.5703125), (0.1328125, 0.5712890625), (0.15625, 0.0), (0.15625, 0.0009765625), (0.15625, 0.0078125), (0.15625, 0.0087890625), (0.15625, 0.0625), (0.15625, 0.0634765625), (0.15625, 0.0703125), (0.15625, 0.0712890625), (0.15625, 0.5), (0.15625, 0.5009765625), (0.15625, 0.5078125), (0.15625, 0.5087890625), (0.15625, 0.5625), (0.15625, 0.5634765625), (0.15625, 0.5703125), (0.15625, 0.5712890625), (0.1640625, 0.0), (0.1640625, 0.0009765625), (0.1640625, 0.0078125), (0.1640625, 0.0087890625), (0.1640625, 0.0625), (0.1640625, 0.0634765625), (0.1640625, 0.0703125), (0.1640625, 0.0712890625), (0.1640625, 0.5), (0.1640625, 0.5009765625), (0.1640625, 0.5078125), (0.1640625, 0.5087890625), (0.1640625, 0.5625), (0.1640625, 0.5634765625), (0.1640625, 0.5703125), (0.1640625, 0.5712890625), (0.5, 0.0), (0.5, 0.0009765625), (0.5, 0.0078125), (0.5, 0.0087890625), (0.5, 0.0625), (0.5, 0.0634765625), (0.5, 0.0703125), (0.5, 0.0712890625), (0.5, 0.5), (0.5, 0.5009765625), (0.5, 0.5078125), (0.5, 0.5087890625), (0.5, 0.5625), (0.5, 0.5634765625), (0.5, 0.5703125), (0.5, 0.5712890625), (0.5078125, 0.0), (0.5078125, 0.0009765625), (0.5078125, 0.0078125), (0.5078125, 0.0087890625), (0.5078125, 0.0625), (0.5078125, 0.0634765625), (0.5078125, 0.0703125), (0.5078125, 0.0712890625), (0.5078125, 0.5), (0.5078125, 0.5009765625), (0.5078125, 0.5078125), (0.5078125, 0.5087890625), (0.5078125, 0.5625), (0.5078125, 0.5634765625), (0.5078125, 0.5703125), (0.5078125, 0.5712890625), (0.53125, 0.0), (0.53125, 0.0009765625), (0.53125, 0.0078125), (0.53125, 0.0087890625), (0.53125, 0.0625), (0.53125, 0.0634765625), (0.53125, 0.0703125), (0.53125, 0.0712890625), (0.53125, 0.5), (0.53125, 0.5009765625), (0.53125, 0.5078125), (0.53125, 0.5087890625), (0.53125, 0.5625), (0.53125, 0.5634765625), (0.53125, 0.5703125), (0.53125, 0.5712890625), (0.5390625, 0.0), (0.5390625, 0.0009765625), (0.5390625, 0.0078125), (0.5390625, 0.0087890625), (0.5390625, 0.0625), (0.5390625, 0.0634765625), (0.5390625, 0.0703125), (0.5390625, 0.0712890625), (0.5390625, 0.5), (0.5390625, 0.5009765625), (0.5390625, 0.5078125), (0.5390625, 0.5087890625), (0.5390625, 0.5625), (0.5390625, 0.5634765625), (0.5390625, 0.5703125), (0.5390625, 0.5712890625), (0.625, 0.0), (0.625, 0.0009765625), (0.625, 0.0078125), (0.625, 0.0087890625), (0.625, 0.0625), (0.625, 0.0634765625), (0.625, 0.0703125), (0.625, 0.0712890625), (0.625, 0.5), (0.625, 0.5009765625), (0.625, 0.5078125), (0.625, 0.5087890625), (0.625, 0.5625), (0.625, 0.5634765625), (0.625, 0.5703125), (0.625, 0.5712890625), (0.6328125, 0.0), (0.6328125, 0.0009765625), (0.6328125, 0.0078125), (0.6328125, 0.0087890625), (0.6328125, 0.0625), (0.6328125, 0.0634765625), (0.6328125, 0.0703125), (0.6328125, 0.0712890625), (0.6328125, 0.5), (0.6328125, 0.5009765625), (0.6328125, 0.5078125), (0.6328125, 0.5087890625), (0.6328125, 0.5625), (0.6328125, 0.5634765625), (0.6328125, 0.5703125), (0.6328125, 0.5712890625), (0.65625, 0.0), (0.65625, 0.0009765625), (0.65625, 0.0078125), (0.65625, 0.0087890625), (0.65625, 0.0625), (0.65625, 0.0634765625), (0.65625, 0.0703125), (0.65625, 0.0712890625), (0.65625, 0.5), (0.65625, 0.5009765625), (0.65625, 0.5078125), (0.65625, 0.5087890625), (0.65625, 0.5625), (0.65625, 0.5634765625), (0.65625, 0.5703125), (0.65625, 0.5712890625), (0.6640625, 0.0), (0.6640625, 0.0009765625), (0.6640625, 0.0078125), (0.6640625, 0.0087890625), (0.6640625, 0.0625), (0.6640625, 0.0634765625), (0.6640625, 0.0703125), (0.6640625, 0.0712890625), (0.6640625, 0.5), (0.6640625, 0.5009765625), (0.6640625, 0.5078125), (0.6640625, 0.5087890625), (0.6640625, 0.5625), (0.6640625, 0.5634765625), (0.6640625, 0.5703125), (0.6640625, 0.5712890625)]\n",
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.0659 - 379ms/epoch - 54ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.0323 - 16ms/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0208 - 18ms/epoch - 3ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0155 - 18ms/epoch - 3ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0102 - 19ms/epoch - 3ms/step\n",
            "Dimension:  2\n",
            "********** 0.09150245751184621\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "\n",
        "\n",
        "def increaseDiscreteDimMV(finalDim):  #it will generate a series of Cantor sets with increasing dimesntionality and the corresponding learning tasks\n",
        "  for i in range(2,finalDim+1):   #finalDim corresponds to the dimention of the last high dim Cantor set in the series of Cantor sets\n",
        "\n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "    \n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,SqrtAllComp)   \n",
        "\n",
        "    print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "    \n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "   \n",
        "    z1 = np.array(HighDimList)\n",
        "    k1 = np.array(outputlist)\n",
        "  \n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=i, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"Dimension: \", i )\n",
        "    print(\"**********\", score2)\n",
        "\n",
        " \n",
        "\n",
        "increaseDiscreteDimMV(2)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Spy7L2fBIgp"
      },
      "source": [
        "## Decomposition x+y to (x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0VykBzWABJCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443bf009-33bb-4d95-bed7-05db24af69e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.1504 - 390ms/epoch - 56ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.0893 - 17ms/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0530 - 15ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0378 - 17ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0365 - 15ms/epoch - 2ms/step\n",
            "********** 0.17768469624561453\n",
            "Epoch 1/5\n",
            "103/103 - 1s - loss: 0.0532 - 508ms/epoch - 5ms/step\n",
            "Epoch 2/5\n",
            "103/103 - 0s - loss: 0.0431 - 148ms/epoch - 1ms/step\n",
            "Epoch 3/5\n",
            "103/103 - 0s - loss: 0.0429 - 156ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "103/103 - 0s - loss: 0.0430 - 158ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "103/103 - 0s - loss: 0.0429 - 155ms/epoch - 2ms/step\n",
            "********** 0.20815332690173713\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def increaseDiscreteDimDECOMP(finalDim):   \n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "    \n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "    \n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,IdentitySum)\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #Distencefunction\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "    \n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "   \n",
        "    z1 = np.array(oneDimlist)\n",
        "    k1 = np.array(HighDimList)\n",
        "    \n",
        "    #print(HighDimListCopy)\n",
        " \n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=1, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(i)) # Output dimension \n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"**********\", score2)\n",
        "\n",
        " \n",
        "\n",
        "increaseDiscreteDimDECOMP(3)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swi2Bw3NCFJk"
      },
      "source": [
        "## Comparing with decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BHTWoyZGCR4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bd5b1a-699d-40a4-e1af-dfbcfa589254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.4193 - 362ms/epoch - 52ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.1742 - 15ms/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0506 - 16ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0214 - 16ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0242 - 15ms/epoch - 2ms/step\n",
            "********** 0.14064577979216578\n",
            "Epoch 1/5\n",
            "103/103 - 0s - loss: 0.0977 - 486ms/epoch - 5ms/step\n",
            "Epoch 2/5\n",
            "103/103 - 0s - loss: 2.6269e-05 - 152ms/epoch - 1ms/step\n",
            "Epoch 3/5\n",
            "103/103 - 0s - loss: 9.8285e-06 - 139ms/epoch - 1ms/step\n",
            "Epoch 4/5\n",
            "103/103 - 0s - loss: 8.3076e-06 - 149ms/epoch - 1ms/step\n",
            "Epoch 5/5\n",
            "103/103 - 0s - loss: 6.9876e-06 - 139ms/epoch - 1ms/step\n",
            "********** 0.0024908115129985754\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def increaseDiscreteDimIDDECOMP(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "    \n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "    \n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,IdentitySum)\n",
        "\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "    \n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "    \n",
        "    z1 = np.array(HighDimList)\n",
        "    k1 = np.array(outputlist)\n",
        "\n",
        "    \n",
        "    #print(HighDimListCopy)\n",
        "  \n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=i, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"**********\", score2)\n",
        "\n",
        " \n",
        "\n",
        "increaseDiscreteDimIDDECOMP(3)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Culi19sXPeND"
      },
      "source": [
        "## x+y to function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mhIcSv-_WeVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028bffae-16ae-4196-cf03-906870f5ce87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.0, 0.0), (0.0, 0.0009765625), (0.0, 0.0078125), (0.0, 0.0087890625), (0.0, 0.0625), (0.0, 0.0634765625), (0.0, 0.0703125), (0.0, 0.0712890625), (0.0, 0.5), (0.0, 0.5009765625), (0.0, 0.5078125), (0.0, 0.5087890625), (0.0, 0.5625), (0.0, 0.5634765625), (0.0, 0.5703125), (0.0, 0.5712890625), (0.0078125, 0.0), (0.0078125, 0.0009765625), (0.0078125, 0.0078125), (0.0078125, 0.0087890625), (0.0078125, 0.0625), (0.0078125, 0.0634765625), (0.0078125, 0.0703125), (0.0078125, 0.0712890625), (0.0078125, 0.5), (0.0078125, 0.5009765625), (0.0078125, 0.5078125), (0.0078125, 0.5087890625), (0.0078125, 0.5625), (0.0078125, 0.5634765625), (0.0078125, 0.5703125), (0.0078125, 0.5712890625), (0.03125, 0.0), (0.03125, 0.0009765625), (0.03125, 0.0078125), (0.03125, 0.0087890625), (0.03125, 0.0625), (0.03125, 0.0634765625), (0.03125, 0.0703125), (0.03125, 0.0712890625), (0.03125, 0.5), (0.03125, 0.5009765625), (0.03125, 0.5078125), (0.03125, 0.5087890625), (0.03125, 0.5625), (0.03125, 0.5634765625), (0.03125, 0.5703125), (0.03125, 0.5712890625), (0.0390625, 0.0), (0.0390625, 0.0009765625), (0.0390625, 0.0078125), (0.0390625, 0.0087890625), (0.0390625, 0.0625), (0.0390625, 0.0634765625), (0.0390625, 0.0703125), (0.0390625, 0.0712890625), (0.0390625, 0.5), (0.0390625, 0.5009765625), (0.0390625, 0.5078125), (0.0390625, 0.5087890625), (0.0390625, 0.5625), (0.0390625, 0.5634765625), (0.0390625, 0.5703125), (0.0390625, 0.5712890625), (0.125, 0.0), (0.125, 0.0009765625), (0.125, 0.0078125), (0.125, 0.0087890625), (0.125, 0.0625), (0.125, 0.0634765625), (0.125, 0.0703125), (0.125, 0.0712890625), (0.125, 0.5), (0.125, 0.5009765625), (0.125, 0.5078125), (0.125, 0.5087890625), (0.125, 0.5625), (0.125, 0.5634765625), (0.125, 0.5703125), (0.125, 0.5712890625), (0.1328125, 0.0), (0.1328125, 0.0009765625), (0.1328125, 0.0078125), (0.1328125, 0.0087890625), (0.1328125, 0.0625), (0.1328125, 0.0634765625), (0.1328125, 0.0703125), (0.1328125, 0.0712890625), (0.1328125, 0.5), (0.1328125, 0.5009765625), (0.1328125, 0.5078125), (0.1328125, 0.5087890625), (0.1328125, 0.5625), (0.1328125, 0.5634765625), (0.1328125, 0.5703125), (0.1328125, 0.5712890625), (0.15625, 0.0), (0.15625, 0.0009765625), (0.15625, 0.0078125), (0.15625, 0.0087890625), (0.15625, 0.0625), (0.15625, 0.0634765625), (0.15625, 0.0703125), (0.15625, 0.0712890625), (0.15625, 0.5), (0.15625, 0.5009765625), (0.15625, 0.5078125), (0.15625, 0.5087890625), (0.15625, 0.5625), (0.15625, 0.5634765625), (0.15625, 0.5703125), (0.15625, 0.5712890625), (0.1640625, 0.0), (0.1640625, 0.0009765625), (0.1640625, 0.0078125), (0.1640625, 0.0087890625), (0.1640625, 0.0625), (0.1640625, 0.0634765625), (0.1640625, 0.0703125), (0.1640625, 0.0712890625), (0.1640625, 0.5), (0.1640625, 0.5009765625), (0.1640625, 0.5078125), (0.1640625, 0.5087890625), (0.1640625, 0.5625), (0.1640625, 0.5634765625), (0.1640625, 0.5703125), (0.1640625, 0.5712890625), (0.5, 0.0), (0.5, 0.0009765625), (0.5, 0.0078125), (0.5, 0.0087890625), (0.5, 0.0625), (0.5, 0.0634765625), (0.5, 0.0703125), (0.5, 0.0712890625), (0.5, 0.5), (0.5, 0.5009765625), (0.5, 0.5078125), (0.5, 0.5087890625), (0.5, 0.5625), (0.5, 0.5634765625), (0.5, 0.5703125), (0.5, 0.5712890625), (0.5078125, 0.0), (0.5078125, 0.0009765625), (0.5078125, 0.0078125), (0.5078125, 0.0087890625), (0.5078125, 0.0625), (0.5078125, 0.0634765625), (0.5078125, 0.0703125), (0.5078125, 0.0712890625), (0.5078125, 0.5), (0.5078125, 0.5009765625), (0.5078125, 0.5078125), (0.5078125, 0.5087890625), (0.5078125, 0.5625), (0.5078125, 0.5634765625), (0.5078125, 0.5703125), (0.5078125, 0.5712890625), (0.53125, 0.0), (0.53125, 0.0009765625), (0.53125, 0.0078125), (0.53125, 0.0087890625), (0.53125, 0.0625), (0.53125, 0.0634765625), (0.53125, 0.0703125), (0.53125, 0.0712890625), (0.53125, 0.5), (0.53125, 0.5009765625), (0.53125, 0.5078125), (0.53125, 0.5087890625), (0.53125, 0.5625), (0.53125, 0.5634765625), (0.53125, 0.5703125), (0.53125, 0.5712890625), (0.5390625, 0.0), (0.5390625, 0.0009765625), (0.5390625, 0.0078125), (0.5390625, 0.0087890625), (0.5390625, 0.0625), (0.5390625, 0.0634765625), (0.5390625, 0.0703125), (0.5390625, 0.0712890625), (0.5390625, 0.5), (0.5390625, 0.5009765625), (0.5390625, 0.5078125), (0.5390625, 0.5087890625), (0.5390625, 0.5625), (0.5390625, 0.5634765625), (0.5390625, 0.5703125), (0.5390625, 0.5712890625), (0.625, 0.0), (0.625, 0.0009765625), (0.625, 0.0078125), (0.625, 0.0087890625), (0.625, 0.0625), (0.625, 0.0634765625), (0.625, 0.0703125), (0.625, 0.0712890625), (0.625, 0.5), (0.625, 0.5009765625), (0.625, 0.5078125), (0.625, 0.5087890625), (0.625, 0.5625), (0.625, 0.5634765625), (0.625, 0.5703125), (0.625, 0.5712890625), (0.6328125, 0.0), (0.6328125, 0.0009765625), (0.6328125, 0.0078125), (0.6328125, 0.0087890625), (0.6328125, 0.0625), (0.6328125, 0.0634765625), (0.6328125, 0.0703125), (0.6328125, 0.0712890625), (0.6328125, 0.5), (0.6328125, 0.5009765625), (0.6328125, 0.5078125), (0.6328125, 0.5087890625), (0.6328125, 0.5625), (0.6328125, 0.5634765625), (0.6328125, 0.5703125), (0.6328125, 0.5712890625), (0.65625, 0.0), (0.65625, 0.0009765625), (0.65625, 0.0078125), (0.65625, 0.0087890625), (0.65625, 0.0625), (0.65625, 0.0634765625), (0.65625, 0.0703125), (0.65625, 0.0712890625), (0.65625, 0.5), (0.65625, 0.5009765625), (0.65625, 0.5078125), (0.65625, 0.5087890625), (0.65625, 0.5625), (0.65625, 0.5634765625), (0.65625, 0.5703125), (0.65625, 0.5712890625), (0.6640625, 0.0), (0.6640625, 0.0009765625), (0.6640625, 0.0078125), (0.6640625, 0.0087890625), (0.6640625, 0.0625), (0.6640625, 0.0634765625), (0.6640625, 0.0703125), (0.6640625, 0.0712890625), (0.6640625, 0.5), (0.6640625, 0.5009765625), (0.6640625, 0.5078125), (0.6640625, 0.5087890625), (0.6640625, 0.5625), (0.6640625, 0.5634765625), (0.6640625, 0.5703125), (0.6640625, 0.5712890625)]\n",
            "[0.0, 0.0009765625, 0.0078125, 0.0087890625, 0.0625, 0.0634765625, 0.0703125, 0.0712890625, 0.5, 0.5009765625, 0.5078125, 0.5087890625, 0.5625, 0.5634765625, 0.5703125, 0.5712890625, 0.0078125, 0.0087890625, 0.015625, 0.0166015625, 0.0703125, 0.0712890625, 0.078125, 0.0791015625, 0.5078125, 0.5087890625, 0.515625, 0.5166015625, 0.5703125, 0.5712890625, 0.578125, 0.5791015625, 0.03125, 0.0322265625, 0.0390625, 0.0400390625, 0.09375, 0.0947265625, 0.1015625, 0.1025390625, 0.53125, 0.5322265625, 0.5390625, 0.5400390625, 0.59375, 0.5947265625, 0.6015625, 0.6025390625, 0.0390625, 0.0400390625, 0.046875, 0.0478515625, 0.1015625, 0.1025390625, 0.109375, 0.1103515625, 0.5390625, 0.5400390625, 0.546875, 0.5478515625, 0.6015625, 0.6025390625, 0.609375, 0.6103515625, 0.125, 0.1259765625, 0.1328125, 0.1337890625, 0.1875, 0.1884765625, 0.1953125, 0.1962890625, 0.625, 0.6259765625, 0.6328125, 0.6337890625, 0.6875, 0.6884765625, 0.6953125, 0.6962890625, 0.1328125, 0.1337890625, 0.140625, 0.1416015625, 0.1953125, 0.1962890625, 0.203125, 0.2041015625, 0.6328125, 0.6337890625, 0.640625, 0.6416015625, 0.6953125, 0.6962890625, 0.703125, 0.7041015625, 0.15625, 0.1572265625, 0.1640625, 0.1650390625, 0.21875, 0.2197265625, 0.2265625, 0.2275390625, 0.65625, 0.6572265625, 0.6640625, 0.6650390625, 0.71875, 0.7197265625, 0.7265625, 0.7275390625, 0.1640625, 0.1650390625, 0.171875, 0.1728515625, 0.2265625, 0.2275390625, 0.234375, 0.2353515625, 0.6640625, 0.6650390625, 0.671875, 0.6728515625, 0.7265625, 0.7275390625, 0.734375, 0.7353515625, 0.5, 0.5009765625, 0.5078125, 0.5087890625, 0.5625, 0.5634765625, 0.5703125, 0.5712890625, 1.0, 1.0009765625, 1.0078125, 1.0087890625, 1.0625, 1.0634765625, 1.0703125, 1.0712890625, 0.5078125, 0.5087890625, 0.515625, 0.5166015625, 0.5703125, 0.5712890625, 0.578125, 0.5791015625, 1.0078125, 1.0087890625, 1.015625, 1.0166015625, 1.0703125, 1.0712890625, 1.078125, 1.0791015625, 0.53125, 0.5322265625, 0.5390625, 0.5400390625, 0.59375, 0.5947265625, 0.6015625, 0.6025390625, 1.03125, 1.0322265625, 1.0390625, 1.0400390625, 1.09375, 1.0947265625, 1.1015625, 1.1025390625, 0.5390625, 0.5400390625, 0.546875, 0.5478515625, 0.6015625, 0.6025390625, 0.609375, 0.6103515625, 1.0390625, 1.0400390625, 1.046875, 1.0478515625, 1.1015625, 1.1025390625, 1.109375, 1.1103515625, 0.625, 0.6259765625, 0.6328125, 0.6337890625, 0.6875, 0.6884765625, 0.6953125, 0.6962890625, 1.125, 1.1259765625, 1.1328125, 1.1337890625, 1.1875, 1.1884765625, 1.1953125, 1.1962890625, 0.6328125, 0.6337890625, 0.640625, 0.6416015625, 0.6953125, 0.6962890625, 0.703125, 0.7041015625, 1.1328125, 1.1337890625, 1.140625, 1.1416015625, 1.1953125, 1.1962890625, 1.203125, 1.2041015625, 0.65625, 0.6572265625, 0.6640625, 0.6650390625, 0.71875, 0.7197265625, 0.7265625, 0.7275390625, 1.15625, 1.1572265625, 1.1640625, 1.1650390625, 1.21875, 1.2197265625, 1.2265625, 1.2275390625, 0.6640625, 0.6650390625, 0.671875, 0.6728515625, 0.7265625, 0.7275390625, 0.734375, 0.7353515625, 1.1640625, 1.1650390625, 1.171875, 1.1728515625, 1.2265625, 1.2275390625, 1.234375, 1.2353515625]\n",
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.0763 - 371ms/epoch - 53ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.0212 - 19ms/epoch - 3ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0178 - 19ms/epoch - 3ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0160 - 18ms/epoch - 3ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0133 - 23ms/epoch - 3ms/step\n",
            "Dimension:  2\n",
            "********** 0.10651015647637592\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "#def Pickhundredpt(list): #list size >=1000\n",
        " # listcopy=random.sample(len(list), 2)\n",
        " # return listcopy\n",
        "\n",
        "#test=[(1,2),(3,4),(3,7),(4,5)]\n",
        "\n",
        "#print(Pickhundredpt(test))\n",
        "\n",
        "\n",
        "def increaseDiscreteDimFRACTALMV(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "    \n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "    \n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,SqrtAllComp)   \n",
        "\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "    \n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "    print(HighDimList)\n",
        "    print(oneDimlist)\n",
        "\n",
        "    z1 = np.array(oneDimlist)\n",
        "    k1 = np.array(outputlist)\n",
        "\n",
        "    \n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "    \n",
        "      \n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=1, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"Dimension: \" ,i)\n",
        "    print(\"**********\", score2)\n",
        "\n",
        " \n",
        "\n",
        "increaseDiscreteDimFRACTALMV(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36rwMIxXoLGH"
      },
      "source": [
        "## HIGHER NEURON TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "SfUfjln-oJSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531df16c-9129-4af7-e57f-ce22c6835536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 - 0s - loss: 0.0883 - 487ms/epoch - 70ms/step\n",
            "Epoch 2/20\n",
            "7/7 - 0s - loss: 0.0443 - 53ms/epoch - 8ms/step\n",
            "Epoch 3/20\n",
            "7/7 - 0s - loss: 0.0383 - 56ms/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "7/7 - 0s - loss: 0.0354 - 56ms/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "7/7 - 0s - loss: 0.0347 - 61ms/epoch - 9ms/step\n",
            "Epoch 6/20\n",
            "7/7 - 0s - loss: 0.0346 - 62ms/epoch - 9ms/step\n",
            "Epoch 7/20\n",
            "7/7 - 0s - loss: 0.0348 - 58ms/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "7/7 - 0s - loss: 0.0347 - 63ms/epoch - 9ms/step\n",
            "Epoch 9/20\n",
            "7/7 - 0s - loss: 0.0352 - 81ms/epoch - 12ms/step\n",
            "Epoch 10/20\n",
            "7/7 - 0s - loss: 0.0351 - 66ms/epoch - 9ms/step\n",
            "Epoch 11/20\n",
            "7/7 - 0s - loss: 0.0348 - 59ms/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "7/7 - 0s - loss: 0.0347 - 68ms/epoch - 10ms/step\n",
            "Epoch 13/20\n",
            "7/7 - 0s - loss: 0.0350 - 56ms/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "7/7 - 0s - loss: 0.0350 - 56ms/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "7/7 - 0s - loss: 0.0344 - 58ms/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "7/7 - 0s - loss: 0.0345 - 63ms/epoch - 9ms/step\n",
            "Epoch 17/20\n",
            "7/7 - 0s - loss: 0.0345 - 57ms/epoch - 8ms/step\n",
            "Epoch 18/20\n",
            "7/7 - 0s - loss: 0.0343 - 63ms/epoch - 9ms/step\n",
            "Epoch 19/20\n",
            "7/7 - 0s - loss: 0.0343 - 56ms/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "7/7 - 0s - loss: 0.0346 - 62ms/epoch - 9ms/step\n",
            "********** 0.1610274686171647\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def increaseDiscreteDimDECOMPHIGH(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "    \n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "    \n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,Distencefunction)\n",
        "\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "    \n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "    \n",
        "    z1 = np.array(oneDimlist)\n",
        "    k1 = np.array(HighDimList)\n",
        "\n",
        "    \n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "    \n",
        "  \n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(1000, input_dim=1, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(500, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(150, activation='relu')) # Hidden 3\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 4\n",
        "    modelh.add(Dense(i)) # Output dimension \n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=20)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"**********\", score2)\n",
        "\n",
        " \n",
        "\n",
        "increaseDiscreteDimDECOMPHIGH(2)   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now We turn our attention to the timeseries forcasting"
      ],
      "metadata": {
        "id": "DUbJ_ZDSZEwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain = []\n",
        "\n",
        "for i in range(0,800):\n",
        "  domain.append(i/1000)\n",
        "\n",
        "test_domain = []\n",
        "\n",
        "for i in range(800,1001):\n",
        "  test_domain.append(i/1000)\n",
        "\n",
        "def timeseries(f):\n",
        "  series = []\n",
        "  for i in domain:\n",
        "    series.append(f(i))\n",
        "  return series\n",
        "\n"
      ],
      "metadata": {
        "id": "y_wz2o1DZLv3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 3\n",
        "\n",
        "#returns the RMSE, percent of variance explained and the L2 norm of the data\n",
        "def function_info (function):\n",
        "  #model for the data\n",
        "  tensorflow.random.set_seed(seed)\n",
        "  model1 = Sequential()\n",
        "  model1.add(Dense(3000, input_dim=1, activation='relu')) # Hidden 1\n",
        "  model1.add(Dense(1500, activation='relu')) # Hidden 2\n",
        "  model1.add(Dense(1)) # Output\n",
        "  model1.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  #fits the model based on the input function\n",
        "  model1.fit(domain,timeseries(function),verbose=2,epochs=16)\n",
        "\n",
        "\n",
        "  #processes the data for the model\n",
        "\n",
        "  pred1 = model1.predict(test_domain)\n",
        "  y1 = np.array(list(map(function, test_domain)))\n",
        "\n",
        "  #score1 = np.sqrt(metrics.mean_squared_error(pred1, y1)) #we shall try both the mean sqaure error or just norm\n",
        "  score1 = norm(pred1-y1,2)/(len(y1))\n",
        "  pvariance1 = (y1.std()-score1)/y1.std()\n",
        "  \n",
        "  #calculates the l2 norm\n",
        "\n",
        "  sum = 0\n",
        "  n = len(y1)\n",
        "  for i in range (n-1):\n",
        "    sum += (y1[i+1]-y1[i])**2\n",
        "\n",
        "  sum = sum/n\n",
        "\n",
        "  sum = np.sqrt(sum)\n",
        "\n",
        "  return score1, pvariance1, sum\n"
      ],
      "metadata": {
        "id": "pu5YmyydZVaJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_array = []\n",
        "\n",
        "for i in range(1,11):\n",
        "  f = lambda x, i=i : (np.sin(1/x))*(x**(1+(1/i))) if x!=0 else 0\n",
        "  function_array.append(f)\n",
        "\n",
        "for i in function_array:\n",
        "  print(i(1/2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kmFi_YJZYc6",
        "outputId": "75953658-d549-4e0b-9793-bd5247cfbaa6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22732435670642043\n",
            "0.32148518831195905\n",
            "0.36085492297376603\n",
            "0.38231247330857127\n",
            "0.3957946935634067\n",
            "0.4050459559836544\n",
            "0.41178609866514904\n",
            "0.4169147084373769\n",
            "0.42094774672290075\n",
            "0.4242022491739435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_those_results = np.array(list(map(function_info, function_array)))\n",
        "all_those_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqELpCRWZbfD",
        "outputId": "0354af42-8ed3-4100-94e1-90cad2578625"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0103 - 1s/epoch - 53ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 3.8586e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 6.8692e-05 - 991ms/epoch - 40ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 2.4801e-05 - 964ms/epoch - 39ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 1.7840e-05 - 979ms/epoch - 39ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 1.4267e-05 - 981ms/epoch - 39ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 1.3146e-05 - 984ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 1.2317e-05 - 991ms/epoch - 40ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 1.1609e-05 - 994ms/epoch - 40ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 1.0862e-05 - 970ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 1.0309e-05 - 965ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 9.9390e-06 - 979ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 9.6654e-06 - 973ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 9.5066e-06 - 975ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 9.4774e-06 - 976ms/epoch - 39ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 9.4736e-06 - 978ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0134 - 1s/epoch - 51ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0013 - 985ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 6.6322e-04 - 974ms/epoch - 39ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 4.3294e-04 - 972ms/epoch - 39ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 2.5132e-04 - 961ms/epoch - 38ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 1.7201e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 1.5242e-04 - 970ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 1.3714e-04 - 973ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 1.2690e-04 - 973ms/epoch - 39ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 1.1503e-04 - 1s/epoch - 40ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 1.0731e-04 - 979ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 2s - loss: 1.0156e-04 - 2s/epoch - 66ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 1.0228e-04 - 972ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 1.0564e-04 - 956ms/epoch - 38ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 1.0575e-04 - 966ms/epoch - 39ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 9.7547e-05 - 969ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0156 - 1s/epoch - 52ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0021 - 963ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0013 - 962ms/epoch - 38ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 8.5715e-04 - 968ms/epoch - 39ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 5.2244e-04 - 962ms/epoch - 38ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 3.8925e-04 - 960ms/epoch - 38ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 3.3521e-04 - 972ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 2.9776e-04 - 966ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 2.5053e-04 - 993ms/epoch - 40ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 2.2131e-04 - 964ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 2.0973e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 2.0749e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 2.0675e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 2.0537e-04 - 980ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 1.9131e-04 - 971ms/epoch - 39ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 1.6994e-04 - 968ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0170 - 1s/epoch - 52ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0027 - 957ms/epoch - 38ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0017 - 988ms/epoch - 40ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0011 - 962ms/epoch - 38ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 7.0235e-04 - 959ms/epoch - 38ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 5.8329e-04 - 959ms/epoch - 38ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 4.9844e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 3.8695e-04 - 961ms/epoch - 38ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 3.5072e-04 - 972ms/epoch - 39ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 3.3141e-04 - 962ms/epoch - 38ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 3.1998e-04 - 970ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 2.9548e-04 - 963ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 2.6322e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 2.3988e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 2.3726e-04 - 977ms/epoch - 39ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 2.4054e-04 - 963ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0180 - 1s/epoch - 52ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0032 - 972ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0020 - 977ms/epoch - 39ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0013 - 954ms/epoch - 38ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 8.8079e-04 - 967ms/epoch - 39ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 7.2938e-04 - 966ms/epoch - 39ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 5.9360e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 4.6450e-04 - 959ms/epoch - 38ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 4.1571e-04 - 967ms/epoch - 39ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 4.0325e-04 - 1s/epoch - 53ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 2s - loss: 3.8365e-04 - 2s/epoch - 66ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 3.4763e-04 - 964ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 3.0746e-04 - 958ms/epoch - 38ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 2.9302e-04 - 959ms/epoch - 38ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 2.9557e-04 - 958ms/epoch - 38ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.0749e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0187 - 1s/epoch - 52ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0035 - 999ms/epoch - 40ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0022 - 977ms/epoch - 39ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0014 - 987ms/epoch - 39ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0010 - 995ms/epoch - 40ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 8.8231e-04 - 997ms/epoch - 40ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 6.6467e-04 - 983ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 5.2506e-04 - 984ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 4.6612e-04 - 989ms/epoch - 40ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 4.4166e-04 - 977ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 4.0367e-04 - 1s/epoch - 44ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 2s - loss: 3.5951e-04 - 2s/epoch - 68ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 3.3375e-04 - 986ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 3.3101e-04 - 966ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 3.3866e-04 - 992ms/epoch - 40ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.8074e-04 - 1s/epoch - 40ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0193 - 1s/epoch - 53ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0038 - 978ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0024 - 1s/epoch - 40ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0015 - 1s/epoch - 40ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0012 - 985ms/epoch - 39ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 9.8909e-04 - 981ms/epoch - 39ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 7.5444e-04 - 975ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 5.7693e-04 - 971ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.1587e-04 - 973ms/epoch - 39ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 4.9978e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 4.7085e-04 - 970ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 4.2747e-04 - 980ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 3.8950e-04 - 975ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 3.7422e-04 - 963ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 3.6851e-04 - 984ms/epoch - 39ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.8171e-04 - 971ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 2s - loss: 0.0198 - 2s/epoch - 65ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0040 - 977ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0025 - 965ms/epoch - 39ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0016 - 969ms/epoch - 39ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0012 - 960ms/epoch - 38ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 0.0011 - 966ms/epoch - 39ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 8.3148e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 6.0975e-04 - 974ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.5480e-04 - 971ms/epoch - 39ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 5.3347e-04 - 968ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 5.0555e-04 - 995ms/epoch - 40ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 4.6358e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 4.2388e-04 - 965ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 4.0166e-04 - 975ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 3.9117e-04 - 961ms/epoch - 38ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.9137e-04 - 962ms/epoch - 38ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0201 - 1s/epoch - 51ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0042 - 962ms/epoch - 38ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0026 - 957ms/epoch - 38ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0017 - 962ms/epoch - 38ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0013 - 964ms/epoch - 39ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 0.0011 - 970ms/epoch - 39ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 8.9982e-04 - 974ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 6.4905e-04 - 971ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.9164e-04 - 962ms/epoch - 38ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 5.7446e-04 - 966ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 5.4820e-04 - 980ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 5.0005e-04 - 976ms/epoch - 39ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 4.5871e-04 - 969ms/epoch - 39ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 4.3227e-04 - 966ms/epoch - 39ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 4.1358e-04 - 959ms/epoch - 38ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 4.1914e-04 - 964ms/epoch - 39ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0204 - 1s/epoch - 53ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0043 - 974ms/epoch - 39ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0028 - 984ms/epoch - 39ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0018 - 970ms/epoch - 39ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0014 - 986ms/epoch - 39ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 0.0012 - 990ms/epoch - 40ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 8.8856e-04 - 982ms/epoch - 39ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 6.6518e-04 - 978ms/epoch - 39ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.9883e-04 - 986ms/epoch - 39ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 5.7026e-04 - 986ms/epoch - 39ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 5.3294e-04 - 984ms/epoch - 39ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 4.8353e-04 - 991ms/epoch - 40ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 4.4455e-04 - 997ms/epoch - 40ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 4.2855e-04 - 993ms/epoch - 40ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 4.2312e-04 - 1s/epoch - 58ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 4.5724e-04 - 987ms/epoch - 39ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.08908554e-02, -1.91362193e-01,  1.16781785e-03],\n",
              "       [ 8.80260623e-02, -8.71150215e-01,  8.12144048e-04],\n",
              "       [ 9.07564358e-02, -1.29395934e+00,  6.84911854e-04],\n",
              "       [ 9.52754523e-02, -1.66737509e+00,  6.19586555e-04],\n",
              "       [ 9.58066456e-02, -1.87033157e+00,  5.79838875e-04],\n",
              "       [ 1.00344172e-01, -2.15511669e+00,  5.53110150e-04],\n",
              "       [ 9.04572409e-02, -1.94918999e+00,  5.33905755e-04],\n",
              "       [ 8.68248549e-02, -1.91170560e+00,  5.19441214e-04],\n",
              "       [ 8.59430550e-02, -1.94792097e+00,  5.08154879e-04],\n",
              "       [ 9.29118027e-02, -2.24639663e+00,  4.99103143e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = []\n",
        "var = []\n",
        "for i in all_those_results:\n",
        "  rmse, pove, l2 = i\n",
        "  loss.append(rmse)\n",
        "  var.append(l2)\n",
        "\n",
        "nploss = np.array(loss)\n",
        "npvar = np.array(var)\n",
        "\n",
        "plt.scatter(npvar,nploss)\n",
        "a, b = np.polyfit(npvar, nploss, 1)\n",
        "plt.plot(npvar, a*npvar + b)\n",
        "plt.xlabel(\"Complexity\")\n",
        "plt.ylabel(\"Loss of the model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "le61RjWhZeo3",
        "outputId": "be494f1e-ec3d-476c-f008-e00e845e220a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Geometric measure group final ver.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}