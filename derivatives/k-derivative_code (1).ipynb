{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the TRIPODS2022 Derivatives group's k-derivative code. \n",
    "Given a function, this code takes k-number(s) of discrete derivative as regressors in order to predict the next numbers in that function's sequence.  Future experimentation will include testing a variety of functions, the number of predicted values, etc. to study how it affects predictive accuracy/variance and from that, find an ideal number of regressors to use that improves the performance of neural network prediction models. Additionally, study how differing function affects accuracy and define what makes a function \"complicated.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from math import log\n",
    "import numpy as np \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes another derivative for use in kthderiv below (list size decreases by 1)\n",
    "def derivative_new(my_list):\n",
    "    '''\n",
    "    Parameters:\n",
    "        my_list (list): list to take derivatives of\n",
    "    \n",
    "    Returns:\n",
    "        list containing differences of consecutive terms of my_list\n",
    "    '''\n",
    "    \n",
    "    return [my_list[i]-my_list[i-1] for i in range(1,len(my_list))]\n",
    "\n",
    "def kthderiv(extended_list, k, original_length):\n",
    "    '''\n",
    "    Parameters:\n",
    "        extended_list (list): list to take derivatives of plus at least k predictions\n",
    "        k (int): number of derivatives\n",
    "        original_length (int): length of original list\n",
    "    \n",
    "    Returns:\n",
    "        derivatives (numpy array): the ith column has the ith derivative of the list, up to the kth derivative\n",
    "    '''\n",
    "    \n",
    "    derivatives = extended_list[:original_length]\n",
    "    for i in range(1,k+1):\n",
    "        next_deriv = extended_list[:original_length + i]\n",
    "        for j in range(i):\n",
    "            next_deriv = derivative_new(next_deriv)\n",
    "        derivatives = np.column_stack((derivatives, next_deriv))\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a neural network with three hidden layers\n",
    "def forecast(x, y, x_pred, dim, layers, neurons):\n",
    "    '''\n",
    "    Trains a model on x and y and returns the array of predictions for x_pred\n",
    "    \n",
    "    Parameters:\n",
    "        x (numpy array): each row is a single input, contains a time column and the appropriate derivatives\n",
    "        y (numpy array): k x 1 array of all of the training labels\n",
    "        x_pred (numpy array): array with row length the same as x, used as input for predictions\n",
    "        dim (int): number of columns in x\n",
    "        layers (int): number of layers (excluding output)\n",
    "        neurons (int): total number of neurons\n",
    "    \n",
    "    Returns:\n",
    "        pred: predicted values for x-coordinates in x_pred\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(layers):\n",
    "        layer_size = 2**(layers - i - 1) * neurons//(2**(layers) - 1)\n",
    "        model.add(Dense(layer_size, activation='relu')) # Add hidden layer half the size of the previous one\n",
    "        \n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(x,y,verbose=0,epochs=512)\n",
    "    pred = model.predict(x_pred) # generate predicted values (output)\n",
    "    return pred\n",
    "\n",
    "def kderiv_predict_nonrecursive_new(x, y, x_pred, k_derivs, pred_length, layers, neurons): \n",
    "    '''\n",
    "    Parameters:\n",
    "        x (list): x-coordinates of known values\n",
    "        y (list): known function values\n",
    "        x_pred (list): original x-coordinates plus x-coordinates we want predictions for\n",
    "        k_derivs (int): number of derivatives to use\n",
    "        pred_length (int): number of predictions\n",
    "        layers (int): number of layers (excluding output)\n",
    "        neurons (int): total number of neurons\n",
    "        \n",
    "    Returns:\n",
    "        updated_pred: new predictions based on derivative regressors\n",
    "    '''\n",
    "    \n",
    "    x_pred_extended = x_pred + [x_pred[-1] + 1 + i for i in range(k_derivs)] # extend x_pred by k_derivs to compensate for decrease in length from differentiating\n",
    "    pred_extended = forecast(x, y, x_pred_extended, 1, layers, neurons) # forecast function values for x-coordinates in x_pred_extended\n",
    "    pred_extended = list(np.squeeze(pred_extended)) # turn pred_extended into a list of floats\n",
    "    x_derivs = kthderiv(y + pred_extended[len(y):len(y) + k_derivs], k_derivs, len(y)) # create array of derivatives regressors\n",
    "    x_pred_multi = kthderiv(y + pred_extended[len(y):len(y) + pred_length + k_derivs], k_derivs, len(y) + pred_length) # create array to be fed into the network\n",
    "    if k_derivs > 0:\n",
    "        y = np.array(y)\n",
    "    updated_pred = forecast(x_derivs, y, x_pred_multi, k_derivs+1, layers, neurons) # forecast based on added regressors\n",
    "    return updated_pred # returns final predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 10\n",
      "Final score (RMSE): 126.67821465825402\n",
      "95.71061664115508 percent of variance explained\n",
      "2 0 20\n",
      "Final score (RMSE): 303.6719736368022\n",
      "89.71752551313355 percent of variance explained\n",
      "2 0 30\n",
      "Final score (RMSE): 789.1976580603667\n",
      "73.27739966610794 percent of variance explained\n",
      "2 0 40\n",
      "Final score (RMSE): 1020.6955903401234\n",
      "65.43877184042684 percent of variance explained\n",
      "2 0 50\n",
      "Final score (RMSE): 1530.3178525546746\n",
      "48.182724644488886 percent of variance explained\n",
      "2 0 60\n",
      "Final score (RMSE): 1942.9694871883005\n",
      "34.21014807027133 percent of variance explained\n",
      "2 0 70\n",
      "Final score (RMSE): 2787.3392443151697\n",
      "5.61939476116698 percent of variance explained\n",
      "2 0 80\n",
      "Final score (RMSE): 3518.22752672039\n",
      "-19.12882295078366 percent of variance explained\n",
      "2 0 90\n",
      "Final score (RMSE): 4327.887824523129\n",
      "-46.54429779846451 percent of variance explained\n",
      "2 1 10\n",
      "Final score (RMSE): 134.72705223052597\n",
      "95.43807925156771 percent of variance explained\n",
      "2 1 20\n",
      "Final score (RMSE): 557.636969802625\n",
      "81.11815243843724 percent of variance explained\n",
      "2 1 30\n",
      "Final score (RMSE): 700.6642625965015\n",
      "76.2751816780286 percent of variance explained\n",
      "2 1 40\n",
      "Final score (RMSE): 1078.8137696275535\n",
      "63.47086321656131 percent of variance explained\n"
     ]
    }
   ],
   "source": [
    "## Example to generate results for polynomials\n",
    "\n",
    "polynomial_results = {}\n",
    "for i in range(2,11):\n",
    "    for deriv in range(5):\n",
    "        for pred_length in range(10, 100, 10):\n",
    "            x = [a for a in range(100)]\n",
    "            y = [a**i for a in range(100)]\n",
    "            x_pred = [a for a in range(100 + pred_length)] \n",
    "            pred = kderiv_predict_nonrecursive_new(x, y, x_pred, deriv, pred_length, 3, 1750)\n",
    "            y_pred = np.array([a**i for a in range(100 + pred_length)])\n",
    "            y = np.array(y)\n",
    "            score = np.sqrt(metrics.mean_squared_error(pred,y_pred))\n",
    "            print(i, deriv, pred_length)\n",
    "            print(f\"Final score (RMSE): {score}\")\n",
    "            variance_explained = ((y.std()-score)/y.std())*100\n",
    "            print(variance_explained, \"percent of variance explained\")\n",
    "            polynomial_results[i, deriv, pred_length] = variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scale in np.logspace(-6, 0):\n",
    "    x = [a for a in range(100)]\n",
    "    y = [a**2 for a in range(100)]\n",
    "    x_pred = [a for a in range(110)] \n",
    "    pred = kderiv_predict_nonrecursive_new(x, y, x_pred, deriv, pred_length, 1, 1750)\n",
    "    y_pred = np.array([a**2 for a in range(110)])\n",
    "    y = np.array(y)\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_pred))\n",
    "    print(scale)\n",
    "    print(f\"Final score (RMSE): {score}\") \n",
    "    variance_explained = ((y.std()-score)/y.std())*100\n",
    "    print(variance_explained, \"percent of variance explained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
