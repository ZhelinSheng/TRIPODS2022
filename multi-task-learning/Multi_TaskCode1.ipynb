{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcuN61x3USsi"
   },
   "source": [
    "# MULTI-TASK DEEP REINFORCEMENT LEARNING\n",
    "\n",
    "This is the python notebook file which houses all code related to the Multi-Task Deep Reinforcement Learning project in the 2022 Tripods/StemForAll REU program run through the University of Rochester. We have built different grid games and different agents to play these grid games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s_rUL8xYEA_J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import time\n",
    "from random import sample\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KwCwbr_18r0"
   },
   "source": [
    "# GRIDWORLD\n",
    "\n",
    "This class allows us to define a grid game and its basic characteristics such as the size of the board, where the agent starts and currently is during an episode, and what its reward matrix and possible moves are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cJ1fNJ-L00gs"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, board_dims, start_state, terminal_state, reward_matrix):\n",
    "        self.board_dims = board_dims\n",
    "        self.height, self.width = board_dims\n",
    "        self.start_state = start_state\n",
    "        self.agent_pos = start_state\n",
    "        self.terminal_state = terminal_state\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.moves = [(-1,0),(0,-1),(1,0),(0,1)] #up,left,down,right\n",
    "        \n",
    "    def get_moves(self, point):\n",
    "        \"\"\"\n",
    "        point - the point our agent is in\n",
    "        This function obtains the points of possible moves you can potentially make from the point provided,\n",
    "        return being the ending position of the move\n",
    "\n",
    "        \"\"\"\n",
    "        return [tuple(np.array(point)+np.array(x)) for x in self.moves]\n",
    "\n",
    "    def point_to_index(self, point):\n",
    "        \"\"\"\n",
    "        point - the point we want to convert to index (for list of lists e.g.)\n",
    "        This function obtains the index when you iterate across each column over each row of our game board\n",
    "        \"\"\"\n",
    "        return point[0]*self.width + point[1]\n",
    "\n",
    "    def index_to_point(self, index):\n",
    "        \"\"\"\n",
    "        index - the index we want to convert to game square point\n",
    "        This function obtains the index when you iterate across each column over each row of our game board\n",
    "        \"\"\"\n",
    "        #(divisions, remainder)\n",
    "        return (index // self.width, index % self.width)\n",
    "\n",
    "    def get_reward(self,point):\n",
    "        \"\"\"\n",
    "        point - a point in our grid\n",
    "        This function returns the reward for our agent going to the state represented by point\n",
    "        \"\"\"\n",
    "        return self.reward_matrix[point]\n",
    "    \n",
    "    def illegal_move(self,point):\n",
    "        \"\"\"\n",
    "        point - a point that may or may not be in our grid\n",
    "        This function returns true if the move we make takes us off the board and false if the move is legal\n",
    "        \"\"\"\n",
    "        return True if ((point[0] < 0 or point[0] == self.height) or (point[1] < 0 or point[1] == self.width)) else False\n",
    "        \n",
    "    def print_board(self):\n",
    "        print(self.reward_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA0KJdeD2Z0F"
   },
   "source": [
    "# Individual Game Implementations\n",
    "\n",
    "Here we define each game and have it inherit attributes from GridWorld object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPEJazkk2lBc"
   },
   "source": [
    "## Targeting Game\n",
    "\n",
    "The targeting game tasks consists of the agent starting in a different square from the \"target\" square. The goal of the game is to have the agent reach the target square in as few moves as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a1vgm-pH03IR"
   },
   "outputs": [],
   "source": [
    "class Targeting_Game(GridWorld):\n",
    "    def __init__(self, board_dims, start_state, terminal_state, reward_matrix):\n",
    "        GridWorld.__init__(self,board_dims, start_state, terminal_state, reward_matrix)\n",
    "        \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        agent_pos - tuple point representing agent's current position on game board \n",
    "        remaining_prizes - list of tuples representing remaining prizes and their positions on game board\n",
    "        This function draws our environment\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.new(\"RGB\", (501, 501), \"black\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        w = 500/(self.width)\n",
    "        h = 500/(self.height)\n",
    "        color = \"white\"\n",
    "\n",
    "        #Draw Grid and Start/Stop Squares\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if(i == self.start_state[0] and j == self.start_state[1]):\n",
    "                    color = \"blue\"\n",
    "                elif(i == self.terminal_state[0] and j == self.terminal_state[1]):\n",
    "                    color = \"red\"\n",
    "                draw.rectangle(((0+j*w, 0+i*h),(w+j*w, h+i*h)), outline = \"black\", fill = color)\n",
    "                color =\"white\"\n",
    "\n",
    "        #Draw Agent\n",
    "        draw.ellipse((self.agent_pos[1]*w + w/4, self.agent_pos[0]*h + h/4, 3*w/4 + self.agent_pos[1]*w, 3*h/4 + self.agent_pos[0]*h), fill=\"black\")\n",
    "\n",
    "        display(image)\n",
    "\n",
    "    def update_state(self,new_pos, if_illegal):\n",
    "      \"\"\"\n",
    "      new_pos - a point in the game grid that the agent has moved to\n",
    "      This function updates the agent position for the GameGrid class variable.\n",
    "      \"\"\"\n",
    "      if not if_illegal:\n",
    "          self.agent_pos = new_pos\n",
    "\n",
    "    def is_episode_terminal(self):\n",
    "        \"\"\"\n",
    "        This function returns a boolean based on if the Game's current episode is finished.\n",
    "        \"\"\"\n",
    "        return True if self.agent_pos == self.terminal_state else False\n",
    "\n",
    "    def refresh_game(self):\n",
    "        \"\"\"\n",
    "        This function refreshes the important features of the game that might have changed within an episode\n",
    "        \"\"\"\n",
    "        self.agent_pos = self.start_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcPGSTP22opf"
   },
   "source": [
    "## Collection Game\n",
    "\n",
    "The collection game tasks consists of the agent starting in a different square from a set of prize squares. The goal of the game is to have the agent collect each prize in the prize squares in as few moves as possible (prizes being removed upon collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZQlA554m09sf"
   },
   "outputs": [],
   "source": [
    "class Collection_Game(GridWorld):\n",
    "    def __init__(self, board_dims, start_state, prize_states, reward_matrix, prize_value=0):\n",
    "        GridWorld.__init__(self,board_dims, start_state, None, reward_matrix)\n",
    "        self.prize_states = prize_states\n",
    "        self.remaining_prize_states = list(prize_states)\n",
    "        self.prize_value = prize_value\n",
    "        for prize_state in prize_states:\n",
    "            self.reward_matrix[prize_state] = prize_value\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        agent_pos - tuple point representing agent's current position on game board \n",
    "        remaining_prizes - list of tuples representing remaining prizes and their positions on game board\n",
    "        This function draws our environment\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.new(\"RGB\", (501, 501), \"black\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        w = 500/(self.width)\n",
    "        h = 500/(self.height)\n",
    "        color = \"white\"\n",
    "\n",
    "        #Draw Grid and Start/Stop Squares\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if(i == self.start_state[0] and j == self.start_state[1]):\n",
    "                    color = \"blue\"\n",
    "                draw.rectangle(((0+j*w, 0+i*h),(w+j*w, h+i*h)), outline = \"black\", fill = color)\n",
    "                color =\"white\"\n",
    "\n",
    "        #Draw Agent\n",
    "        draw.ellipse((self.agent_pos[1]*w + w/4, self.agent_pos[0]*h + h/4, 3*w/4 + self.agent_pos[1]*w, 3*h/4 + self.agent_pos[0]*h), fill=\"black\")\n",
    "\n",
    "        #Draw Prizes\n",
    "        if len(self.remaining_prize_states) > 1:\n",
    "            for x in self.remaining_prize_states:\n",
    "                draw.rectangle(((x[1]*w + w/4, x[0]*h + h/4), (3*w/4+x[1]*w, 3*h/4+x[0]*h)), outline = \"black\", fill = \"yellow\")\n",
    "        elif len(self.remaining_prize_states) == 1:\n",
    "            remaining_prize = self.remaining_prize_states[0]\n",
    "            draw.rectangle(((remaining_prize[1]*w + w/4, remaining_prize[0]*h + h/4), (3*w/4+remaining_prize[1]*w, 3*h/4+remaining_prize[0]*h)), outline = \"black\", fill = \"yellow\")\n",
    "      \n",
    "        display(image)\n",
    "\n",
    "    def update_state(self,new_pos, if_illegal):\n",
    "        \"\"\"\n",
    "        new_pos - a point in the game grid that the agent has moved to\n",
    "        This function updates the agent position for the GameGrid class variable.\n",
    "        \"\"\"\n",
    "        if not if_illegal:\n",
    "            self.agent_pos = new_pos\n",
    "\n",
    "        if self.agent_pos in self.remaining_prize_states:\n",
    "            self.remove_prize(self.agent_pos)\n",
    "\n",
    "    def remove_prize(self, prize_point):\n",
    "        \"\"\"\n",
    "        prize_point - a point in the game grid that contained a prize\n",
    "        This function removes the prize at the prize_point supplied from the remaining prizes. This function also\n",
    "            updates the reward matrix accordingly.\n",
    "        \"\"\"\n",
    "        #remove prize from remaining prizes\n",
    "        self.remaining_prize_states.remove(prize_point)\n",
    "        #adjust reward matrix to account for no prize at this prize_point for the rest of the episode\n",
    "        self.reward_matrix[prize_point] = self.reward_matrix[self.start_state]\n",
    "        #if there remains one prize, set that to be the terminal state for the episode\n",
    "        if len(self.remaining_prize_states) == 1:\n",
    "            self.terminal_state = self.remaining_prize_states[0]\n",
    "        \n",
    "    def is_episode_terminal(self):\n",
    "        return True if len(self.remaining_prize_states) == 0 else False\n",
    "\n",
    "    def refresh_game(self):\n",
    "        \"\"\"\n",
    "        This function refreshes the game's agent position, the remaining prizes, the reward matrix, and terminal \n",
    "            state. This is used between each episode\n",
    "        \"\"\"\n",
    "        self.agent_pos = self.start_state\n",
    "        self.remaining_prize_states = list(self.prize_states)\n",
    "        self.terminal_state = None\n",
    "        \n",
    "        for prize_state in self.prize_states:\n",
    "            self.reward_matrix[prize_state] = self.prize_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT0g8b1o7751"
   },
   "source": [
    "## FindMax_Game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eVgZfOTR8J4o"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFont\n",
    "\n",
    "class FindMax_Game(GridWorld):\n",
    "    \n",
    "    def __init__(self, board_dims, start_state, reward_matrix):\n",
    "        # the terminal state is where the max value locates\n",
    "        self.terminal_state = np.unravel_index(np.argmax(reward_matrix, axis=None), board_dims)\n",
    "        GridWorld.__init__(self, board_dims, start_state, self.terminal_state, reward_matrix)\n",
    "        # self.max_value = reward_matrix[self.terminal_state]\n",
    "        self.reward_matrix = reward_matrix\n",
    "        # initial sum of rewards is the reward value at the start state\n",
    "        self.sum = reward_matrix[start_state] \n",
    "        # the number of steps\n",
    "        self.n_steps = 0\n",
    "        # the step limit is the Manhattan distance between the start state and the terminal state\n",
    "        self.step_limit = sum(abs(np.array(self.terminal_state)-np.array(start_state)))\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def draw(self):\n",
    "\n",
    "        image = Image.new(\"RGBA\", (501, 501), (255, 255, 255, 255)) # white\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        w = 500/(self.width)\n",
    "        h = 500/(self.height)\n",
    "        color = (255, 255, 255, 0) # white, transparent\n",
    "\n",
    "        # use !fc-list or !fc-list | grep \"\" to get the path of the font-type on colab\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", 40) \n",
    "        \n",
    "        \n",
    "        #Draw Grid\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if(i == self.start_state[0] and j == self.start_state[1]):\n",
    "                    color = (0, 0, 255, 255)  # blue for the starting point\n",
    "                if(i == self.terminal_state[0] and j == self.terminal_state[1]):\n",
    "                    color = (255, 0, 0, 255)  # red for the max\n",
    "                draw.rectangle(((0+j*w, 0+i*h),(w+j*w, h+i*h)), outline = \"black\", fill = color)\n",
    "                text_h = font.getsize(str(self.reward_matrix[(i,j)]))[1]\n",
    "                text_w = font.getsize(str(self.reward_matrix[(i,j)]))[0]\n",
    "                draw.text((j*w + w/2 - text_w/2, i*h + h/2 - text_h/2), \n",
    "                          str(self.reward_matrix[(i,j)]), font=font, fill=(0, 0, 0, 255))\n",
    "                color = (255, 255, 255, 0)\n",
    "\n",
    "        #Draw Agent\n",
    "        agent_layer = Image.new('RGBA', (501, 501), (255, 255, 255, 0))\n",
    "        draw2 = ImageDraw.Draw(agent_layer)\n",
    "        draw2.ellipse((self.agent_pos[1]*w + w/4, self.agent_pos[0]*h + h/4, 3*w/4 + self.agent_pos[1]*w, 3*h/4 + self.agent_pos[0]*h), \n",
    "                     fill=(255, 0, 255, 128)) \n",
    "      \n",
    "        out = Image.alpha_composite(image, agent_layer)\n",
    "        display(out)\n",
    "\n",
    "      \n",
    "    def update_state(self,new_pos, if_illegal):\n",
    "        \"\"\"\n",
    "        new_pos - a point in the game grid that the agent has moved to\n",
    "        a function to update the position of agent, number of steps, and sum of rewards,\n",
    "        \"\"\"\n",
    "        if not if_illegal:\n",
    "            self.agent_pos = new_pos\n",
    "        self.n_steps += 1 # once the agent moves, the number of steps taken (var n_step) +1\n",
    "        # self.remaining_prize_states -= 1\n",
    "        self.sum += reward_matrix[self.agent_pos] \n",
    "\n",
    "\n",
    "    # def update_sum(self, new_pos):\n",
    "    #   \"\"\"\n",
    "    #   new_pos - a point in the game grid that the agent has moved to\n",
    "    #   This function updates the sum of rewards\n",
    "    #   \"\"\"\n",
    "    #   self.sum += reward_matrix[new_pos] \n",
    "\n",
    "\n",
    "    def is_episode_terminal(self):\n",
    "      # return True if self.n_steps == self.step_limit else False\n",
    "      return True if self.agent_pos == self.terminal_state else False\n",
    "\n",
    "\n",
    "    def refresh_game(self):\n",
    "        self.agent_pos = self.start_state\n",
    "        self.sum = self.reward_matrix[self.start_state] \n",
    "        self.n_steps = 0\n",
    "        # self.remaining_prize_states = self.step_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XIJSK11mq2Q"
   },
   "source": [
    "## MaxPath_Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lrIn74v4mxmU"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFont\n",
    "\n",
    "class MaxPath_Game(GridWorld):\n",
    "    \n",
    "    def __init__(self, board_dims, start_state, reward_matrix):\n",
    "        self.max_state = np.unravel_index(np.argmax(reward_matrix, axis=None), board_dims)\n",
    "        # no fixed terminal state, this game will terminate when the step limit is reached \n",
    "        GridWorld.__init__(self, board_dims, start_state, None, reward_matrix) \n",
    "        # self.max_value = reward_matrix[self.max_state]\n",
    "        self.reward_matrix = reward_matrix\n",
    "        # initial sum of rewards is the reward value at the start state\n",
    "        # self.sum = reward_matrix[start_state] \n",
    "        self.sum = 0 # initial sum of rewards is 0\n",
    "        # the number of steps\n",
    "        self.n_steps = 0\n",
    "        # the step limit is (board_width - 1)+(board_height - 1)\n",
    "        self.step_limit = (self.width - 1)+(self.height - 1)\n",
    "\n",
    "\n",
    "    def draw(self):\n",
    "\n",
    "        image = Image.new(\"RGBA\", (501, 501), (255, 255, 255, 255)) # white\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        w = 500/(self.width)\n",
    "        h = 500/(self.height)\n",
    "        color = (255, 255, 255, 0) # white, transparent\n",
    "\n",
    "        # use !fc-list or !fc-list | grep \"\" to get the path of the font-type on colab\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", 40) \n",
    "        \n",
    "        \n",
    "        #Draw Grid \n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if(i == self.start_state[0] and j == self.start_state[1]):\n",
    "                    color = (0, 0, 255, 255)  # blue for the starting point\n",
    "                if(i == self.max_state[0] and j == self.max_state[1]):\n",
    "                    color = (255, 0, 0, 255)  # red for the max\n",
    "                draw.rectangle(((0+j*w, 0+i*h),(w+j*w, h+i*h)), outline = \"black\", fill = color)\n",
    "                text_h = font.getsize(str(self.reward_matrix[(i,j)]))[1]\n",
    "                text_w = font.getsize(str(self.reward_matrix[(i,j)]))[0]\n",
    "                draw.text((j*w + w/2 - text_w/2, i*h + h/2 - text_h/2), \n",
    "                          str(self.reward_matrix[(i,j)]), font=font, fill=(0, 0, 0, 255))\n",
    "                color = (255, 255, 255, 0)\n",
    "\n",
    "        #Draw Agent\n",
    "        agent_layer = Image.new('RGBA', (501, 501), (255, 255, 255, 0))\n",
    "        draw2 = ImageDraw.Draw(agent_layer)\n",
    "        draw2.ellipse((self.agent_pos[1]*w + w/4, self.agent_pos[0]*h + h/4, 3*w/4 + self.agent_pos[1]*w, 3*h/4 + self.agent_pos[0]*h), \n",
    "                     fill=(255, 0, 255, 128)) \n",
    "      \n",
    "        out = Image.alpha_composite(image, agent_layer)\n",
    "        display(out)\n",
    "\n",
    "      \n",
    "    def update_state(self, new_pos, if_illegal):\n",
    "        \"\"\"\n",
    "        new_pos - a point in the game grid that the agent has moved to\n",
    "        a function to update the position of agent, number of steps, and sum of rewards,\n",
    "        \"\"\"\n",
    "        if not if_illegal:\n",
    "            self.agent_pos = new_pos\n",
    "\n",
    "        self.n_steps += 1 # once the agent moves, the number of steps taken (var n_step) +1\n",
    "        # self.remaining_prize_states -= 1\n",
    "        self.sum += reward_matrix[self.agent_pos] \n",
    "\n",
    "\n",
    "    # def update_sum(self, new_pos):\n",
    "    #   \"\"\"\n",
    "    #   new_pos - a point in the game grid that the agent has moved to\n",
    "    #   This function updates the sum of rewards\n",
    "    #   \"\"\"\n",
    "    #   self.sum += reward_matrix[new_pos] \n",
    "\n",
    "\n",
    "    def is_episode_terminal(self):\n",
    "        return True if self.n_steps >= self.step_limit else False\n",
    "\n",
    "\n",
    "    def refresh_game(self):\n",
    "        self.agent_pos = self.start_state\n",
    "        self.sum = self.reward_matrix[self.start_state] \n",
    "        self.n_steps = 0\n",
    "        # self.remaining_prize_states = self.step_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OPb4iNm2tFc"
   },
   "source": [
    "# Players and Agents\n",
    "\n",
    "Here we define different agents and allow them to play our games that we define above in the previous section\n",
    "\n",
    "For Tabular solving agents we've made Monte Carlo Agents, Temporal Difference Agents, and Planning Agents. Monte Carlo and Temporal Difference Agents learn strictly from real experience (playing the game) and the planning agent learns from both real experience and simulated experience (build a model of the game's reward matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XNoE9vPIEC-W"
   },
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"\n",
    "    This class implements the qtable object used for all tabular agent qtable classes\n",
    "      This class includes parameters that are used in every tabular agent\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha, epsilon, discount, alpha_decay_rate = 0.6, epsilon_decay_rate=1.0):\n",
    "        self.Game = Game\n",
    "        self.qtable = np.zeros([Game.height*Game.width, len(self.Game.moves)])\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha_decay_rate = alpha_decay_rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "\n",
    "    def alpha_decay(self):\n",
    "        self.alpha = self.alpha*self.alpha_decay_rate\n",
    "        \n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = self.epsilon*self.epsilon_decay_rate\n",
    "        \n",
    "    def get_return(self, state, action):\n",
    "        \"\"\"\n",
    "        state - some integer index corresponding to a tuple point in the grid game\n",
    "        action - some integer index corresponding to a possible action taken in the grid game\n",
    "        This function returns the expected return from the qtable for a specific action made in a specific state\n",
    "        \"\"\"\n",
    "        return self.qtable[state, action]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7KLLoROD-gW"
   },
   "source": [
    "## Monte Carlo\n",
    "\n",
    "Monte Carlo agents accumulate state,action,reward tuples for an entire episode before updating their action-value functions. We consider two cases of Monte Carlo RL agents: on-policy and off-policy. On-policy agents have one policy that determines the agent's behavior in the environment and converges to an optimal policy over time. Off-policy agents have two policies, one that the determines the agent's behavior in the environment (behavior policy) and one that corresponds to the agents optimal policy after sufficient experience (target policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xIxbTkYEggx"
   },
   "source": [
    "### MCAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_XR8K8AiEdpk"
   },
   "outputs": [],
   "source": [
    "class MCAgent:\n",
    "    \"\"\"\n",
    "    This class implements a Monte Carlo agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, model):\n",
    "        self.Game = Game\n",
    "        self.model = model\n",
    "        self.target_policy = None\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the target policy derived (greedily) from the qtable\n",
    "        \"\"\"\n",
    "        self.target_policy = self.model.target_policy\n",
    "\n",
    "    def print_models(self):\n",
    "        self.model.print_models()\n",
    "\n",
    "    def play_episode(self):\n",
    "        \"\"\"\n",
    "        This function plays an episode of the grid game and keeps track of the \n",
    "        states visited, actions made, and rewards obtained during the episode. \n",
    "        Once completion of the episode, we update our qtable model with the episode story\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        episode_reward = 0\n",
    "        episode_story = []\n",
    "        #while agent is not in a terminal state\n",
    "        while not self.Game.is_episode_terminal():\n",
    "            t += 1\n",
    "            #Take action A, get reward R, step into s'\n",
    "            \n",
    "            current_state = self.Game.agent_pos\n",
    "            #get possible moves going up, left, down, and right\n",
    "            new_action = self.model.get_action(self.Game.agent_pos)\n",
    "            new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "            \n",
    "            \n",
    "            #if move is illegal (going off the board), set reward to very bad\n",
    "            if self.Game.illegal_move(new_state):\n",
    "                reward = -100\n",
    "                self.Game.update_state(tuple(new_state), True)\n",
    "            else:\n",
    "            #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                reward = self.Game.get_reward(new_state)\n",
    "                self.Game.update_state(tuple(new_state), False)\n",
    "            \n",
    "            #convert to qtable indices\n",
    "            state = self.Game.point_to_index(current_state)\n",
    "            action = self.Game.moves.index(new_action)\n",
    "            \n",
    "            #add s_t, a_t, r_t+1 to the episode story\n",
    "            episode_story.append(tuple((state,action,reward)))\n",
    "            \n",
    "            episode_reward += reward\n",
    "        \n",
    "        self.model.update_model(episode_story)\n",
    "        self.model.alpha_decay()\n",
    "        self.model.epsilon_decay()\n",
    "        return episode_reward\n",
    "                \n",
    "    \n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "            episode_reward = self.play_episode()\n",
    "            player_scores.append(episode_reward)\n",
    "            self.Game.refresh_game()\n",
    "            \n",
    "            print(\"Reward for Episode: \",i,\" -> \",episode_reward)\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weTxyREaEBfO"
   },
   "source": [
    "### On-Policy First Visit QTable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kbwAyPdDEMQm"
   },
   "outputs": [],
   "source": [
    "class MC_OnPolicy_FirstVisit_QTable(QTable):\n",
    "    \"\"\"\n",
    "    This class represents the qtable with on-policy first visit Monte Carlo update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha=0.1, epsilon=0.5, discount=0.5, alpha_decay_rate = 0.9, epsilon_decay_rate=0.9):\n",
    "        QTable.__init__(self, Game, alpha, epsilon, discount, alpha_decay_rate, epsilon_decay_rate)\n",
    "        self.target_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        \n",
    "    def print_models(self):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        print(self.qtable)\n",
    "        print(self.target_policy)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.target_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "    \n",
    "    def update_model(self,episode_story):\n",
    "        \"\"\"\n",
    "        episode_story - a list of tuples corresponding to rewards obtained from \n",
    "          making a specific action from a specific state\n",
    "        This function updates the policy using on-policy first visit MC methods\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        \n",
    "        #for each state action reward starting from terminal state time step (T) - 1\n",
    "        for t, (state,action,reward) in enumerate(reversed(episode_story)):\n",
    "            G = self.discount*G + reward\n",
    "\n",
    "            if tuple((state,action,reward)) not in episode_story[0:len(episode_story)-t-1]: #first visit condition. if the tuple has not appeard in previous stages of episode, then update its qtable values\n",
    "                self.qtable[state,action] += self.alpha * (G - self.qtable[state,action])\n",
    "                \n",
    "                max_action_idx = np.argmax(self.qtable[state])\n",
    "                #update e-greedy policy\n",
    "                for a in range(0,len(self.Game.moves)):\n",
    "                    #update sub-optimal actions in state to be chosen with explore probability\n",
    "                    self.target_policy[state][a] = self.epsilon/len(self.Game.moves)\n",
    "                #update optimal action in state to be chosen with exploit probability + explore probability\n",
    "                self.target_policy[state][max_action_idx] = 1 - self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "            \n",
    "#             print(\"Reward for Time Step: \",len(episode_story)-t,\" -> \",G, \"\\t State-Action: \",state,\",\",action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO6e1ff9ETAi"
   },
   "source": [
    "### Off-Policy QTable Model\n",
    "\n",
    "Off-policy methods utilize this important idea of importance sampling. When generating experience and rewards from the environment from some behavior policy b(a|s), the expected reward will be slightly different for the behavior policy and the target policy. Thus to correct for this, we mulitply our reward by the ratio of the probability of seeing the episode's state action trajectory occur under each policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X0-KaPM_EWH7"
   },
   "outputs": [],
   "source": [
    "class MC_OffPolicy_QTable(QTable):\n",
    "    \"\"\"\n",
    "    This class represents the qtable with off-policy Monte Carlo update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha=0.1, epsilon=0.5, discount=0.5, alpha_decay_rate = 0.9, epsilon_decay_rate=0.9):\n",
    "        QTable.__init__(self, Game, alpha, epsilon, discount, alpha_decay_rate, epsilon_decay_rate)\n",
    "        self.target_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.behavior_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.cumul_weights = np.zeros_like(self.qtable)\n",
    "        \n",
    "    def print_models(self):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        print(self.qtable)\n",
    "        print(self.target_policy)\n",
    "        print(self.behavior_policy)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.behavior_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "    \n",
    "    def update_model(self,episode_story):\n",
    "        \"\"\"\n",
    "        episode_story - a list of tuples corresponding to rewards obtained from \n",
    "          making a specific action from a specific state\n",
    "        This function updates the policy using off-policy MC methods and importance sampling\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        #for each (state action reward) tuple starting from terminal state time step (T) - 1\n",
    "        for t, (state,action,reward) in enumerate(reversed(episode_story)):\n",
    "            G = self.discount*G + reward\n",
    "            self.cumul_weights[state,action] += W\n",
    "            self.qtable[state,action] += (W / self.cumul_weights[state,action]) * (G - self.qtable[state,action])\n",
    "\n",
    "            max_action_idx = np.argmax(self.qtable[state])\n",
    "\n",
    "            #update policy deterministically (greedily) w.r.t the qtable\n",
    "            for a in range(0,len(self.Game.moves)):\n",
    "                self.target_policy[state][a] = 0\n",
    "            self.target_policy[state][max_action_idx] = 1\n",
    "            \n",
    "            #update e-greedy policy\n",
    "            for a in range(0,len(self.Game.moves)):\n",
    "                #update sub-optimal actions in state to be chosen with explore probability\n",
    "                self.behavior_policy[state][a] = self.epsilon/len(self.Game.moves)\n",
    "            #update optimal action in state to be chosen with exploit probability + explore probability\n",
    "            self.behavior_policy[state][max_action_idx] = 1 - self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "            \n",
    "#             if action != max_action_idx:\n",
    "#                 break\n",
    "            \n",
    "            W = W / self.behavior_policy[state,action]\n",
    "            \n",
    "#             print(\"Reward for Time Step: \",len(episode_story)-t,\" -> \",G, \"\\t State-Action: \",state,\",\",action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnT_vA1p-dM0"
   },
   "source": [
    "## Temporal Difference\n",
    "\n",
    "Temporal Difference Agents utilize Monte Carlo and Dynamic Programming ideas. If we want to update are qtable before the episode ends, we utilize temporal difference learning. The idea is to lookahead only a finite number of steps, and only factor the next n-steps into your return estimate for time step t. The popular Q-Learning method is an example of a one-step temporal difference learning method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "499Xu0M4Fvew"
   },
   "source": [
    "### one-step TDAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JsDdM-yoFyRY"
   },
   "outputs": [],
   "source": [
    "class TDAgent:\n",
    "    \"\"\"\n",
    "    This class implements a one-step temporal difference agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, model):\n",
    "        self.Game = Game\n",
    "        self.model = model\n",
    "        self.target_policy = None\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the target policy derived from the qtable\n",
    "        \"\"\"\n",
    "        self.target_policy = self.model.target_policy\n",
    "\n",
    "    def print_models(self):\n",
    "        self.model.print_models()\n",
    "\n",
    "    def play_episode(self):\n",
    "        \"\"\"\n",
    "        This function plays an episode of the grid game and keeps track of the \n",
    "        states visited, actions made, and rewards obtained during each time step. \n",
    "        Once completion of a single time step, we update our qtable model with\n",
    "        current state, action made in current state, reward for making action in\n",
    "        current state, and the new state the agent moved to from making action.\n",
    "        \"\"\"\n",
    "\n",
    "        t = 0\n",
    "        episode_reward = 0\n",
    "        #while agent is not in a terminal state\n",
    "        while not self.Game.is_episode_terminal():\n",
    "            #Take action A, get reward R, step into s'\n",
    "\n",
    "            t += 1\n",
    "            current_state = self.Game.agent_pos\n",
    "            #get possible moves going up, left, down, and right\n",
    "            new_action = self.model.get_action(self.Game.agent_pos)\n",
    "            new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "            \n",
    "            #if move is illegal (going off the board), set reward to very bad\n",
    "            if self.Game.illegal_move(new_state):\n",
    "                reward = -100\n",
    "                self.Game.update_state(tuple(new_state), True)\n",
    "            else:\n",
    "            #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                reward = self.Game.get_reward(new_state)\n",
    "                self.Game.update_state(tuple(new_state), False)\n",
    "            \n",
    "            #convert to qtable indices\n",
    "            state = self.Game.point_to_index(current_state)\n",
    "            action = self.Game.moves.index(new_action)\n",
    "            new_state = self.Game.point_to_index(self.Game.agent_pos)\n",
    "\n",
    "            #pass s_t, a_t, r_t+1,s_t+1 to update the qtable\n",
    "            self.model.update_model(tuple((state,action,reward,new_state)))\n",
    "\n",
    "            episode_reward += reward\n",
    "        \n",
    "        \n",
    "        self.model.alpha_decay()\n",
    "        self.model.epsilon_decay()\n",
    "        return tuple((t,episode_reward))\n",
    "                \n",
    "    \n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        player_speeds = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "            t,episode_reward = self.play_episode()\n",
    "            player_scores.append(episode_reward)\n",
    "            player_speeds.append(t)\n",
    "            self.Game.refresh_game()\n",
    "            \n",
    "            \n",
    "            print(\"Reward for Episode: \",i,\" -> \",episode_reward, \"\\t time steps: \",t)\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()\n",
    "\n",
    "        plt.plot(np.arange(episodes),player_speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICSdKrSqSbPW"
   },
   "source": [
    "### n-step TDAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JUqbZbanSgU8"
   },
   "outputs": [],
   "source": [
    "class nstepTDAgent:\n",
    "    \"\"\"\n",
    "    This class implements an n-step temporal difference agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, model):\n",
    "        self.Game = Game\n",
    "        self.model = model\n",
    "        self.target_policy = None\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the target policy derived from the qtable\n",
    "        \"\"\"\n",
    "        self.target_policy = self.model.target_policy\n",
    "\n",
    "    def print_models(self):\n",
    "        self.model.print_models()\n",
    "\n",
    "    def play_episode(self):\n",
    "        \"\"\"\n",
    "        This function plays an episode of the grid game and keeps track of the \n",
    "        states visited, actions made, and rewards obtained during each time step\n",
    "        in their own unique lists. Once completion of a n time steps in the episode, \n",
    "        we update our qtable model every time step with recorded states, actions,\n",
    "        and rewards from the episode.\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        tau = 0\n",
    "        self.model.T = 10000\n",
    "        episode_reward = 0\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        episode_states = []\n",
    "\n",
    "        #make initial move\n",
    "        current_state = self.Game.agent_pos\n",
    "        new_action = self.model.get_action(self.Game.agent_pos)\n",
    "\n",
    "        state = self.Game.point_to_index(current_state)\n",
    "        action = self.Game.moves.index(new_action)\n",
    "\n",
    "        #append action_0,state_0,reward_0 to episode story lists\n",
    "        episode_actions.append(action)\n",
    "        episode_rewards.append(None)\n",
    "        episode_states.append(state)\n",
    "\n",
    "        #while agent is not in a terminal state\n",
    "        while tau != self.model.T - 1:\n",
    "            \n",
    "            if t < self.model.T:\n",
    "\n",
    "                #Take action A, get reward R, step into s'\n",
    "                new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "\n",
    "                #if move is illegal (going off the board), set reward to very bad\n",
    "                if self.Game.illegal_move(new_state):\n",
    "                    reward = -100\n",
    "                    self.Game.update_state(tuple(new_state), True)\n",
    "                else:\n",
    "                #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                    reward = self.Game.get_reward(new_state)\n",
    "                    self.Game.update_state(tuple(new_state), False)\n",
    "\n",
    "                #update story rewards and states\n",
    "                current_state = self.Game.agent_pos\n",
    "                state = self.Game.point_to_index(current_state)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_states.append(state)\n",
    "                \n",
    "                if self.Game.is_episode_terminal():\n",
    "                    #Obtain terminal time step\n",
    "                    self.model.T = t + 1\n",
    "                else:\n",
    "                    #get possible moves going up, left, down, and right\n",
    "                    new_action = self.model.get_action(self.Game.agent_pos)\n",
    "                    action = self.Game.moves.index(new_action)\n",
    "                    episode_actions.append(action)\n",
    "\n",
    "            #update model once n time steps occur so we can do n-step updates\n",
    "            tau = t - self.model.n + 1\n",
    "            if tau >= 0:\n",
    "                self.model.update_model(tau,episode_states,episode_actions,episode_rewards)\n",
    "\n",
    "            episode_reward += reward\n",
    "            t += 1\n",
    "        \n",
    "        self.model.alpha_decay()\n",
    "        self.model.epsilon_decay()\n",
    "        return episode_reward\n",
    "                \n",
    "    \n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "            episode_reward = self.play_episode()\n",
    "            player_scores.append(episode_reward)\n",
    "            self.Game.refresh_game()\n",
    "            \n",
    "            print(\"Reward for Episode: \",i,\" -> \",episode_reward)\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu2KFyASFywC"
   },
   "source": [
    "### Q-Learning QTable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "946lNpxgHaqm"
   },
   "outputs": [],
   "source": [
    "class QLearningQTable(QTable):\n",
    "    \"\"\"\n",
    "    This class represents the qtable with temporal difference qlearning update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha=0.1, epsilon=0.5, discount=0.5, alpha_decay_rate = 0.9, epsilon_decay_rate=0.9):\n",
    "        QTable.__init__(self, Game, alpha, epsilon, discount, alpha_decay_rate, epsilon_decay_rate)\n",
    "        self.target_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "\n",
    "    def print_models(self):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        print(self.qtable)\n",
    "        print(self.target_policy)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.target_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "    \n",
    "    def update_model(self,SARS):\n",
    "        \"\"\"\n",
    "        SARS - a tuple corresponding to state_t, action_t, reward_t, new_state_t+1 for time step t\n",
    "        This function updates the policy using temporal difference qlearning method\n",
    "        \"\"\"\n",
    "\n",
    "        state, action, reward, new_state = SARS\n",
    "        #obtain return value of best action taken in new_state\n",
    "        maxQ_new_state = np.max(self.qtable[new_state])\n",
    "        \n",
    "        #qlearning update rule\n",
    "        self.qtable[state,action] += self.alpha * (reward + self.discount*maxQ_new_state - self.qtable[state,action])\n",
    "\n",
    "        max_action_idx = np.argmax(self.qtable[state])\n",
    "        #update e-greedy policy\n",
    "        for a in range(0,len(self.Game.moves)):\n",
    "            #update sub-optimal actions in state to be chosen with explore probability\n",
    "            self.target_policy[state][a] = self.epsilon/len(self.Game.moves)\n",
    "        #update optimal action in state to be chosen with exploit probability + explore probability\n",
    "        self.target_policy[state][max_action_idx] = 1 - self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "        \n",
    "#             print(\"Reward for Time Step: \",len(episode_story)-t,\" -> \",G, \"\\t State-Action: \",state,\",\",action)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E_JIR9IGHwL"
   },
   "source": [
    "### n-step Sarsa QTable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q1tVkSTkGMjD"
   },
   "outputs": [],
   "source": [
    "class nstepSarsaQTable(QTable):\n",
    "    \"\"\"\n",
    "    This class represents the qtable with n-step temporal difference Sarsa update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, n, alpha=0.1, epsilon=0.5, discount=0.5, alpha_decay_rate = 0.9, epsilon_decay_rate=0.9):\n",
    "        QTable.__init__(self, Game, alpha, epsilon, discount, alpha_decay_rate, epsilon_decay_rate)\n",
    "        self.behavior_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.target_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.n = n\n",
    "        self.T = 10000\n",
    "\n",
    "    def print_models(self):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        print(self.qtable)\n",
    "        print(self.behavior_policy)\n",
    "        print(self.target_policy)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.behavior_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "    \n",
    "    def update_model(self,tau,states,actions,rewards):\n",
    "        \"\"\"\n",
    "        tau - an integer corresponding to the time step that we want to update (state_tau, action_tau) qvalue\n",
    "        states - a list of state qtable indices that represents the agent's state trajectory through an episode\n",
    "        actions - a list of action qtable indices that represents the agent's action trajectory through an episode\n",
    "        rewards - a list of rewards obtained by the agent during an episode\n",
    "        This function updates the policy using n-step TD Sarsa update rules\n",
    "        \"\"\"\n",
    "\n",
    "        #finds important sampling ratio\n",
    "        rho = np.prod([self.target_policy[states[i],actions[i]] / self.behavior_policy[states[i],actions[i]] \\\n",
    "                       for i in range(tau+1,1+np.min([tau+self.n,self.T-1]))])\n",
    "        \n",
    "        #finds return\n",
    "        G = np.sum([self.discount**(i-tau-1)*rewards[i] \\\n",
    "                    for i in range(tau+1,1+np.min([tau+self.n,self.T]))])\n",
    "\n",
    "        #if updated time step plus number of lookahead time steps is less than terminal time step, then add to return\n",
    "        if tau + self.n < self.T:\n",
    "            #G = G_[tau:tau+n]\n",
    "            G += self.discount**self.n * self.qtable[states[tau+self.n],actions[tau+self.n]]\n",
    "        \n",
    "        #n-step Sarsa update rule for state_tau, action_tau\n",
    "        self.qtable[states[tau],actions[tau]] += self.alpha * rho * (G - self.qtable[states[tau],actions[tau]])\n",
    "\n",
    "        max_action_idx = np.argmax(self.qtable[states[tau]])\n",
    "        #update e-greedy policy\n",
    "        for a in range(0,len(self.Game.moves)):\n",
    "            #update sub-optimal actions in state to be chosen with explore probability\n",
    "            self.target_policy[states[tau]][a] = self.epsilon/len(self.Game.moves)\n",
    "        #update optimal action in state to be chosen with exploit probability + explore probability\n",
    "        self.target_policy[states[tau]][max_action_idx] = 1 - self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "        \n",
    "#             print(\"Reward for Time Step: \",len(episode_story)-t,\" -> \",G, \"\\t State-Action: \",state,\",\",action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReNFpqxrGM6I"
   },
   "source": [
    "### n-step Tree Backup QTable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u0IrVyOoGeL5"
   },
   "outputs": [],
   "source": [
    "class nstepBackupQTable(QTable):\n",
    "    \"\"\"\n",
    "    This class represents the qtable with n-step temporal difference Tree Backup update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, n, alpha=0.1, epsilon=0.5, discount=0.5, alpha_decay_rate = 0.9, epsilon_decay_rate=0.9):\n",
    "        QTable.__init__(self, Game, alpha, epsilon, discount, alpha_decay_rate, epsilon_decay_rate)\n",
    "        self.behavior_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.target_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.n = n\n",
    "        self.T = 10000\n",
    "\n",
    "    def print_models(self):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        print(self.qtable)\n",
    "        print(self.behavior_policy)\n",
    "        print(self.target_policy)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.behavior_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "    \n",
    "    def update_model(self,tau,states,actions,rewards):\n",
    "        \"\"\"\n",
    "        tau - an integer corresponding to the time step that we want to update (state_tau, action_tau) qvalue\n",
    "        states - a list of state qtable indices that represents the agent's state trajectory through an episode\n",
    "        actions - a list of action qtable indices that represents the agent's action trajectory through an episode\n",
    "        rewards - a list of rewards obtained by the agent during an episode\n",
    "        This function updates the policy using n-step TD Sarsa update rules\n",
    "        \"\"\"\n",
    "        #if updated time step plus lookahead steps is greater than terminal time step, then get terminal reward\n",
    "        if tau + self.n >= self.T:\n",
    "            G = rewards[self.T]\n",
    "        else:\n",
    "            #else calculate the expected sarsa return for time step (tau+n-1)\n",
    "            G = rewards[tau+self.n] + self.discount * \\\n",
    "            np.sum([self.target_policy[states[tau+self.n],a]*self.qtable[states[tau+self.n],a] for a in range(0,len(self.Game.moves))])\n",
    "        \n",
    "        #add all expected sarsa returns at each time step k between tau and tau+n\n",
    "        for k in reversed(range(tau+1,np.min([tau+self.n-1,self.T-1]))):\n",
    "            G = rewards[k] + self.discount * \\\n",
    "            np.sum([self.target_policy[states[k],a]*self.qtable[states[k],a] for a in range(0,len(self.Game.moves)) if a != actions[k]]) \\\n",
    "            + self.discount * self.target_policy[states[k],actions[k]] * G\n",
    "        \n",
    "        #n-step tree backup update\n",
    "        self.qtable[states[tau],actions[tau]] += self.alpha * (G - self.qtable[states[tau],actions[tau]])\n",
    "\n",
    "        max_action_idx = np.argmax(self.qtable[states[tau]])\n",
    "        #update e-greedy policy\n",
    "        for a in range(0,len(self.Game.moves)):\n",
    "            #update sub-optimal actions in state to be chosen with explore probability\n",
    "            self.target_policy[states[tau]][a] = self.epsilon/len(self.Game.moves)\n",
    "        #update optimal action in state to be chosen with exploit probability + explore probability\n",
    "        self.target_policy[states[tau]][max_action_idx] = 1 - self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "        \n",
    "#             print(\"Reward for Time Step: \",len(episode_story)-t,\" -> \",G, \"\\t State-Action: \",state,\",\",action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYmt986Nb0eM"
   },
   "source": [
    "## DynaQ\n",
    "\n",
    "DynaQ is a planning agent which updates its qtable based on real experience and simulated experience derived from an environment model. This agent is more powerful because the way it updates its qtable isn't bounded by the actions taken during collection of real experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzRIKOuCJ8Ra"
   },
   "source": [
    "### DynaQAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n8wHrWiqJz1P"
   },
   "outputs": [],
   "source": [
    "class DynaQAgent:\n",
    "    \"\"\"\n",
    "    This class implements an integrated planning, acting, and learning agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, model):\n",
    "        self.Game = Game\n",
    "        self.model = model\n",
    "        self.target_policy = None\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the target policy derived from the qtable\n",
    "        \"\"\"\n",
    "        self.target_policy = self.model.target_policy\n",
    "\n",
    "    def print_models(self):\n",
    "        self.model.print_models()\n",
    "\n",
    "    def play_episode(self):\n",
    "        \"\"\"\n",
    "        This function plays an episode of the grid game and keeps track of the \n",
    "        states visited, actions made, and rewards obtained during each time step. \n",
    "        Once completion of a single time step, we update our qtable model with\n",
    "        current state, action made in current state, reward for making action in\n",
    "        current state, and the new state the agent moved to from making action.\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        episode_reward = 0\n",
    "        #while agent is not in a terminal state\n",
    "        while not self.Game.is_episode_terminal():\n",
    "            #Take action A, get reward R, step into s'\n",
    "\n",
    "            t += 1\n",
    "            current_state = self.Game.agent_pos\n",
    "            #get possible moves going up, left, down, and right\n",
    "            new_action = self.model.get_action(self.Game.agent_pos)\n",
    "            new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "            \n",
    "            #if move is illegal (going off the board), set reward to very bad\n",
    "            if self.Game.illegal_move(new_state):\n",
    "                reward = -100\n",
    "                self.Game.update_state(tuple(new_state), True)\n",
    "            else:\n",
    "                #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                reward = self.Game.get_reward(new_state)\n",
    "                self.Game.update_state(tuple(new_state), False)\n",
    "            \n",
    "            #convert to qtable indices\n",
    "            state = self.Game.point_to_index(current_state)\n",
    "            action = self.Game.moves.index(new_action)\n",
    "            new_state = self.Game.point_to_index(self.Game.agent_pos)\n",
    "\n",
    "            #add s_t, a_t, r_t+1, s_t+1 to the episode story\n",
    "            self.model.update_model(tuple((state,action,reward,new_state)))\n",
    "\n",
    "            episode_reward += reward\n",
    "        \n",
    "        \n",
    "        self.model.alpha_decay()\n",
    "        self.model.epsilon_decay()\n",
    "        return episode_reward\n",
    "                \n",
    "    \n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "            episode_reward = self.play_episode()\n",
    "            player_scores.append(episode_reward)\n",
    "            self.Game.refresh_game()\n",
    "            \n",
    "            print(\"Reward for Episode: \",i,\" -> \",episode_reward)\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQxu0MvaJ_ve"
   },
   "source": [
    "### DynaQTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Nj5keFeSKCFU"
   },
   "outputs": [],
   "source": [
    "class DynaQTable(QTable):\n",
    "    \"\"\"\n",
    "    This class represents the qtable with DynaQ update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, n, alpha=0.1, epsilon=0.5, discount=0.5, alpha_decay_rate = 0.9, epsilon_decay_rate=0.9):\n",
    "        QTable.__init__(self, Game, alpha, epsilon, discount, alpha_decay_rate, epsilon_decay_rate)\n",
    "        self.target_policy = np.ones_like(self.qtable) / len(self.Game.moves)\n",
    "        self.environment_model = np.zeros(self.qtable.shape,dtype = 'i,i') #model(s,a) = (r,s')\n",
    "        self.n = n\n",
    "        self.observed = []\n",
    "\n",
    "    def print_models(self):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        print(self.qtable)\n",
    "        print(self.target_policy)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.target_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "    \n",
    "    def update_model(self,SARS):\n",
    "        \"\"\"\n",
    "        SARS - a tuple corresponding to state_t, action_t, reward_t, new_state_t+1 for time step t\n",
    "        This function updates the policy using temporal difference qlearning method\n",
    "        \"\"\"\n",
    "\n",
    "        state, action, reward, new_state = SARS\n",
    "        maxQ_new_state = np.max(self.qtable[new_state])\n",
    "\n",
    "        self.qtable[state,action] += self.alpha * (reward + self.discount*maxQ_new_state - self.qtable[state,action])\n",
    "\n",
    "        self.environment_model[state,action] = (reward,new_state)\n",
    "\n",
    "        if tuple((state,action)) not in self.observed:\n",
    "            self.observed.append(tuple((state,action)))\n",
    "\n",
    "        #generate simulated experience from your model of the environment to update your qtable\n",
    "        for n in range(0,self.n):\n",
    "            #select random state action pair already observed\n",
    "            state,action = self.observed[random.randint(0,len(self.observed))]\n",
    "            #get model reward and next state from state action pair sampled\n",
    "            reward,new_state = self.environment_model[state,action]\n",
    "            maxQ_new_state = np.max(self.qtable[new_state])\n",
    "\n",
    "            #update qtable with dynaQ update rules (qlearning)\n",
    "            self.qtable[state,action] += self.alpha * (reward + self.discount*maxQ_new_state - self.qtable[state,action])\n",
    "\n",
    "        max_action_idx = np.argmax(self.qtable[state])\n",
    "        #update e-greedy policy\n",
    "        for a in range(0,len(self.Game.moves)):\n",
    "            #update sub-optimal actions in state to be chosen with explore probability\n",
    "            self.target_policy[state][a] = self.epsilon/len(self.Game.moves)\n",
    "        #update optimal action in state to be chosen with exploit probability + explore probability\n",
    "        self.target_policy[state][max_action_idx] = 1 - self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "        \n",
    "#             print(\"Reward for Time Step: \",len(episode_story)-t,\" -> \",G, \"\\t State-Action: \",state,\",\",action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG2zJqgV4pzc"
   },
   "source": [
    "# Testing\n",
    "\n",
    "This is where we make our game examples and test all the code from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCyx9Z3ZVJ6b"
   },
   "source": [
    "## Make New Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1VA1HKDPV7do"
   },
   "outputs": [],
   "source": [
    "def make_maxgame_grid(board_size):\n",
    "    n,m = board_size\n",
    "    rewards = list(np.arange(0,m*n))\n",
    "    reward_idxs = sample(rewards, len(rewards))\n",
    "\n",
    "    start_state_idx = reward_idxs.index(0)\n",
    "    reward_matrix = np.reshape(reward_idxs, (n,m))\n",
    "    start_state = (start_state_idx // m, start_state_idx % m)\n",
    "    \n",
    "    return board_size, start_state, reward_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJOfZlDA4yCu"
   },
   "source": [
    "## Target Game Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHOyKpW0En7i"
   },
   "source": [
    "### MC On-Policy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gMWDUdjyEq_c",
    "outputId": "e5fd9c2c-e4f6-410b-bd26-8db929683f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [-1. -1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7F92B52946D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for Episode:  0  ->  -1123.0\n",
      "Reward for Episode:  1  ->  -3539.0\n",
      "Reward for Episode:  2  ->  -103.0\n",
      "Reward for Episode:  3  ->  -105.0\n",
      "Reward for Episode:  4  ->  -1691.0\n",
      "Reward for Episode:  5  ->  -203.0\n",
      "Reward for Episode:  6  ->  -109.0\n",
      "Reward for Episode:  7  ->  -3.0\n",
      "Reward for Episode:  8  ->  -103.0\n",
      "Reward for Episode:  9  ->  -107.0\n",
      "Reward for Episode:  10  ->  -3.0\n",
      "Reward for Episode:  11  ->  -7.0\n",
      "Reward for Episode:  12  ->  -3.0\n",
      "Reward for Episode:  13  ->  -3.0\n",
      "Reward for Episode:  14  ->  -3.0\n",
      "Reward for Episode:  15  ->  -103.0\n",
      "Reward for Episode:  16  ->  -3.0\n",
      "Reward for Episode:  17  ->  -3.0\n",
      "Reward for Episode:  18  ->  -3.0\n",
      "Reward for Episode:  19  ->  -3.0\n",
      "Reward for Episode:  20  ->  -3.0\n",
      "Reward for Episode:  21  ->  -3.0\n",
      "Reward for Episode:  22  ->  -3.0\n",
      "Reward for Episode:  23  ->  -3.0\n",
      "Reward for Episode:  24  ->  -103.0\n",
      "Reward for Episode:  25  ->  -3.0\n",
      "Reward for Episode:  26  ->  -3.0\n",
      "Reward for Episode:  27  ->  -3.0\n",
      "Reward for Episode:  28  ->  -3.0\n",
      "Reward for Episode:  29  ->  -3.0\n",
      "Reward for Episode:  30  ->  -3.0\n",
      "Reward for Episode:  31  ->  -3.0\n",
      "Reward for Episode:  32  ->  -3.0\n",
      "Reward for Episode:  33  ->  -3.0\n",
      "Reward for Episode:  34  ->  -3.0\n",
      "Reward for Episode:  35  ->  -3.0\n",
      "Reward for Episode:  36  ->  -5.0\n",
      "Reward for Episode:  37  ->  -3.0\n",
      "Reward for Episode:  38  ->  -3.0\n",
      "Reward for Episode:  39  ->  -3.0\n",
      "Reward for Episode:  40  ->  -3.0\n",
      "Reward for Episode:  41  ->  -3.0\n",
      "Reward for Episode:  42  ->  -3.0\n",
      "Reward for Episode:  43  ->  -3.0\n",
      "Reward for Episode:  44  ->  -3.0\n",
      "Reward for Episode:  45  ->  -3.0\n",
      "Reward for Episode:  46  ->  -3.0\n",
      "Reward for Episode:  47  ->  -3.0\n",
      "Reward for Episode:  48  ->  -3.0\n",
      "Reward for Episode:  49  ->  -3.0\n",
      "Reward for Episode:  50  ->  -3.0\n",
      "Reward for Episode:  51  ->  -3.0\n",
      "Reward for Episode:  52  ->  -3.0\n",
      "Reward for Episode:  53  ->  -3.0\n",
      "Reward for Episode:  54  ->  -3.0\n",
      "Reward for Episode:  55  ->  -3.0\n",
      "Reward for Episode:  56  ->  -3.0\n",
      "Reward for Episode:  57  ->  -3.0\n",
      "Reward for Episode:  58  ->  -3.0\n",
      "Reward for Episode:  59  ->  -3.0\n",
      "Reward for Episode:  60  ->  -3.0\n",
      "Reward for Episode:  61  ->  -3.0\n",
      "Reward for Episode:  62  ->  -3.0\n",
      "Reward for Episode:  63  ->  -3.0\n",
      "Reward for Episode:  64  ->  -3.0\n",
      "Reward for Episode:  65  ->  -3.0\n",
      "Reward for Episode:  66  ->  -3.0\n",
      "Reward for Episode:  67  ->  -3.0\n",
      "Reward for Episode:  68  ->  -3.0\n",
      "Reward for Episode:  69  ->  -3.0\n",
      "Reward for Episode:  70  ->  -3.0\n",
      "Reward for Episode:  71  ->  -3.0\n",
      "Reward for Episode:  72  ->  -3.0\n",
      "Reward for Episode:  73  ->  -3.0\n",
      "Reward for Episode:  74  ->  -3.0\n",
      "Reward for Episode:  75  ->  -3.0\n",
      "Reward for Episode:  76  ->  -3.0\n",
      "Reward for Episode:  77  ->  -3.0\n",
      "Reward for Episode:  78  ->  -3.0\n",
      "Reward for Episode:  79  ->  -3.0\n",
      "Reward for Episode:  80  ->  -3.0\n",
      "Reward for Episode:  81  ->  -3.0\n",
      "Reward for Episode:  82  ->  -3.0\n",
      "Reward for Episode:  83  ->  -3.0\n",
      "Reward for Episode:  84  ->  -3.0\n",
      "Reward for Episode:  85  ->  -3.0\n",
      "Reward for Episode:  86  ->  -3.0\n",
      "Reward for Episode:  87  ->  -3.0\n",
      "Reward for Episode:  88  ->  -3.0\n",
      "Reward for Episode:  89  ->  -3.0\n",
      "Reward for Episode:  90  ->  -3.0\n",
      "Reward for Episode:  91  ->  -3.0\n",
      "Reward for Episode:  92  ->  -3.0\n",
      "Reward for Episode:  93  ->  -3.0\n",
      "Reward for Episode:  94  ->  -3.0\n",
      "Reward for Episode:  95  ->  -3.0\n",
      "Reward for Episode:  96  ->  -3.0\n",
      "Reward for Episode:  97  ->  -3.0\n",
      "Reward for Episode:  98  ->  -3.0\n",
      "Reward for Episode:  99  ->  -3.0\n",
      "Reward for Episode:  100  ->  -3.0\n",
      "Reward for Episode:  101  ->  -3.0\n",
      "Reward for Episode:  102  ->  -3.0\n",
      "Reward for Episode:  103  ->  -3.0\n",
      "Reward for Episode:  104  ->  -3.0\n",
      "Reward for Episode:  105  ->  -3.0\n",
      "Reward for Episode:  106  ->  -3.0\n",
      "Reward for Episode:  107  ->  -3.0\n",
      "Reward for Episode:  108  ->  -3.0\n",
      "Reward for Episode:  109  ->  -3.0\n",
      "Reward for Episode:  110  ->  -3.0\n",
      "Reward for Episode:  111  ->  -3.0\n",
      "Reward for Episode:  112  ->  -3.0\n",
      "Reward for Episode:  113  ->  -3.0\n",
      "Reward for Episode:  114  ->  -3.0\n",
      "Reward for Episode:  115  ->  -3.0\n",
      "Reward for Episode:  116  ->  -3.0\n",
      "Reward for Episode:  117  ->  -3.0\n",
      "Reward for Episode:  118  ->  -3.0\n",
      "Reward for Episode:  119  ->  -3.0\n",
      "Reward for Episode:  120  ->  -3.0\n",
      "Reward for Episode:  121  ->  -3.0\n",
      "Reward for Episode:  122  ->  -3.0\n",
      "Reward for Episode:  123  ->  -3.0\n",
      "Reward for Episode:  124  ->  -3.0\n",
      "Reward for Episode:  125  ->  -3.0\n",
      "Reward for Episode:  126  ->  -3.0\n",
      "Reward for Episode:  127  ->  -3.0\n",
      "Reward for Episode:  128  ->  -3.0\n",
      "Reward for Episode:  129  ->  -3.0\n",
      "Reward for Episode:  130  ->  -3.0\n",
      "Reward for Episode:  131  ->  -3.0\n",
      "Reward for Episode:  132  ->  -3.0\n",
      "Reward for Episode:  133  ->  -3.0\n",
      "Reward for Episode:  134  ->  -3.0\n",
      "Reward for Episode:  135  ->  -3.0\n",
      "Reward for Episode:  136  ->  -3.0\n",
      "Reward for Episode:  137  ->  -3.0\n",
      "Reward for Episode:  138  ->  -3.0\n",
      "Reward for Episode:  139  ->  -3.0\n",
      "Reward for Episode:  140  ->  -3.0\n",
      "Reward for Episode:  141  ->  -3.0\n",
      "Reward for Episode:  142  ->  -3.0\n",
      "Reward for Episode:  143  ->  -3.0\n",
      "Reward for Episode:  144  ->  -3.0\n",
      "Reward for Episode:  145  ->  -3.0\n",
      "Reward for Episode:  146  ->  -3.0\n",
      "Reward for Episode:  147  ->  -3.0\n",
      "Reward for Episode:  148  ->  -3.0\n",
      "Reward for Episode:  149  ->  -3.0\n",
      "Reward for Episode:  150  ->  -3.0\n",
      "Reward for Episode:  151  ->  -3.0\n",
      "Reward for Episode:  152  ->  -3.0\n",
      "Reward for Episode:  153  ->  -3.0\n",
      "Reward for Episode:  154  ->  -3.0\n",
      "Reward for Episode:  155  ->  -3.0\n",
      "Reward for Episode:  156  ->  -3.0\n",
      "Reward for Episode:  157  ->  -3.0\n",
      "Reward for Episode:  158  ->  -3.0\n",
      "Reward for Episode:  159  ->  -3.0\n",
      "Reward for Episode:  160  ->  -3.0\n",
      "Reward for Episode:  161  ->  -3.0\n",
      "Reward for Episode:  162  ->  -3.0\n",
      "Reward for Episode:  163  ->  -3.0\n",
      "Reward for Episode:  164  ->  -3.0\n",
      "Reward for Episode:  165  ->  -3.0\n",
      "Reward for Episode:  166  ->  -3.0\n",
      "Reward for Episode:  167  ->  -3.0\n",
      "Reward for Episode:  168  ->  -3.0\n",
      "Reward for Episode:  169  ->  -3.0\n",
      "Reward for Episode:  170  ->  -3.0\n",
      "Reward for Episode:  171  ->  -3.0\n",
      "Reward for Episode:  172  ->  -3.0\n",
      "Reward for Episode:  173  ->  -3.0\n",
      "Reward for Episode:  174  ->  -3.0\n",
      "Reward for Episode:  175  ->  -3.0\n",
      "Reward for Episode:  176  ->  -3.0\n",
      "Reward for Episode:  177  ->  -3.0\n",
      "Reward for Episode:  178  ->  -3.0\n",
      "Reward for Episode:  179  ->  -3.0\n",
      "Reward for Episode:  180  ->  -3.0\n",
      "Reward for Episode:  181  ->  -3.0\n",
      "Reward for Episode:  182  ->  -3.0\n",
      "Reward for Episode:  183  ->  -3.0\n",
      "Reward for Episode:  184  ->  -3.0\n",
      "Reward for Episode:  185  ->  -3.0\n",
      "Reward for Episode:  186  ->  -3.0\n",
      "Reward for Episode:  187  ->  -3.0\n",
      "Reward for Episode:  188  ->  -3.0\n",
      "Reward for Episode:  189  ->  -3.0\n",
      "Reward for Episode:  190  ->  -3.0\n",
      "Reward for Episode:  191  ->  -3.0\n",
      "Reward for Episode:  192  ->  -3.0\n",
      "Reward for Episode:  193  ->  -3.0\n",
      "Reward for Episode:  194  ->  -3.0\n",
      "Reward for Episode:  195  ->  -3.0\n",
      "Reward for Episode:  196  ->  -3.0\n",
      "Reward for Episode:  197  ->  -3.0\n",
      "Reward for Episode:  198  ->  -3.0\n",
      "Reward for Episode:  199  ->  -3.0\n",
      "Reward for Episode:  200  ->  -3.0\n",
      "Reward for Episode:  201  ->  -3.0\n",
      "Reward for Episode:  202  ->  -3.0\n",
      "Reward for Episode:  203  ->  -3.0\n",
      "Reward for Episode:  204  ->  -3.0\n",
      "Reward for Episode:  205  ->  -3.0\n",
      "Reward for Episode:  206  ->  -3.0\n",
      "Reward for Episode:  207  ->  -3.0\n",
      "Reward for Episode:  208  ->  -3.0\n",
      "Reward for Episode:  209  ->  -3.0\n",
      "Reward for Episode:  210  ->  -3.0\n",
      "Reward for Episode:  211  ->  -3.0\n",
      "Reward for Episode:  212  ->  -3.0\n",
      "Reward for Episode:  213  ->  -3.0\n",
      "Reward for Episode:  214  ->  -3.0\n",
      "Reward for Episode:  215  ->  -3.0\n",
      "Reward for Episode:  216  ->  -3.0\n",
      "Reward for Episode:  217  ->  -3.0\n",
      "Reward for Episode:  218  ->  -3.0\n",
      "Reward for Episode:  219  ->  -3.0\n",
      "Reward for Episode:  220  ->  -3.0\n",
      "Reward for Episode:  221  ->  -3.0\n",
      "Reward for Episode:  222  ->  -3.0\n",
      "Reward for Episode:  223  ->  -3.0\n",
      "Reward for Episode:  224  ->  -3.0\n",
      "Reward for Episode:  225  ->  -3.0\n",
      "Reward for Episode:  226  ->  -3.0\n",
      "Reward for Episode:  227  ->  -3.0\n",
      "Reward for Episode:  228  ->  -3.0\n",
      "Reward for Episode:  229  ->  -3.0\n",
      "Reward for Episode:  230  ->  -3.0\n",
      "Reward for Episode:  231  ->  -3.0\n",
      "Reward for Episode:  232  ->  -3.0\n",
      "Reward for Episode:  233  ->  -3.0\n",
      "Reward for Episode:  234  ->  -3.0\n",
      "Reward for Episode:  235  ->  -3.0\n",
      "Reward for Episode:  236  ->  -3.0\n",
      "Reward for Episode:  237  ->  -3.0\n",
      "Reward for Episode:  238  ->  -3.0\n",
      "Reward for Episode:  239  ->  -3.0\n",
      "Reward for Episode:  240  ->  -3.0\n",
      "Reward for Episode:  241  ->  -3.0\n",
      "Reward for Episode:  242  ->  -3.0\n",
      "Reward for Episode:  243  ->  -3.0\n",
      "Reward for Episode:  244  ->  -3.0\n",
      "Reward for Episode:  245  ->  -3.0\n",
      "Reward for Episode:  246  ->  -3.0\n",
      "Reward for Episode:  247  ->  -3.0\n",
      "Reward for Episode:  248  ->  -3.0\n",
      "Reward for Episode:  249  ->  -3.0\n",
      "Reward for Episode:  250  ->  -3.0\n",
      "Reward for Episode:  251  ->  -3.0\n",
      "Reward for Episode:  252  ->  -3.0\n",
      "Reward for Episode:  253  ->  -3.0\n",
      "Reward for Episode:  254  ->  -3.0\n",
      "Reward for Episode:  255  ->  -3.0\n",
      "Reward for Episode:  256  ->  -3.0\n",
      "Reward for Episode:  257  ->  -3.0\n",
      "Reward for Episode:  258  ->  -3.0\n",
      "Reward for Episode:  259  ->  -3.0\n",
      "Reward for Episode:  260  ->  -3.0\n",
      "Reward for Episode:  261  ->  -3.0\n",
      "Reward for Episode:  262  ->  -3.0\n",
      "Reward for Episode:  263  ->  -3.0\n",
      "Reward for Episode:  264  ->  -3.0\n",
      "Reward for Episode:  265  ->  -3.0\n",
      "Reward for Episode:  266  ->  -3.0\n",
      "Reward for Episode:  267  ->  -3.0\n",
      "Reward for Episode:  268  ->  -3.0\n",
      "Reward for Episode:  269  ->  -3.0\n",
      "Reward for Episode:  270  ->  -3.0\n",
      "Reward for Episode:  271  ->  -3.0\n",
      "Reward for Episode:  272  ->  -3.0\n",
      "Reward for Episode:  273  ->  -3.0\n",
      "Reward for Episode:  274  ->  -3.0\n",
      "Reward for Episode:  275  ->  -3.0\n",
      "Reward for Episode:  276  ->  -3.0\n",
      "Reward for Episode:  277  ->  -3.0\n",
      "Reward for Episode:  278  ->  -3.0\n",
      "Reward for Episode:  279  ->  -3.0\n",
      "Reward for Episode:  280  ->  -3.0\n",
      "Reward for Episode:  281  ->  -3.0\n",
      "Reward for Episode:  282  ->  -3.0\n",
      "Reward for Episode:  283  ->  -3.0\n",
      "Reward for Episode:  284  ->  -3.0\n",
      "Reward for Episode:  285  ->  -3.0\n",
      "Reward for Episode:  286  ->  -3.0\n",
      "Reward for Episode:  287  ->  -3.0\n",
      "Reward for Episode:  288  ->  -3.0\n",
      "Reward for Episode:  289  ->  -3.0\n",
      "Reward for Episode:  290  ->  -3.0\n",
      "Reward for Episode:  291  ->  -3.0\n",
      "Reward for Episode:  292  ->  -3.0\n",
      "Reward for Episode:  293  ->  -3.0\n",
      "Reward for Episode:  294  ->  -3.0\n",
      "Reward for Episode:  295  ->  -3.0\n",
      "Reward for Episode:  296  ->  -3.0\n",
      "Reward for Episode:  297  ->  -3.0\n",
      "Reward for Episode:  298  ->  -3.0\n",
      "Reward for Episode:  299  ->  -3.0\n",
      "Reward for Episode:  300  ->  -3.0\n",
      "Reward for Episode:  301  ->  -3.0\n",
      "Reward for Episode:  302  ->  -3.0\n",
      "Reward for Episode:  303  ->  -3.0\n",
      "Reward for Episode:  304  ->  -3.0\n",
      "Reward for Episode:  305  ->  -3.0\n",
      "Reward for Episode:  306  ->  -3.0\n",
      "Reward for Episode:  307  ->  -3.0\n",
      "Reward for Episode:  308  ->  -3.0\n",
      "Reward for Episode:  309  ->  -3.0\n",
      "Reward for Episode:  310  ->  -3.0\n",
      "Reward for Episode:  311  ->  -3.0\n",
      "Reward for Episode:  312  ->  -3.0\n",
      "Reward for Episode:  313  ->  -3.0\n",
      "Reward for Episode:  314  ->  -3.0\n",
      "Reward for Episode:  315  ->  -3.0\n",
      "Reward for Episode:  316  ->  -3.0\n",
      "Reward for Episode:  317  ->  -3.0\n",
      "Reward for Episode:  318  ->  -3.0\n",
      "Reward for Episode:  319  ->  -3.0\n",
      "Reward for Episode:  320  ->  -3.0\n",
      "Reward for Episode:  321  ->  -3.0\n",
      "Reward for Episode:  322  ->  -3.0\n",
      "Reward for Episode:  323  ->  -3.0\n",
      "Reward for Episode:  324  ->  -3.0\n",
      "Reward for Episode:  325  ->  -3.0\n",
      "Reward for Episode:  326  ->  -3.0\n",
      "Reward for Episode:  327  ->  -3.0\n",
      "Reward for Episode:  328  ->  -3.0\n",
      "Reward for Episode:  329  ->  -3.0\n",
      "Reward for Episode:  330  ->  -3.0\n",
      "Reward for Episode:  331  ->  -3.0\n",
      "Reward for Episode:  332  ->  -3.0\n",
      "Reward for Episode:  333  ->  -3.0\n",
      "Reward for Episode:  334  ->  -3.0\n",
      "Reward for Episode:  335  ->  -3.0\n",
      "Reward for Episode:  336  ->  -3.0\n",
      "Reward for Episode:  337  ->  -3.0\n",
      "Reward for Episode:  338  ->  -3.0\n",
      "Reward for Episode:  339  ->  -3.0\n",
      "Reward for Episode:  340  ->  -3.0\n",
      "Reward for Episode:  341  ->  -3.0\n",
      "Reward for Episode:  342  ->  -3.0\n",
      "Reward for Episode:  343  ->  -3.0\n",
      "Reward for Episode:  344  ->  -3.0\n",
      "Reward for Episode:  345  ->  -3.0\n",
      "Reward for Episode:  346  ->  -3.0\n",
      "Reward for Episode:  347  ->  -3.0\n",
      "Reward for Episode:  348  ->  -3.0\n",
      "Reward for Episode:  349  ->  -3.0\n",
      "Reward for Episode:  350  ->  -3.0\n",
      "Reward for Episode:  351  ->  -3.0\n",
      "Reward for Episode:  352  ->  -3.0\n",
      "Reward for Episode:  353  ->  -3.0\n",
      "Reward for Episode:  354  ->  -3.0\n",
      "Reward for Episode:  355  ->  -3.0\n",
      "Reward for Episode:  356  ->  -3.0\n",
      "Reward for Episode:  357  ->  -3.0\n",
      "Reward for Episode:  358  ->  -3.0\n",
      "Reward for Episode:  359  ->  -3.0\n",
      "Reward for Episode:  360  ->  -3.0\n",
      "Reward for Episode:  361  ->  -3.0\n",
      "Reward for Episode:  362  ->  -3.0\n",
      "Reward for Episode:  363  ->  -3.0\n",
      "Reward for Episode:  364  ->  -3.0\n",
      "Reward for Episode:  365  ->  -3.0\n",
      "Reward for Episode:  366  ->  -3.0\n",
      "Reward for Episode:  367  ->  -3.0\n",
      "Reward for Episode:  368  ->  -3.0\n",
      "Reward for Episode:  369  ->  -3.0\n",
      "Reward for Episode:  370  ->  -3.0\n",
      "Reward for Episode:  371  ->  -3.0\n",
      "Reward for Episode:  372  ->  -3.0\n",
      "Reward for Episode:  373  ->  -3.0\n",
      "Reward for Episode:  374  ->  -3.0\n",
      "Reward for Episode:  375  ->  -3.0\n",
      "Reward for Episode:  376  ->  -3.0\n",
      "Reward for Episode:  377  ->  -3.0\n",
      "Reward for Episode:  378  ->  -3.0\n",
      "Reward for Episode:  379  ->  -3.0\n",
      "Reward for Episode:  380  ->  -3.0\n",
      "Reward for Episode:  381  ->  -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for Episode:  382  ->  -3.0\n",
      "Reward for Episode:  383  ->  -3.0\n",
      "Reward for Episode:  384  ->  -3.0\n",
      "Reward for Episode:  385  ->  -3.0\n",
      "Reward for Episode:  386  ->  -3.0\n",
      "Reward for Episode:  387  ->  -3.0\n",
      "Reward for Episode:  388  ->  -3.0\n",
      "Reward for Episode:  389  ->  -3.0\n",
      "Reward for Episode:  390  ->  -3.0\n",
      "Reward for Episode:  391  ->  -3.0\n",
      "Reward for Episode:  392  ->  -3.0\n",
      "Reward for Episode:  393  ->  -3.0\n",
      "Reward for Episode:  394  ->  -3.0\n",
      "Reward for Episode:  395  ->  -3.0\n",
      "Reward for Episode:  396  ->  -3.0\n",
      "Reward for Episode:  397  ->  -3.0\n",
      "Reward for Episode:  398  ->  -3.0\n",
      "Reward for Episode:  399  ->  -3.0\n",
      "Reward for Episode:  400  ->  -3.0\n",
      "Reward for Episode:  401  ->  -3.0\n",
      "Reward for Episode:  402  ->  -3.0\n",
      "Reward for Episode:  403  ->  -3.0\n",
      "Reward for Episode:  404  ->  -3.0\n",
      "Reward for Episode:  405  ->  -3.0\n",
      "Reward for Episode:  406  ->  -3.0\n",
      "Reward for Episode:  407  ->  -3.0\n",
      "Reward for Episode:  408  ->  -3.0\n",
      "Reward for Episode:  409  ->  -3.0\n",
      "Reward for Episode:  410  ->  -3.0\n",
      "Reward for Episode:  411  ->  -3.0\n",
      "Reward for Episode:  412  ->  -3.0\n",
      "Reward for Episode:  413  ->  -3.0\n",
      "Reward for Episode:  414  ->  -3.0\n",
      "Reward for Episode:  415  ->  -3.0\n",
      "Reward for Episode:  416  ->  -3.0\n",
      "Reward for Episode:  417  ->  -3.0\n",
      "Reward for Episode:  418  ->  -3.0\n",
      "Reward for Episode:  419  ->  -3.0\n",
      "Reward for Episode:  420  ->  -3.0\n",
      "Reward for Episode:  421  ->  -3.0\n",
      "Reward for Episode:  422  ->  -3.0\n",
      "Reward for Episode:  423  ->  -3.0\n",
      "Reward for Episode:  424  ->  -3.0\n",
      "Reward for Episode:  425  ->  -3.0\n",
      "Reward for Episode:  426  ->  -3.0\n",
      "Reward for Episode:  427  ->  -3.0\n",
      "Reward for Episode:  428  ->  -3.0\n",
      "Reward for Episode:  429  ->  -3.0\n",
      "Reward for Episode:  430  ->  -3.0\n",
      "Reward for Episode:  431  ->  -3.0\n",
      "Reward for Episode:  432  ->  -3.0\n",
      "Reward for Episode:  433  ->  -3.0\n",
      "Reward for Episode:  434  ->  -3.0\n",
      "Reward for Episode:  435  ->  -3.0\n",
      "Reward for Episode:  436  ->  -3.0\n",
      "Reward for Episode:  437  ->  -3.0\n",
      "Reward for Episode:  438  ->  -3.0\n",
      "Reward for Episode:  439  ->  -3.0\n",
      "Reward for Episode:  440  ->  -3.0\n",
      "Reward for Episode:  441  ->  -3.0\n",
      "Reward for Episode:  442  ->  -3.0\n",
      "Reward for Episode:  443  ->  -3.0\n",
      "Reward for Episode:  444  ->  -3.0\n",
      "Reward for Episode:  445  ->  -3.0\n",
      "Reward for Episode:  446  ->  -3.0\n",
      "Reward for Episode:  447  ->  -3.0\n",
      "Reward for Episode:  448  ->  -3.0\n",
      "Reward for Episode:  449  ->  -3.0\n",
      "Reward for Episode:  450  ->  -3.0\n",
      "Reward for Episode:  451  ->  -3.0\n",
      "Reward for Episode:  452  ->  -3.0\n",
      "Reward for Episode:  453  ->  -3.0\n",
      "Reward for Episode:  454  ->  -3.0\n",
      "Reward for Episode:  455  ->  -3.0\n",
      "Reward for Episode:  456  ->  -3.0\n",
      "Reward for Episode:  457  ->  -3.0\n",
      "Reward for Episode:  458  ->  -3.0\n",
      "Reward for Episode:  459  ->  -3.0\n",
      "Reward for Episode:  460  ->  -3.0\n",
      "Reward for Episode:  461  ->  -3.0\n",
      "Reward for Episode:  462  ->  -3.0\n",
      "Reward for Episode:  463  ->  -3.0\n",
      "Reward for Episode:  464  ->  -3.0\n",
      "Reward for Episode:  465  ->  -3.0\n",
      "Reward for Episode:  466  ->  -3.0\n",
      "Reward for Episode:  467  ->  -3.0\n",
      "Reward for Episode:  468  ->  -3.0\n",
      "Reward for Episode:  469  ->  -3.0\n",
      "Reward for Episode:  470  ->  -3.0\n",
      "Reward for Episode:  471  ->  -3.0\n",
      "Reward for Episode:  472  ->  -3.0\n",
      "Reward for Episode:  473  ->  -3.0\n",
      "Reward for Episode:  474  ->  -3.0\n",
      "Reward for Episode:  475  ->  -3.0\n",
      "Reward for Episode:  476  ->  -3.0\n",
      "Reward for Episode:  477  ->  -3.0\n",
      "Reward for Episode:  478  ->  -3.0\n",
      "Reward for Episode:  479  ->  -3.0\n",
      "Reward for Episode:  480  ->  -3.0\n",
      "Reward for Episode:  481  ->  -3.0\n",
      "Reward for Episode:  482  ->  -3.0\n",
      "Reward for Episode:  483  ->  -3.0\n",
      "Reward for Episode:  484  ->  -3.0\n",
      "Reward for Episode:  485  ->  -3.0\n",
      "Reward for Episode:  486  ->  -3.0\n",
      "Reward for Episode:  487  ->  -3.0\n",
      "Reward for Episode:  488  ->  -3.0\n",
      "Reward for Episode:  489  ->  -3.0\n",
      "Reward for Episode:  490  ->  -3.0\n",
      "Reward for Episode:  491  ->  -3.0\n",
      "Reward for Episode:  492  ->  -3.0\n",
      "Reward for Episode:  493  ->  -3.0\n",
      "Reward for Episode:  494  ->  -3.0\n",
      "Reward for Episode:  495  ->  -3.0\n",
      "Reward for Episode:  496  ->  -3.0\n",
      "Reward for Episode:  497  ->  -3.0\n",
      "Reward for Episode:  498  ->  -3.0\n",
      "Reward for Episode:  499  ->  -3.0\n",
      "Reward for Episode:  500  ->  -3.0\n",
      "Reward for Episode:  501  ->  -3.0\n",
      "Reward for Episode:  502  ->  -3.0\n",
      "Reward for Episode:  503  ->  -3.0\n",
      "Reward for Episode:  504  ->  -3.0\n",
      "Reward for Episode:  505  ->  -3.0\n",
      "Reward for Episode:  506  ->  -3.0\n",
      "Reward for Episode:  507  ->  -3.0\n",
      "Reward for Episode:  508  ->  -3.0\n",
      "Reward for Episode:  509  ->  -3.0\n",
      "Reward for Episode:  510  ->  -3.0\n",
      "Reward for Episode:  511  ->  -3.0\n",
      "Reward for Episode:  512  ->  -3.0\n",
      "Reward for Episode:  513  ->  -3.0\n",
      "Reward for Episode:  514  ->  -3.0\n",
      "Reward for Episode:  515  ->  -3.0\n",
      "Reward for Episode:  516  ->  -3.0\n",
      "Reward for Episode:  517  ->  -3.0\n",
      "Reward for Episode:  518  ->  -3.0\n",
      "Reward for Episode:  519  ->  -3.0\n",
      "Reward for Episode:  520  ->  -3.0\n",
      "Reward for Episode:  521  ->  -3.0\n",
      "Reward for Episode:  522  ->  -3.0\n",
      "Reward for Episode:  523  ->  -3.0\n",
      "Reward for Episode:  524  ->  -3.0\n",
      "Reward for Episode:  525  ->  -3.0\n",
      "Reward for Episode:  526  ->  -3.0\n",
      "Reward for Episode:  527  ->  -3.0\n",
      "Reward for Episode:  528  ->  -3.0\n",
      "Reward for Episode:  529  ->  -3.0\n",
      "Reward for Episode:  530  ->  -3.0\n",
      "Reward for Episode:  531  ->  -3.0\n",
      "Reward for Episode:  532  ->  -3.0\n",
      "Reward for Episode:  533  ->  -3.0\n",
      "Reward for Episode:  534  ->  -3.0\n",
      "Reward for Episode:  535  ->  -3.0\n",
      "Reward for Episode:  536  ->  -3.0\n",
      "Reward for Episode:  537  ->  -3.0\n",
      "Reward for Episode:  538  ->  -3.0\n",
      "Reward for Episode:  539  ->  -3.0\n",
      "Reward for Episode:  540  ->  -3.0\n",
      "Reward for Episode:  541  ->  -3.0\n",
      "Reward for Episode:  542  ->  -3.0\n",
      "Reward for Episode:  543  ->  -3.0\n",
      "Reward for Episode:  544  ->  -3.0\n",
      "Reward for Episode:  545  ->  -3.0\n",
      "Reward for Episode:  546  ->  -3.0\n",
      "Reward for Episode:  547  ->  -3.0\n",
      "Reward for Episode:  548  ->  -3.0\n",
      "Reward for Episode:  549  ->  -3.0\n",
      "Reward for Episode:  550  ->  -3.0\n",
      "Reward for Episode:  551  ->  -3.0\n",
      "Reward for Episode:  552  ->  -3.0\n",
      "Reward for Episode:  553  ->  -3.0\n",
      "Reward for Episode:  554  ->  -3.0\n",
      "Reward for Episode:  555  ->  -3.0\n",
      "Reward for Episode:  556  ->  -3.0\n",
      "Reward for Episode:  557  ->  -3.0\n",
      "Reward for Episode:  558  ->  -3.0\n",
      "Reward for Episode:  559  ->  -3.0\n",
      "Reward for Episode:  560  ->  -3.0\n",
      "Reward for Episode:  561  ->  -3.0\n",
      "Reward for Episode:  562  ->  -3.0\n",
      "Reward for Episode:  563  ->  -3.0\n",
      "Reward for Episode:  564  ->  -3.0\n",
      "Reward for Episode:  565  ->  -3.0\n",
      "Reward for Episode:  566  ->  -3.0\n",
      "Reward for Episode:  567  ->  -3.0\n",
      "Reward for Episode:  568  ->  -3.0\n",
      "Reward for Episode:  569  ->  -3.0\n",
      "Reward for Episode:  570  ->  -3.0\n",
      "Reward for Episode:  571  ->  -3.0\n",
      "Reward for Episode:  572  ->  -3.0\n",
      "Reward for Episode:  573  ->  -3.0\n",
      "Reward for Episode:  574  ->  -3.0\n",
      "Reward for Episode:  575  ->  -3.0\n",
      "Reward for Episode:  576  ->  -3.0\n",
      "Reward for Episode:  577  ->  -3.0\n",
      "Reward for Episode:  578  ->  -3.0\n",
      "Reward for Episode:  579  ->  -3.0\n",
      "Reward for Episode:  580  ->  -3.0\n",
      "Reward for Episode:  581  ->  -3.0\n",
      "Reward for Episode:  582  ->  -3.0\n",
      "Reward for Episode:  583  ->  -3.0\n",
      "Reward for Episode:  584  ->  -3.0\n",
      "Reward for Episode:  585  ->  -3.0\n",
      "Reward for Episode:  586  ->  -3.0\n",
      "Reward for Episode:  587  ->  -3.0\n",
      "Reward for Episode:  588  ->  -3.0\n",
      "Reward for Episode:  589  ->  -3.0\n",
      "Reward for Episode:  590  ->  -3.0\n",
      "Reward for Episode:  591  ->  -3.0\n",
      "Reward for Episode:  592  ->  -3.0\n",
      "Reward for Episode:  593  ->  -3.0\n",
      "Reward for Episode:  594  ->  -3.0\n",
      "Reward for Episode:  595  ->  -3.0\n",
      "Reward for Episode:  596  ->  -3.0\n",
      "Reward for Episode:  597  ->  -3.0\n",
      "Reward for Episode:  598  ->  -3.0\n",
      "Reward for Episode:  599  ->  -3.0\n",
      "Reward for Episode:  600  ->  -3.0\n",
      "Reward for Episode:  601  ->  -3.0\n",
      "Reward for Episode:  602  ->  -3.0\n",
      "Reward for Episode:  603  ->  -3.0\n",
      "Reward for Episode:  604  ->  -3.0\n",
      "Reward for Episode:  605  ->  -3.0\n",
      "Reward for Episode:  606  ->  -3.0\n",
      "Reward for Episode:  607  ->  -3.0\n",
      "Reward for Episode:  608  ->  -3.0\n",
      "Reward for Episode:  609  ->  -3.0\n",
      "Reward for Episode:  610  ->  -3.0\n",
      "Reward for Episode:  611  ->  -3.0\n",
      "Reward for Episode:  612  ->  -3.0\n",
      "Reward for Episode:  613  ->  -3.0\n",
      "Reward for Episode:  614  ->  -3.0\n",
      "Reward for Episode:  615  ->  -3.0\n",
      "Reward for Episode:  616  ->  -3.0\n",
      "Reward for Episode:  617  ->  -3.0\n",
      "Reward for Episode:  618  ->  -3.0\n",
      "Reward for Episode:  619  ->  -3.0\n",
      "Reward for Episode:  620  ->  -3.0\n",
      "Reward for Episode:  621  ->  -3.0\n",
      "Reward for Episode:  622  ->  -3.0\n",
      "Reward for Episode:  623  ->  -3.0\n",
      "Reward for Episode:  624  ->  -3.0\n",
      "Reward for Episode:  625  ->  -3.0\n",
      "Reward for Episode:  626  ->  -3.0\n",
      "Reward for Episode:  627  ->  -3.0\n",
      "Reward for Episode:  628  ->  -3.0\n",
      "Reward for Episode:  629  ->  -3.0\n",
      "Reward for Episode:  630  ->  -3.0\n",
      "Reward for Episode:  631  ->  -3.0\n",
      "Reward for Episode:  632  ->  -3.0\n",
      "Reward for Episode:  633  ->  -3.0\n",
      "Reward for Episode:  634  ->  -3.0\n",
      "Reward for Episode:  635  ->  -3.0\n",
      "Reward for Episode:  636  ->  -3.0\n",
      "Reward for Episode:  637  ->  -3.0\n",
      "Reward for Episode:  638  ->  -3.0\n",
      "Reward for Episode:  639  ->  -3.0\n",
      "Reward for Episode:  640  ->  -3.0\n",
      "Reward for Episode:  641  ->  -3.0\n",
      "Reward for Episode:  642  ->  -3.0\n",
      "Reward for Episode:  643  ->  -3.0\n",
      "Reward for Episode:  644  ->  -3.0\n",
      "Reward for Episode:  645  ->  -3.0\n",
      "Reward for Episode:  646  ->  -3.0\n",
      "Reward for Episode:  647  ->  -3.0\n",
      "Reward for Episode:  648  ->  -3.0\n",
      "Reward for Episode:  649  ->  -3.0\n",
      "Reward for Episode:  650  ->  -3.0\n",
      "Reward for Episode:  651  ->  -3.0\n",
      "Reward for Episode:  652  ->  -3.0\n",
      "Reward for Episode:  653  ->  -3.0\n",
      "Reward for Episode:  654  ->  -3.0\n",
      "Reward for Episode:  655  ->  -3.0\n",
      "Reward for Episode:  656  ->  -3.0\n",
      "Reward for Episode:  657  ->  -3.0\n",
      "Reward for Episode:  658  ->  -3.0\n",
      "Reward for Episode:  659  ->  -3.0\n",
      "Reward for Episode:  660  ->  -3.0\n",
      "Reward for Episode:  661  ->  -3.0\n",
      "Reward for Episode:  662  ->  -3.0\n",
      "Reward for Episode:  663  ->  -3.0\n",
      "Reward for Episode:  664  ->  -3.0\n",
      "Reward for Episode:  665  ->  -3.0\n",
      "Reward for Episode:  666  ->  -3.0\n",
      "Reward for Episode:  667  ->  -3.0\n",
      "Reward for Episode:  668  ->  -3.0\n",
      "Reward for Episode:  669  ->  -3.0\n",
      "Reward for Episode:  670  ->  -3.0\n",
      "Reward for Episode:  671  ->  -3.0\n",
      "Reward for Episode:  672  ->  -3.0\n",
      "Reward for Episode:  673  ->  -3.0\n",
      "Reward for Episode:  674  ->  -3.0\n",
      "Reward for Episode:  675  ->  -3.0\n",
      "Reward for Episode:  676  ->  -3.0\n",
      "Reward for Episode:  677  ->  -3.0\n",
      "Reward for Episode:  678  ->  -3.0\n",
      "Reward for Episode:  679  ->  -3.0\n",
      "Reward for Episode:  680  ->  -3.0\n",
      "Reward for Episode:  681  ->  -3.0\n",
      "Reward for Episode:  682  ->  -3.0\n",
      "Reward for Episode:  683  ->  -3.0\n",
      "Reward for Episode:  684  ->  -3.0\n",
      "Reward for Episode:  685  ->  -3.0\n",
      "Reward for Episode:  686  ->  -3.0\n",
      "Reward for Episode:  687  ->  -3.0\n",
      "Reward for Episode:  688  ->  -3.0\n",
      "Reward for Episode:  689  ->  -3.0\n",
      "Reward for Episode:  690  ->  -3.0\n",
      "Reward for Episode:  691  ->  -3.0\n",
      "Reward for Episode:  692  ->  -3.0\n",
      "Reward for Episode:  693  ->  -3.0\n",
      "Reward for Episode:  694  ->  -3.0\n",
      "Reward for Episode:  695  ->  -3.0\n",
      "Reward for Episode:  696  ->  -3.0\n",
      "Reward for Episode:  697  ->  -3.0\n",
      "Reward for Episode:  698  ->  -3.0\n",
      "Reward for Episode:  699  ->  -3.0\n",
      "Reward for Episode:  700  ->  -3.0\n",
      "Reward for Episode:  701  ->  -3.0\n",
      "Reward for Episode:  702  ->  -3.0\n",
      "Reward for Episode:  703  ->  -3.0\n",
      "Reward for Episode:  704  ->  -3.0\n",
      "Reward for Episode:  705  ->  -3.0\n",
      "Reward for Episode:  706  ->  -3.0\n",
      "Reward for Episode:  707  ->  -3.0\n",
      "Reward for Episode:  708  ->  -3.0\n",
      "Reward for Episode:  709  ->  -3.0\n",
      "Reward for Episode:  710  ->  -3.0\n",
      "Reward for Episode:  711  ->  -3.0\n",
      "Reward for Episode:  712  ->  -3.0\n",
      "Reward for Episode:  713  ->  -3.0\n",
      "Reward for Episode:  714  ->  -3.0\n",
      "Reward for Episode:  715  ->  -3.0\n",
      "Reward for Episode:  716  ->  -3.0\n",
      "Reward for Episode:  717  ->  -3.0\n",
      "Reward for Episode:  718  ->  -3.0\n",
      "Reward for Episode:  719  ->  -3.0\n",
      "Reward for Episode:  720  ->  -3.0\n",
      "Reward for Episode:  721  ->  -3.0\n",
      "Reward for Episode:  722  ->  -3.0\n",
      "Reward for Episode:  723  ->  -3.0\n",
      "Reward for Episode:  724  ->  -3.0\n",
      "Reward for Episode:  725  ->  -3.0\n",
      "Reward for Episode:  726  ->  -3.0\n",
      "Reward for Episode:  727  ->  -3.0\n",
      "Reward for Episode:  728  ->  -3.0\n",
      "Reward for Episode:  729  ->  -3.0\n",
      "Reward for Episode:  730  ->  -3.0\n",
      "Reward for Episode:  731  ->  -3.0\n",
      "Reward for Episode:  732  ->  -3.0\n",
      "Reward for Episode:  733  ->  -3.0\n",
      "Reward for Episode:  734  ->  -3.0\n",
      "Reward for Episode:  735  ->  -3.0\n",
      "Reward for Episode:  736  ->  -3.0\n",
      "Reward for Episode:  737  ->  -3.0\n",
      "Reward for Episode:  738  ->  -3.0\n",
      "Reward for Episode:  739  ->  -3.0\n",
      "Reward for Episode:  740  ->  -3.0\n",
      "Reward for Episode:  741  ->  -3.0\n",
      "Reward for Episode:  742  ->  -3.0\n",
      "Reward for Episode:  743  ->  -3.0\n",
      "Reward for Episode:  744  ->  -3.0\n",
      "Reward for Episode:  745  ->  -3.0\n",
      "Reward for Episode:  746  ->  -3.0\n",
      "Reward for Episode:  747  ->  -3.0\n",
      "Reward for Episode:  748  ->  -3.0\n",
      "Reward for Episode:  749  ->  -3.0\n",
      "Reward for Episode:  750  ->  -3.0\n",
      "Reward for Episode:  751  ->  -3.0\n",
      "Reward for Episode:  752  ->  -3.0\n",
      "Reward for Episode:  753  ->  -3.0\n",
      "Reward for Episode:  754  ->  -3.0\n",
      "Reward for Episode:  755  ->  -3.0\n",
      "Reward for Episode:  756  ->  -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for Episode:  757  ->  -3.0\n",
      "Reward for Episode:  758  ->  -3.0\n",
      "Reward for Episode:  759  ->  -3.0\n",
      "Reward for Episode:  760  ->  -3.0\n",
      "Reward for Episode:  761  ->  -3.0\n",
      "Reward for Episode:  762  ->  -3.0\n",
      "Reward for Episode:  763  ->  -3.0\n",
      "Reward for Episode:  764  ->  -3.0\n",
      "Reward for Episode:  765  ->  -3.0\n",
      "Reward for Episode:  766  ->  -3.0\n",
      "Reward for Episode:  767  ->  -3.0\n",
      "Reward for Episode:  768  ->  -3.0\n",
      "Reward for Episode:  769  ->  -3.0\n",
      "Reward for Episode:  770  ->  -3.0\n",
      "Reward for Episode:  771  ->  -3.0\n",
      "Reward for Episode:  772  ->  -3.0\n",
      "Reward for Episode:  773  ->  -3.0\n",
      "Reward for Episode:  774  ->  -3.0\n",
      "Reward for Episode:  775  ->  -3.0\n",
      "Reward for Episode:  776  ->  -3.0\n",
      "Reward for Episode:  777  ->  -3.0\n",
      "Reward for Episode:  778  ->  -3.0\n",
      "Reward for Episode:  779  ->  -3.0\n",
      "Reward for Episode:  780  ->  -3.0\n",
      "Reward for Episode:  781  ->  -3.0\n",
      "Reward for Episode:  782  ->  -3.0\n",
      "Reward for Episode:  783  ->  -3.0\n",
      "Reward for Episode:  784  ->  -3.0\n",
      "Reward for Episode:  785  ->  -3.0\n",
      "Reward for Episode:  786  ->  -3.0\n",
      "Reward for Episode:  787  ->  -3.0\n",
      "Reward for Episode:  788  ->  -3.0\n",
      "Reward for Episode:  789  ->  -3.0\n",
      "Reward for Episode:  790  ->  -3.0\n",
      "Reward for Episode:  791  ->  -3.0\n",
      "Reward for Episode:  792  ->  -3.0\n",
      "Reward for Episode:  793  ->  -3.0\n",
      "Reward for Episode:  794  ->  -3.0\n",
      "Reward for Episode:  795  ->  -3.0\n",
      "Reward for Episode:  796  ->  -3.0\n",
      "Reward for Episode:  797  ->  -3.0\n",
      "Reward for Episode:  798  ->  -3.0\n",
      "Reward for Episode:  799  ->  -3.0\n",
      "Reward for Episode:  800  ->  -3.0\n",
      "Reward for Episode:  801  ->  -3.0\n",
      "Reward for Episode:  802  ->  -3.0\n",
      "Reward for Episode:  803  ->  -3.0\n",
      "Reward for Episode:  804  ->  -3.0\n",
      "Reward for Episode:  805  ->  -3.0\n",
      "Reward for Episode:  806  ->  -3.0\n",
      "Reward for Episode:  807  ->  -3.0\n",
      "Reward for Episode:  808  ->  -3.0\n",
      "Reward for Episode:  809  ->  -3.0\n",
      "Reward for Episode:  810  ->  -3.0\n",
      "Reward for Episode:  811  ->  -3.0\n",
      "Reward for Episode:  812  ->  -3.0\n",
      "Reward for Episode:  813  ->  -3.0\n",
      "Reward for Episode:  814  ->  -3.0\n",
      "Reward for Episode:  815  ->  -3.0\n",
      "Reward for Episode:  816  ->  -3.0\n",
      "Reward for Episode:  817  ->  -3.0\n",
      "Reward for Episode:  818  ->  -3.0\n",
      "Reward for Episode:  819  ->  -3.0\n",
      "Reward for Episode:  820  ->  -3.0\n",
      "Reward for Episode:  821  ->  -3.0\n",
      "Reward for Episode:  822  ->  -3.0\n",
      "Reward for Episode:  823  ->  -3.0\n",
      "Reward for Episode:  824  ->  -3.0\n",
      "Reward for Episode:  825  ->  -3.0\n",
      "Reward for Episode:  826  ->  -3.0\n",
      "Reward for Episode:  827  ->  -3.0\n",
      "Reward for Episode:  828  ->  -3.0\n",
      "Reward for Episode:  829  ->  -3.0\n",
      "Reward for Episode:  830  ->  -3.0\n",
      "Reward for Episode:  831  ->  -3.0\n",
      "Reward for Episode:  832  ->  -3.0\n",
      "Reward for Episode:  833  ->  -3.0\n",
      "Reward for Episode:  834  ->  -3.0\n",
      "Reward for Episode:  835  ->  -3.0\n",
      "Reward for Episode:  836  ->  -3.0\n",
      "Reward for Episode:  837  ->  -3.0\n",
      "Reward for Episode:  838  ->  -3.0\n",
      "Reward for Episode:  839  ->  -3.0\n",
      "Reward for Episode:  840  ->  -3.0\n",
      "Reward for Episode:  841  ->  -3.0\n",
      "Reward for Episode:  842  ->  -3.0\n",
      "Reward for Episode:  843  ->  -3.0\n",
      "Reward for Episode:  844  ->  -3.0\n",
      "Reward for Episode:  845  ->  -3.0\n",
      "Reward for Episode:  846  ->  -3.0\n",
      "Reward for Episode:  847  ->  -3.0\n",
      "Reward for Episode:  848  ->  -3.0\n",
      "Reward for Episode:  849  ->  -3.0\n",
      "Reward for Episode:  850  ->  -3.0\n",
      "Reward for Episode:  851  ->  -3.0\n",
      "Reward for Episode:  852  ->  -3.0\n",
      "Reward for Episode:  853  ->  -3.0\n",
      "Reward for Episode:  854  ->  -3.0\n",
      "Reward for Episode:  855  ->  -3.0\n",
      "Reward for Episode:  856  ->  -3.0\n",
      "Reward for Episode:  857  ->  -3.0\n",
      "Reward for Episode:  858  ->  -3.0\n",
      "Reward for Episode:  859  ->  -3.0\n",
      "Reward for Episode:  860  ->  -3.0\n",
      "Reward for Episode:  861  ->  -3.0\n",
      "Reward for Episode:  862  ->  -3.0\n",
      "Reward for Episode:  863  ->  -3.0\n",
      "Reward for Episode:  864  ->  -3.0\n",
      "Reward for Episode:  865  ->  -3.0\n",
      "Reward for Episode:  866  ->  -3.0\n",
      "Reward for Episode:  867  ->  -3.0\n",
      "Reward for Episode:  868  ->  -3.0\n",
      "Reward for Episode:  869  ->  -3.0\n",
      "Reward for Episode:  870  ->  -3.0\n",
      "Reward for Episode:  871  ->  -3.0\n",
      "Reward for Episode:  872  ->  -3.0\n",
      "Reward for Episode:  873  ->  -3.0\n",
      "Reward for Episode:  874  ->  -3.0\n",
      "Reward for Episode:  875  ->  -3.0\n",
      "Reward for Episode:  876  ->  -3.0\n",
      "Reward for Episode:  877  ->  -3.0\n",
      "Reward for Episode:  878  ->  -3.0\n",
      "Reward for Episode:  879  ->  -3.0\n",
      "Reward for Episode:  880  ->  -3.0\n",
      "Reward for Episode:  881  ->  -3.0\n",
      "Reward for Episode:  882  ->  -3.0\n",
      "Reward for Episode:  883  ->  -3.0\n",
      "Reward for Episode:  884  ->  -3.0\n",
      "Reward for Episode:  885  ->  -3.0\n",
      "Reward for Episode:  886  ->  -3.0\n",
      "Reward for Episode:  887  ->  -3.0\n",
      "Reward for Episode:  888  ->  -3.0\n",
      "Reward for Episode:  889  ->  -3.0\n",
      "Reward for Episode:  890  ->  -3.0\n",
      "Reward for Episode:  891  ->  -3.0\n",
      "Reward for Episode:  892  ->  -3.0\n",
      "Reward for Episode:  893  ->  -3.0\n",
      "Reward for Episode:  894  ->  -3.0\n",
      "Reward for Episode:  895  ->  -3.0\n",
      "Reward for Episode:  896  ->  -3.0\n",
      "Reward for Episode:  897  ->  -3.0\n",
      "Reward for Episode:  898  ->  -3.0\n",
      "Reward for Episode:  899  ->  -3.0\n",
      "Reward for Episode:  900  ->  -3.0\n",
      "Reward for Episode:  901  ->  -3.0\n",
      "Reward for Episode:  902  ->  -3.0\n",
      "Reward for Episode:  903  ->  -3.0\n",
      "Reward for Episode:  904  ->  -3.0\n",
      "Reward for Episode:  905  ->  -3.0\n",
      "Reward for Episode:  906  ->  -3.0\n",
      "Reward for Episode:  907  ->  -3.0\n",
      "Reward for Episode:  908  ->  -3.0\n",
      "Reward for Episode:  909  ->  -3.0\n",
      "Reward for Episode:  910  ->  -3.0\n",
      "Reward for Episode:  911  ->  -3.0\n",
      "Reward for Episode:  912  ->  -3.0\n",
      "Reward for Episode:  913  ->  -3.0\n",
      "Reward for Episode:  914  ->  -3.0\n",
      "Reward for Episode:  915  ->  -3.0\n",
      "Reward for Episode:  916  ->  -3.0\n",
      "Reward for Episode:  917  ->  -3.0\n",
      "Reward for Episode:  918  ->  -3.0\n",
      "Reward for Episode:  919  ->  -3.0\n",
      "Reward for Episode:  920  ->  -3.0\n",
      "Reward for Episode:  921  ->  -3.0\n",
      "Reward for Episode:  922  ->  -3.0\n",
      "Reward for Episode:  923  ->  -3.0\n",
      "Reward for Episode:  924  ->  -3.0\n",
      "Reward for Episode:  925  ->  -3.0\n",
      "Reward for Episode:  926  ->  -3.0\n",
      "Reward for Episode:  927  ->  -3.0\n",
      "Reward for Episode:  928  ->  -3.0\n",
      "Reward for Episode:  929  ->  -3.0\n",
      "Reward for Episode:  930  ->  -3.0\n",
      "Reward for Episode:  931  ->  -3.0\n",
      "Reward for Episode:  932  ->  -3.0\n",
      "Reward for Episode:  933  ->  -3.0\n",
      "Reward for Episode:  934  ->  -3.0\n",
      "Reward for Episode:  935  ->  -3.0\n",
      "Reward for Episode:  936  ->  -3.0\n",
      "Reward for Episode:  937  ->  -3.0\n",
      "Reward for Episode:  938  ->  -3.0\n",
      "Reward for Episode:  939  ->  -3.0\n",
      "Reward for Episode:  940  ->  -3.0\n",
      "Reward for Episode:  941  ->  -3.0\n",
      "Reward for Episode:  942  ->  -3.0\n",
      "Reward for Episode:  943  ->  -3.0\n",
      "Reward for Episode:  944  ->  -3.0\n",
      "Reward for Episode:  945  ->  -3.0\n",
      "Reward for Episode:  946  ->  -3.0\n",
      "Reward for Episode:  947  ->  -3.0\n",
      "Reward for Episode:  948  ->  -3.0\n",
      "Reward for Episode:  949  ->  -3.0\n",
      "Reward for Episode:  950  ->  -3.0\n",
      "Reward for Episode:  951  ->  -3.0\n",
      "Reward for Episode:  952  ->  -3.0\n",
      "Reward for Episode:  953  ->  -3.0\n",
      "Reward for Episode:  954  ->  -3.0\n",
      "Reward for Episode:  955  ->  -3.0\n",
      "Reward for Episode:  956  ->  -3.0\n",
      "Reward for Episode:  957  ->  -3.0\n",
      "Reward for Episode:  958  ->  -3.0\n",
      "Reward for Episode:  959  ->  -3.0\n",
      "Reward for Episode:  960  ->  -3.0\n",
      "Reward for Episode:  961  ->  -3.0\n",
      "Reward for Episode:  962  ->  -3.0\n",
      "Reward for Episode:  963  ->  -3.0\n",
      "Reward for Episode:  964  ->  -3.0\n",
      "Reward for Episode:  965  ->  -3.0\n",
      "Reward for Episode:  966  ->  -3.0\n",
      "Reward for Episode:  967  ->  -3.0\n",
      "Reward for Episode:  968  ->  -3.0\n",
      "Reward for Episode:  969  ->  -3.0\n",
      "Reward for Episode:  970  ->  -3.0\n",
      "Reward for Episode:  971  ->  -3.0\n",
      "Reward for Episode:  972  ->  -3.0\n",
      "Reward for Episode:  973  ->  -3.0\n",
      "Reward for Episode:  974  ->  -3.0\n",
      "Reward for Episode:  975  ->  -3.0\n",
      "Reward for Episode:  976  ->  -3.0\n",
      "Reward for Episode:  977  ->  -3.0\n",
      "Reward for Episode:  978  ->  -3.0\n",
      "Reward for Episode:  979  ->  -3.0\n",
      "Reward for Episode:  980  ->  -3.0\n",
      "Reward for Episode:  981  ->  -3.0\n",
      "Reward for Episode:  982  ->  -3.0\n",
      "Reward for Episode:  983  ->  -3.0\n",
      "Reward for Episode:  984  ->  -3.0\n",
      "Reward for Episode:  985  ->  -3.0\n",
      "Reward for Episode:  986  ->  -3.0\n",
      "Reward for Episode:  987  ->  -3.0\n",
      "Reward for Episode:  988  ->  -3.0\n",
      "Reward for Episode:  989  ->  -3.0\n",
      "Reward for Episode:  990  ->  -3.0\n",
      "Reward for Episode:  991  ->  -3.0\n",
      "Reward for Episode:  992  ->  -3.0\n",
      "Reward for Episode:  993  ->  -3.0\n",
      "Reward for Episode:  994  ->  -3.0\n",
      "Reward for Episode:  995  ->  -3.0\n",
      "Reward for Episode:  996  ->  -3.0\n",
      "Reward for Episode:  997  ->  -3.0\n",
      "Reward for Episode:  998  ->  -3.0\n",
      "Reward for Episode:  999  ->  -3.0\n",
      "Player scores for every episode:  [-1123.0, -3539.0, -103.0, -105.0, -1691.0, -203.0, -109.0, -3.0, -103.0, -107.0, -3.0, -7.0, -3.0, -3.0, -3.0, -103.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -103.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -5.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7F92B5332AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-50.67531803 -61.82532303 -13.67292818  -5.08510232]\n",
      " [-38.59807795 -21.3945753   -6.34921987 -20.98312345]\n",
      " [-22.99631584  -2.16612699  -7.09493662 -38.10861075]\n",
      " [-16.11540256 -56.15284288  -9.59806819  -0.78426824]\n",
      " [ -7.718156   -10.46050711  -8.17949976  -6.05373184]\n",
      " [-13.50750774  -3.09694658   0.         -26.02027242]\n",
      " [-14.34462438 -24.22109375 -25.1375      -0.93513363]\n",
      " [ -1.46518438  -4.56949554 -31.16999617   0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "[[0.         0.         0.         1.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.03922632 0.88232103 0.03922632 0.03922632]\n",
      " [0.01230964 0.01230964 0.01230964 0.96307109]\n",
      " [0.         0.         0.         1.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.0820125  0.0820125  0.0820125  0.7539625 ]\n",
      " [0.03922632 0.03922632 0.03922632 0.88232103]\n",
      " [0.25       0.25       0.25       0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "reward_matrix = np.ones((3,3))*-1\n",
    "reward_matrix[(2,2)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (3,3), (0,0), (2,2), reward_matrix)\n",
    "\n",
    "on_policy_qtable = MC_OnPolicy_FirstVisit_QTable(target_game, \\\n",
    "                                       alpha=0.2, epsilon=0.5, discount=0.5, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.9)\n",
    "\n",
    "agent = MCAgent(target_game, on_policy_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsLZdUCuEtV4"
   },
   "source": [
    "### MC Off-Policy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lmV4fHiiExdC",
    "outputId": "e3947994-dea7-45c6-f842-5974e87dd020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [-1. -1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7F92B3795160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for Episode:  0  ->  -621.0\n",
      "Reward for Episode:  1  ->  -6337.0\n",
      "Reward for Episode:  2  ->  -223.0\n",
      "Reward for Episode:  3  ->  -3.0\n",
      "Reward for Episode:  4  ->  -103.0\n",
      "Reward for Episode:  5  ->  -11.0\n",
      "Reward for Episode:  6  ->  -305.0\n",
      "Reward for Episode:  7  ->  -5.0\n",
      "Reward for Episode:  8  ->  -521.0\n",
      "Reward for Episode:  9  ->  -3.0\n",
      "Reward for Episode:  10  ->  -5.0\n",
      "Reward for Episode:  11  ->  -107.0\n",
      "Reward for Episode:  12  ->  -115.0\n",
      "Reward for Episode:  13  ->  -103.0\n",
      "Reward for Episode:  14  ->  -103.0\n",
      "Reward for Episode:  15  ->  -7.0\n",
      "Reward for Episode:  16  ->  -103.0\n",
      "Reward for Episode:  17  ->  -5.0\n",
      "Reward for Episode:  18  ->  -103.0\n",
      "Reward for Episode:  19  ->  -203.0\n",
      "Reward for Episode:  20  ->  -3.0\n",
      "Reward for Episode:  21  ->  -5.0\n",
      "Reward for Episode:  22  ->  -105.0\n",
      "Reward for Episode:  23  ->  -5.0\n",
      "Reward for Episode:  24  ->  -3.0\n",
      "Reward for Episode:  25  ->  -3.0\n",
      "Reward for Episode:  26  ->  -3.0\n",
      "Reward for Episode:  27  ->  -203.0\n",
      "Reward for Episode:  28  ->  -3.0\n",
      "Reward for Episode:  29  ->  -3.0\n",
      "Reward for Episode:  30  ->  -103.0\n",
      "Reward for Episode:  31  ->  -5.0\n",
      "Reward for Episode:  32  ->  -107.0\n",
      "Reward for Episode:  33  ->  -5.0\n",
      "Reward for Episode:  34  ->  -103.0\n",
      "Reward for Episode:  35  ->  -3.0\n",
      "Reward for Episode:  36  ->  -109.0\n",
      "Reward for Episode:  37  ->  -3.0\n",
      "Reward for Episode:  38  ->  -3.0\n",
      "Reward for Episode:  39  ->  -3.0\n",
      "Reward for Episode:  40  ->  -103.0\n",
      "Reward for Episode:  41  ->  -205.0\n",
      "Reward for Episode:  42  ->  -203.0\n",
      "Reward for Episode:  43  ->  -203.0\n",
      "Reward for Episode:  44  ->  -9.0\n",
      "Reward for Episode:  45  ->  -3.0\n",
      "Reward for Episode:  46  ->  -203.0\n",
      "Reward for Episode:  47  ->  -3.0\n",
      "Reward for Episode:  48  ->  -3.0\n",
      "Reward for Episode:  49  ->  -3.0\n",
      "Reward for Episode:  50  ->  -203.0\n",
      "Reward for Episode:  51  ->  -103.0\n",
      "Reward for Episode:  52  ->  -405.0\n",
      "Reward for Episode:  53  ->  -3.0\n",
      "Reward for Episode:  54  ->  -3.0\n",
      "Reward for Episode:  55  ->  -3.0\n",
      "Reward for Episode:  56  ->  -9.0\n",
      "Reward for Episode:  57  ->  -105.0\n",
      "Reward for Episode:  58  ->  -3.0\n",
      "Reward for Episode:  59  ->  -5.0\n",
      "Reward for Episode:  60  ->  -3.0\n",
      "Reward for Episode:  61  ->  -103.0\n",
      "Reward for Episode:  62  ->  -103.0\n",
      "Reward for Episode:  63  ->  -3.0\n",
      "Reward for Episode:  64  ->  -103.0\n",
      "Reward for Episode:  65  ->  -211.0\n",
      "Reward for Episode:  66  ->  -3.0\n",
      "Reward for Episode:  67  ->  -105.0\n",
      "Reward for Episode:  68  ->  -309.0\n",
      "Reward for Episode:  69  ->  -103.0\n",
      "Reward for Episode:  70  ->  -3.0\n",
      "Reward for Episode:  71  ->  -109.0\n",
      "Reward for Episode:  72  ->  -3.0\n",
      "Reward for Episode:  73  ->  -209.0\n",
      "Reward for Episode:  74  ->  -5.0\n",
      "Reward for Episode:  75  ->  -3.0\n",
      "Reward for Episode:  76  ->  -5.0\n",
      "Reward for Episode:  77  ->  -5.0\n",
      "Reward for Episode:  78  ->  -105.0\n",
      "Reward for Episode:  79  ->  -105.0\n",
      "Reward for Episode:  80  ->  -107.0\n",
      "Reward for Episode:  81  ->  -107.0\n",
      "Reward for Episode:  82  ->  -203.0\n",
      "Reward for Episode:  83  ->  -3.0\n",
      "Reward for Episode:  84  ->  -103.0\n",
      "Reward for Episode:  85  ->  -5.0\n",
      "Reward for Episode:  86  ->  -3.0\n",
      "Reward for Episode:  87  ->  -103.0\n",
      "Reward for Episode:  88  ->  -3.0\n",
      "Reward for Episode:  89  ->  -7.0\n",
      "Reward for Episode:  90  ->  -3.0\n",
      "Reward for Episode:  91  ->  -111.0\n",
      "Reward for Episode:  92  ->  -211.0\n",
      "Reward for Episode:  93  ->  -103.0\n",
      "Reward for Episode:  94  ->  -103.0\n",
      "Reward for Episode:  95  ->  -205.0\n",
      "Reward for Episode:  96  ->  -3.0\n",
      "Reward for Episode:  97  ->  -109.0\n",
      "Reward for Episode:  98  ->  -105.0\n",
      "Reward for Episode:  99  ->  -3.0\n",
      "Reward for Episode:  100  ->  -3.0\n",
      "Reward for Episode:  101  ->  -3.0\n",
      "Reward for Episode:  102  ->  -5.0\n",
      "Reward for Episode:  103  ->  -3.0\n",
      "Reward for Episode:  104  ->  -3.0\n",
      "Reward for Episode:  105  ->  -3.0\n",
      "Reward for Episode:  106  ->  -5.0\n",
      "Reward for Episode:  107  ->  -3.0\n",
      "Reward for Episode:  108  ->  -3.0\n",
      "Reward for Episode:  109  ->  -5.0\n",
      "Reward for Episode:  110  ->  -3.0\n",
      "Reward for Episode:  111  ->  -3.0\n",
      "Reward for Episode:  112  ->  -103.0\n",
      "Reward for Episode:  113  ->  -5.0\n",
      "Reward for Episode:  114  ->  -103.0\n",
      "Reward for Episode:  115  ->  -3.0\n",
      "Reward for Episode:  116  ->  -313.0\n",
      "Reward for Episode:  117  ->  -109.0\n",
      "Reward for Episode:  118  ->  -103.0\n",
      "Reward for Episode:  119  ->  -3.0\n",
      "Reward for Episode:  120  ->  -9.0\n",
      "Reward for Episode:  121  ->  -105.0\n",
      "Reward for Episode:  122  ->  -105.0\n",
      "Reward for Episode:  123  ->  -3.0\n",
      "Reward for Episode:  124  ->  -105.0\n",
      "Reward for Episode:  125  ->  -103.0\n",
      "Reward for Episode:  126  ->  -105.0\n",
      "Reward for Episode:  127  ->  -207.0\n",
      "Reward for Episode:  128  ->  -103.0\n",
      "Reward for Episode:  129  ->  -5.0\n",
      "Reward for Episode:  130  ->  -305.0\n",
      "Reward for Episode:  131  ->  -3.0\n",
      "Reward for Episode:  132  ->  -5.0\n",
      "Reward for Episode:  133  ->  -211.0\n",
      "Reward for Episode:  134  ->  -111.0\n",
      "Reward for Episode:  135  ->  -5.0\n",
      "Reward for Episode:  136  ->  -105.0\n",
      "Reward for Episode:  137  ->  -3.0\n",
      "Reward for Episode:  138  ->  -7.0\n",
      "Reward for Episode:  139  ->  -3.0\n",
      "Reward for Episode:  140  ->  -111.0\n",
      "Reward for Episode:  141  ->  -105.0\n",
      "Reward for Episode:  142  ->  -303.0\n",
      "Reward for Episode:  143  ->  -103.0\n",
      "Reward for Episode:  144  ->  -109.0\n",
      "Reward for Episode:  145  ->  -3.0\n",
      "Reward for Episode:  146  ->  -109.0\n",
      "Reward for Episode:  147  ->  -7.0\n",
      "Reward for Episode:  148  ->  -103.0\n",
      "Reward for Episode:  149  ->  -3.0\n",
      "Reward for Episode:  150  ->  -105.0\n",
      "Reward for Episode:  151  ->  -105.0\n",
      "Reward for Episode:  152  ->  -3.0\n",
      "Reward for Episode:  153  ->  -207.0\n",
      "Reward for Episode:  154  ->  -103.0\n",
      "Reward for Episode:  155  ->  -3.0\n",
      "Reward for Episode:  156  ->  -5.0\n",
      "Reward for Episode:  157  ->  -3.0\n",
      "Reward for Episode:  158  ->  -9.0\n",
      "Reward for Episode:  159  ->  -3.0\n",
      "Reward for Episode:  160  ->  -103.0\n",
      "Reward for Episode:  161  ->  -103.0\n",
      "Reward for Episode:  162  ->  -105.0\n",
      "Reward for Episode:  163  ->  -3.0\n",
      "Reward for Episode:  164  ->  -203.0\n",
      "Reward for Episode:  165  ->  -105.0\n",
      "Reward for Episode:  166  ->  -3.0\n",
      "Reward for Episode:  167  ->  -5.0\n",
      "Reward for Episode:  168  ->  -3.0\n",
      "Reward for Episode:  169  ->  -3.0\n",
      "Reward for Episode:  170  ->  -3.0\n",
      "Reward for Episode:  171  ->  -3.0\n",
      "Reward for Episode:  172  ->  -111.0\n",
      "Reward for Episode:  173  ->  -3.0\n",
      "Reward for Episode:  174  ->  -103.0\n",
      "Reward for Episode:  175  ->  -203.0\n",
      "Reward for Episode:  176  ->  -3.0\n",
      "Reward for Episode:  177  ->  -5.0\n",
      "Reward for Episode:  178  ->  -3.0\n",
      "Reward for Episode:  179  ->  -205.0\n",
      "Reward for Episode:  180  ->  -103.0\n",
      "Reward for Episode:  181  ->  -103.0\n",
      "Reward for Episode:  182  ->  -205.0\n",
      "Reward for Episode:  183  ->  -107.0\n",
      "Reward for Episode:  184  ->  -103.0\n",
      "Reward for Episode:  185  ->  -3.0\n",
      "Reward for Episode:  186  ->  -3.0\n",
      "Reward for Episode:  187  ->  -305.0\n",
      "Reward for Episode:  188  ->  -3.0\n",
      "Reward for Episode:  189  ->  -303.0\n",
      "Reward for Episode:  190  ->  -103.0\n",
      "Reward for Episode:  191  ->  -205.0\n",
      "Reward for Episode:  192  ->  -5.0\n",
      "Reward for Episode:  193  ->  -3.0\n",
      "Reward for Episode:  194  ->  -3.0\n",
      "Reward for Episode:  195  ->  -103.0\n",
      "Reward for Episode:  196  ->  -3.0\n",
      "Reward for Episode:  197  ->  -105.0\n",
      "Reward for Episode:  198  ->  -5.0\n",
      "Reward for Episode:  199  ->  -11.0\n",
      "Reward for Episode:  200  ->  -3.0\n",
      "Reward for Episode:  201  ->  -105.0\n",
      "Reward for Episode:  202  ->  -103.0\n",
      "Reward for Episode:  203  ->  -105.0\n",
      "Reward for Episode:  204  ->  -5.0\n",
      "Reward for Episode:  205  ->  -5.0\n",
      "Reward for Episode:  206  ->  -3.0\n",
      "Reward for Episode:  207  ->  -3.0\n",
      "Reward for Episode:  208  ->  -209.0\n",
      "Reward for Episode:  209  ->  -3.0\n",
      "Reward for Episode:  210  ->  -207.0\n",
      "Reward for Episode:  211  ->  -7.0\n",
      "Reward for Episode:  212  ->  -3.0\n",
      "Reward for Episode:  213  ->  -205.0\n",
      "Reward for Episode:  214  ->  -107.0\n",
      "Reward for Episode:  215  ->  -111.0\n",
      "Reward for Episode:  216  ->  -107.0\n",
      "Reward for Episode:  217  ->  -3.0\n",
      "Reward for Episode:  218  ->  -3.0\n",
      "Reward for Episode:  219  ->  -3.0\n",
      "Reward for Episode:  220  ->  -3.0\n",
      "Reward for Episode:  221  ->  -103.0\n",
      "Reward for Episode:  222  ->  -203.0\n",
      "Reward for Episode:  223  ->  -3.0\n",
      "Reward for Episode:  224  ->  -5.0\n",
      "Reward for Episode:  225  ->  -5.0\n",
      "Reward for Episode:  226  ->  -5.0\n",
      "Reward for Episode:  227  ->  -105.0\n",
      "Reward for Episode:  228  ->  -3.0\n",
      "Reward for Episode:  229  ->  -5.0\n",
      "Reward for Episode:  230  ->  -3.0\n",
      "Reward for Episode:  231  ->  -3.0\n",
      "Reward for Episode:  232  ->  -103.0\n",
      "Reward for Episode:  233  ->  -3.0\n",
      "Reward for Episode:  234  ->  -205.0\n",
      "Reward for Episode:  235  ->  -103.0\n",
      "Reward for Episode:  236  ->  -105.0\n",
      "Reward for Episode:  237  ->  -3.0\n",
      "Reward for Episode:  238  ->  -3.0\n",
      "Reward for Episode:  239  ->  -111.0\n",
      "Reward for Episode:  240  ->  -5.0\n",
      "Reward for Episode:  241  ->  -5.0\n",
      "Reward for Episode:  242  ->  -3.0\n",
      "Reward for Episode:  243  ->  -3.0\n",
      "Reward for Episode:  244  ->  -303.0\n",
      "Reward for Episode:  245  ->  -3.0\n",
      "Reward for Episode:  246  ->  -103.0\n",
      "Reward for Episode:  247  ->  -3.0\n",
      "Reward for Episode:  248  ->  -3.0\n",
      "Reward for Episode:  249  ->  -3.0\n",
      "Reward for Episode:  250  ->  -3.0\n",
      "Reward for Episode:  251  ->  -207.0\n",
      "Reward for Episode:  252  ->  -103.0\n",
      "Reward for Episode:  253  ->  -105.0\n",
      "Reward for Episode:  254  ->  -205.0\n",
      "Reward for Episode:  255  ->  -9.0\n",
      "Reward for Episode:  256  ->  -205.0\n",
      "Reward for Episode:  257  ->  -3.0\n",
      "Reward for Episode:  258  ->  -203.0\n",
      "Reward for Episode:  259  ->  -3.0\n",
      "Reward for Episode:  260  ->  -3.0\n",
      "Reward for Episode:  261  ->  -5.0\n",
      "Reward for Episode:  262  ->  -109.0\n",
      "Reward for Episode:  263  ->  -3.0\n",
      "Reward for Episode:  264  ->  -3.0\n",
      "Reward for Episode:  265  ->  -3.0\n",
      "Reward for Episode:  266  ->  -3.0\n",
      "Reward for Episode:  267  ->  -3.0\n",
      "Reward for Episode:  268  ->  -5.0\n",
      "Reward for Episode:  269  ->  -111.0\n",
      "Reward for Episode:  270  ->  -103.0\n",
      "Reward for Episode:  271  ->  -103.0\n",
      "Reward for Episode:  272  ->  -109.0\n",
      "Reward for Episode:  273  ->  -103.0\n",
      "Reward for Episode:  274  ->  -3.0\n",
      "Reward for Episode:  275  ->  -107.0\n",
      "Reward for Episode:  276  ->  -103.0\n",
      "Reward for Episode:  277  ->  -103.0\n",
      "Reward for Episode:  278  ->  -103.0\n",
      "Reward for Episode:  279  ->  -3.0\n",
      "Reward for Episode:  280  ->  -5.0\n",
      "Reward for Episode:  281  ->  -7.0\n",
      "Reward for Episode:  282  ->  -207.0\n",
      "Reward for Episode:  283  ->  -5.0\n",
      "Reward for Episode:  284  ->  -7.0\n",
      "Reward for Episode:  285  ->  -3.0\n",
      "Reward for Episode:  286  ->  -103.0\n",
      "Reward for Episode:  287  ->  -3.0\n",
      "Reward for Episode:  288  ->  -3.0\n",
      "Reward for Episode:  289  ->  -103.0\n",
      "Reward for Episode:  290  ->  -3.0\n",
      "Reward for Episode:  291  ->  -105.0\n",
      "Reward for Episode:  292  ->  -3.0\n",
      "Reward for Episode:  293  ->  -103.0\n",
      "Reward for Episode:  294  ->  -209.0\n",
      "Reward for Episode:  295  ->  -107.0\n",
      "Reward for Episode:  296  ->  -5.0\n",
      "Reward for Episode:  297  ->  -5.0\n",
      "Reward for Episode:  298  ->  -103.0\n",
      "Reward for Episode:  299  ->  -3.0\n",
      "Reward for Episode:  300  ->  -103.0\n",
      "Reward for Episode:  301  ->  -305.0\n",
      "Reward for Episode:  302  ->  -203.0\n",
      "Reward for Episode:  303  ->  -3.0\n",
      "Reward for Episode:  304  ->  -5.0\n",
      "Reward for Episode:  305  ->  -3.0\n",
      "Reward for Episode:  306  ->  -103.0\n",
      "Reward for Episode:  307  ->  -103.0\n",
      "Reward for Episode:  308  ->  -3.0\n",
      "Reward for Episode:  309  ->  -5.0\n",
      "Reward for Episode:  310  ->  -3.0\n",
      "Reward for Episode:  311  ->  -3.0\n",
      "Reward for Episode:  312  ->  -3.0\n",
      "Reward for Episode:  313  ->  -409.0\n",
      "Reward for Episode:  314  ->  -9.0\n",
      "Reward for Episode:  315  ->  -205.0\n",
      "Reward for Episode:  316  ->  -103.0\n",
      "Reward for Episode:  317  ->  -105.0\n",
      "Reward for Episode:  318  ->  -103.0\n",
      "Reward for Episode:  319  ->  -105.0\n",
      "Reward for Episode:  320  ->  -103.0\n",
      "Reward for Episode:  321  ->  -5.0\n",
      "Reward for Episode:  322  ->  -109.0\n",
      "Reward for Episode:  323  ->  -407.0\n",
      "Reward for Episode:  324  ->  -107.0\n",
      "Reward for Episode:  325  ->  -3.0\n",
      "Reward for Episode:  326  ->  -107.0\n",
      "Reward for Episode:  327  ->  -103.0\n",
      "Reward for Episode:  328  ->  -3.0\n",
      "Reward for Episode:  329  ->  -7.0\n",
      "Reward for Episode:  330  ->  -103.0\n",
      "Reward for Episode:  331  ->  -3.0\n",
      "Reward for Episode:  332  ->  -3.0\n",
      "Reward for Episode:  333  ->  -103.0\n",
      "Reward for Episode:  334  ->  -7.0\n",
      "Reward for Episode:  335  ->  -105.0\n",
      "Reward for Episode:  336  ->  -5.0\n",
      "Reward for Episode:  337  ->  -309.0\n",
      "Reward for Episode:  338  ->  -203.0\n",
      "Reward for Episode:  339  ->  -403.0\n",
      "Reward for Episode:  340  ->  -405.0\n",
      "Reward for Episode:  341  ->  -203.0\n",
      "Reward for Episode:  342  ->  -3.0\n",
      "Reward for Episode:  343  ->  -5.0\n",
      "Reward for Episode:  344  ->  -11.0\n",
      "Reward for Episode:  345  ->  -103.0\n",
      "Reward for Episode:  346  ->  -3.0\n",
      "Reward for Episode:  347  ->  -3.0\n",
      "Reward for Episode:  348  ->  -3.0\n",
      "Reward for Episode:  349  ->  -3.0\n",
      "Reward for Episode:  350  ->  -3.0\n",
      "Reward for Episode:  351  ->  -107.0\n",
      "Reward for Episode:  352  ->  -105.0\n",
      "Reward for Episode:  353  ->  -3.0\n",
      "Reward for Episode:  354  ->  -3.0\n",
      "Reward for Episode:  355  ->  -107.0\n",
      "Reward for Episode:  356  ->  -107.0\n",
      "Reward for Episode:  357  ->  -3.0\n",
      "Reward for Episode:  358  ->  -203.0\n",
      "Reward for Episode:  359  ->  -109.0\n",
      "Reward for Episode:  360  ->  -103.0\n",
      "Reward for Episode:  361  ->  -7.0\n",
      "Reward for Episode:  362  ->  -3.0\n",
      "Reward for Episode:  363  ->  -3.0\n",
      "Reward for Episode:  364  ->  -103.0\n",
      "Reward for Episode:  365  ->  -7.0\n",
      "Reward for Episode:  366  ->  -3.0\n",
      "Reward for Episode:  367  ->  -307.0\n",
      "Reward for Episode:  368  ->  -3.0\n",
      "Reward for Episode:  369  ->  -105.0\n",
      "Reward for Episode:  370  ->  -3.0\n",
      "Reward for Episode:  371  ->  -103.0\n",
      "Reward for Episode:  372  ->  -3.0\n",
      "Reward for Episode:  373  ->  -3.0\n",
      "Reward for Episode:  374  ->  -205.0\n",
      "Reward for Episode:  375  ->  -111.0\n",
      "Reward for Episode:  376  ->  -305.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for Episode:  377  ->  -311.0\n",
      "Reward for Episode:  378  ->  -3.0\n",
      "Reward for Episode:  379  ->  -109.0\n",
      "Reward for Episode:  380  ->  -5.0\n",
      "Reward for Episode:  381  ->  -3.0\n",
      "Reward for Episode:  382  ->  -103.0\n",
      "Reward for Episode:  383  ->  -103.0\n",
      "Reward for Episode:  384  ->  -5.0\n",
      "Reward for Episode:  385  ->  -3.0\n",
      "Reward for Episode:  386  ->  -3.0\n",
      "Reward for Episode:  387  ->  -105.0\n",
      "Reward for Episode:  388  ->  -103.0\n",
      "Reward for Episode:  389  ->  -105.0\n",
      "Reward for Episode:  390  ->  -5.0\n",
      "Reward for Episode:  391  ->  -3.0\n",
      "Reward for Episode:  392  ->  -209.0\n",
      "Reward for Episode:  393  ->  -105.0\n",
      "Reward for Episode:  394  ->  -3.0\n",
      "Reward for Episode:  395  ->  -103.0\n",
      "Reward for Episode:  396  ->  -103.0\n",
      "Reward for Episode:  397  ->  -207.0\n",
      "Reward for Episode:  398  ->  -7.0\n",
      "Reward for Episode:  399  ->  -105.0\n",
      "Reward for Episode:  400  ->  -103.0\n",
      "Reward for Episode:  401  ->  -5.0\n",
      "Reward for Episode:  402  ->  -205.0\n",
      "Reward for Episode:  403  ->  -3.0\n",
      "Reward for Episode:  404  ->  -3.0\n",
      "Reward for Episode:  405  ->  -9.0\n",
      "Reward for Episode:  406  ->  -3.0\n",
      "Reward for Episode:  407  ->  -3.0\n",
      "Reward for Episode:  408  ->  -205.0\n",
      "Reward for Episode:  409  ->  -205.0\n",
      "Reward for Episode:  410  ->  -3.0\n",
      "Reward for Episode:  411  ->  -5.0\n",
      "Reward for Episode:  412  ->  -3.0\n",
      "Reward for Episode:  413  ->  -5.0\n",
      "Reward for Episode:  414  ->  -209.0\n",
      "Reward for Episode:  415  ->  -3.0\n",
      "Reward for Episode:  416  ->  -3.0\n",
      "Reward for Episode:  417  ->  -3.0\n",
      "Reward for Episode:  418  ->  -5.0\n",
      "Reward for Episode:  419  ->  -3.0\n",
      "Reward for Episode:  420  ->  -3.0\n",
      "Reward for Episode:  421  ->  -305.0\n",
      "Reward for Episode:  422  ->  -3.0\n",
      "Reward for Episode:  423  ->  -103.0\n",
      "Reward for Episode:  424  ->  -5.0\n",
      "Reward for Episode:  425  ->  -103.0\n",
      "Reward for Episode:  426  ->  -303.0\n",
      "Reward for Episode:  427  ->  -107.0\n",
      "Reward for Episode:  428  ->  -103.0\n",
      "Reward for Episode:  429  ->  -5.0\n",
      "Reward for Episode:  430  ->  -203.0\n",
      "Reward for Episode:  431  ->  -3.0\n",
      "Reward for Episode:  432  ->  -3.0\n",
      "Reward for Episode:  433  ->  -5.0\n",
      "Reward for Episode:  434  ->  -205.0\n",
      "Reward for Episode:  435  ->  -3.0\n",
      "Reward for Episode:  436  ->  -103.0\n",
      "Reward for Episode:  437  ->  -103.0\n",
      "Reward for Episode:  438  ->  -3.0\n",
      "Reward for Episode:  439  ->  -105.0\n",
      "Reward for Episode:  440  ->  -211.0\n",
      "Reward for Episode:  441  ->  -203.0\n",
      "Reward for Episode:  442  ->  -7.0\n",
      "Reward for Episode:  443  ->  -103.0\n",
      "Reward for Episode:  444  ->  -205.0\n",
      "Reward for Episode:  445  ->  -109.0\n",
      "Reward for Episode:  446  ->  -305.0\n",
      "Reward for Episode:  447  ->  -105.0\n",
      "Reward for Episode:  448  ->  -203.0\n",
      "Reward for Episode:  449  ->  -3.0\n",
      "Reward for Episode:  450  ->  -3.0\n",
      "Reward for Episode:  451  ->  -5.0\n",
      "Reward for Episode:  452  ->  -3.0\n",
      "Reward for Episode:  453  ->  -3.0\n",
      "Reward for Episode:  454  ->  -5.0\n",
      "Reward for Episode:  455  ->  -5.0\n",
      "Reward for Episode:  456  ->  -307.0\n",
      "Reward for Episode:  457  ->  -3.0\n",
      "Reward for Episode:  458  ->  -3.0\n",
      "Reward for Episode:  459  ->  -103.0\n",
      "Reward for Episode:  460  ->  -3.0\n",
      "Reward for Episode:  461  ->  -203.0\n",
      "Reward for Episode:  462  ->  -3.0\n",
      "Reward for Episode:  463  ->  -109.0\n",
      "Reward for Episode:  464  ->  -105.0\n",
      "Reward for Episode:  465  ->  -5.0\n",
      "Reward for Episode:  466  ->  -103.0\n",
      "Reward for Episode:  467  ->  -3.0\n",
      "Reward for Episode:  468  ->  -3.0\n",
      "Reward for Episode:  469  ->  -3.0\n",
      "Reward for Episode:  470  ->  -3.0\n",
      "Reward for Episode:  471  ->  -103.0\n",
      "Reward for Episode:  472  ->  -5.0\n",
      "Reward for Episode:  473  ->  -409.0\n",
      "Reward for Episode:  474  ->  -203.0\n",
      "Reward for Episode:  475  ->  -3.0\n",
      "Reward for Episode:  476  ->  -5.0\n",
      "Reward for Episode:  477  ->  -105.0\n",
      "Reward for Episode:  478  ->  -5.0\n",
      "Reward for Episode:  479  ->  -203.0\n",
      "Reward for Episode:  480  ->  -103.0\n",
      "Reward for Episode:  481  ->  -507.0\n",
      "Reward for Episode:  482  ->  -3.0\n",
      "Reward for Episode:  483  ->  -109.0\n",
      "Reward for Episode:  484  ->  -5.0\n",
      "Reward for Episode:  485  ->  -105.0\n",
      "Reward for Episode:  486  ->  -203.0\n",
      "Reward for Episode:  487  ->  -207.0\n",
      "Reward for Episode:  488  ->  -103.0\n",
      "Reward for Episode:  489  ->  -109.0\n",
      "Reward for Episode:  490  ->  -5.0\n",
      "Reward for Episode:  491  ->  -103.0\n",
      "Reward for Episode:  492  ->  -103.0\n",
      "Reward for Episode:  493  ->  -3.0\n",
      "Reward for Episode:  494  ->  -205.0\n",
      "Reward for Episode:  495  ->  -107.0\n",
      "Reward for Episode:  496  ->  -3.0\n",
      "Reward for Episode:  497  ->  -303.0\n",
      "Reward for Episode:  498  ->  -103.0\n",
      "Reward for Episode:  499  ->  -203.0\n",
      "Reward for Episode:  500  ->  -107.0\n",
      "Reward for Episode:  501  ->  -103.0\n",
      "Reward for Episode:  502  ->  -5.0\n",
      "Reward for Episode:  503  ->  -103.0\n",
      "Reward for Episode:  504  ->  -103.0\n",
      "Reward for Episode:  505  ->  -3.0\n",
      "Reward for Episode:  506  ->  -3.0\n",
      "Reward for Episode:  507  ->  -3.0\n",
      "Reward for Episode:  508  ->  -3.0\n",
      "Reward for Episode:  509  ->  -3.0\n",
      "Reward for Episode:  510  ->  -211.0\n",
      "Reward for Episode:  511  ->  -3.0\n",
      "Reward for Episode:  512  ->  -105.0\n",
      "Reward for Episode:  513  ->  -205.0\n",
      "Reward for Episode:  514  ->  -5.0\n",
      "Reward for Episode:  515  ->  -103.0\n",
      "Reward for Episode:  516  ->  -311.0\n",
      "Reward for Episode:  517  ->  -105.0\n",
      "Reward for Episode:  518  ->  -103.0\n",
      "Reward for Episode:  519  ->  -3.0\n",
      "Reward for Episode:  520  ->  -103.0\n",
      "Reward for Episode:  521  ->  -203.0\n",
      "Reward for Episode:  522  ->  -5.0\n",
      "Reward for Episode:  523  ->  -103.0\n",
      "Reward for Episode:  524  ->  -3.0\n",
      "Reward for Episode:  525  ->  -203.0\n",
      "Reward for Episode:  526  ->  -205.0\n",
      "Reward for Episode:  527  ->  -3.0\n",
      "Reward for Episode:  528  ->  -103.0\n",
      "Reward for Episode:  529  ->  -203.0\n",
      "Reward for Episode:  530  ->  -209.0\n",
      "Reward for Episode:  531  ->  -3.0\n",
      "Reward for Episode:  532  ->  -3.0\n",
      "Reward for Episode:  533  ->  -203.0\n",
      "Reward for Episode:  534  ->  -5.0\n",
      "Reward for Episode:  535  ->  -3.0\n",
      "Reward for Episode:  536  ->  -105.0\n",
      "Reward for Episode:  537  ->  -211.0\n",
      "Reward for Episode:  538  ->  -3.0\n",
      "Reward for Episode:  539  ->  -103.0\n",
      "Reward for Episode:  540  ->  -105.0\n",
      "Reward for Episode:  541  ->  -3.0\n",
      "Reward for Episode:  542  ->  -3.0\n",
      "Reward for Episode:  543  ->  -5.0\n",
      "Reward for Episode:  544  ->  -7.0\n",
      "Reward for Episode:  545  ->  -5.0\n",
      "Reward for Episode:  546  ->  -103.0\n",
      "Reward for Episode:  547  ->  -209.0\n",
      "Reward for Episode:  548  ->  -5.0\n",
      "Reward for Episode:  549  ->  -203.0\n",
      "Reward for Episode:  550  ->  -3.0\n",
      "Reward for Episode:  551  ->  -3.0\n",
      "Reward for Episode:  552  ->  -5.0\n",
      "Reward for Episode:  553  ->  -3.0\n",
      "Reward for Episode:  554  ->  -7.0\n",
      "Reward for Episode:  555  ->  -413.0\n",
      "Reward for Episode:  556  ->  -205.0\n",
      "Reward for Episode:  557  ->  -105.0\n",
      "Reward for Episode:  558  ->  -3.0\n",
      "Reward for Episode:  559  ->  -103.0\n",
      "Reward for Episode:  560  ->  -203.0\n",
      "Reward for Episode:  561  ->  -3.0\n",
      "Reward for Episode:  562  ->  -205.0\n",
      "Reward for Episode:  563  ->  -3.0\n",
      "Reward for Episode:  564  ->  -109.0\n",
      "Reward for Episode:  565  ->  -5.0\n",
      "Reward for Episode:  566  ->  -5.0\n",
      "Reward for Episode:  567  ->  -5.0\n",
      "Reward for Episode:  568  ->  -207.0\n",
      "Reward for Episode:  569  ->  -7.0\n",
      "Reward for Episode:  570  ->  -103.0\n",
      "Reward for Episode:  571  ->  -5.0\n",
      "Reward for Episode:  572  ->  -7.0\n",
      "Reward for Episode:  573  ->  -3.0\n",
      "Reward for Episode:  574  ->  -107.0\n",
      "Reward for Episode:  575  ->  -103.0\n",
      "Reward for Episode:  576  ->  -3.0\n",
      "Reward for Episode:  577  ->  -105.0\n",
      "Reward for Episode:  578  ->  -103.0\n",
      "Reward for Episode:  579  ->  -107.0\n",
      "Reward for Episode:  580  ->  -3.0\n",
      "Reward for Episode:  581  ->  -205.0\n",
      "Reward for Episode:  582  ->  -107.0\n",
      "Reward for Episode:  583  ->  -3.0\n",
      "Reward for Episode:  584  ->  -309.0\n",
      "Reward for Episode:  585  ->  -105.0\n",
      "Reward for Episode:  586  ->  -205.0\n",
      "Reward for Episode:  587  ->  -303.0\n",
      "Reward for Episode:  588  ->  -3.0\n",
      "Reward for Episode:  589  ->  -5.0\n",
      "Reward for Episode:  590  ->  -103.0\n",
      "Reward for Episode:  591  ->  -3.0\n",
      "Reward for Episode:  592  ->  -205.0\n",
      "Reward for Episode:  593  ->  -105.0\n",
      "Reward for Episode:  594  ->  -3.0\n",
      "Reward for Episode:  595  ->  -3.0\n",
      "Reward for Episode:  596  ->  -3.0\n",
      "Reward for Episode:  597  ->  -3.0\n",
      "Reward for Episode:  598  ->  -3.0\n",
      "Reward for Episode:  599  ->  -117.0\n",
      "Reward for Episode:  600  ->  -103.0\n",
      "Reward for Episode:  601  ->  -3.0\n",
      "Reward for Episode:  602  ->  -813.0\n",
      "Reward for Episode:  603  ->  -103.0\n",
      "Reward for Episode:  604  ->  -103.0\n",
      "Reward for Episode:  605  ->  -3.0\n",
      "Reward for Episode:  606  ->  -205.0\n",
      "Reward for Episode:  607  ->  -7.0\n",
      "Reward for Episode:  608  ->  -3.0\n",
      "Reward for Episode:  609  ->  -3.0\n",
      "Reward for Episode:  610  ->  -9.0\n",
      "Reward for Episode:  611  ->  -5.0\n",
      "Reward for Episode:  612  ->  -3.0\n",
      "Reward for Episode:  613  ->  -103.0\n",
      "Reward for Episode:  614  ->  -3.0\n",
      "Reward for Episode:  615  ->  -3.0\n",
      "Reward for Episode:  616  ->  -107.0\n",
      "Reward for Episode:  617  ->  -3.0\n",
      "Reward for Episode:  618  ->  -5.0\n",
      "Reward for Episode:  619  ->  -105.0\n",
      "Reward for Episode:  620  ->  -103.0\n",
      "Reward for Episode:  621  ->  -207.0\n",
      "Reward for Episode:  622  ->  -3.0\n",
      "Reward for Episode:  623  ->  -7.0\n",
      "Reward for Episode:  624  ->  -303.0\n",
      "Reward for Episode:  625  ->  -7.0\n",
      "Reward for Episode:  626  ->  -3.0\n",
      "Reward for Episode:  627  ->  -5.0\n",
      "Reward for Episode:  628  ->  -3.0\n",
      "Reward for Episode:  629  ->  -5.0\n",
      "Reward for Episode:  630  ->  -15.0\n",
      "Reward for Episode:  631  ->  -3.0\n",
      "Reward for Episode:  632  ->  -3.0\n",
      "Reward for Episode:  633  ->  -5.0\n",
      "Reward for Episode:  634  ->  -3.0\n",
      "Reward for Episode:  635  ->  -209.0\n",
      "Reward for Episode:  636  ->  -207.0\n",
      "Reward for Episode:  637  ->  -103.0\n",
      "Reward for Episode:  638  ->  -3.0\n",
      "Reward for Episode:  639  ->  -103.0\n",
      "Reward for Episode:  640  ->  -103.0\n",
      "Reward for Episode:  641  ->  -103.0\n",
      "Reward for Episode:  642  ->  -3.0\n",
      "Reward for Episode:  643  ->  -5.0\n",
      "Reward for Episode:  644  ->  -3.0\n",
      "Reward for Episode:  645  ->  -203.0\n",
      "Reward for Episode:  646  ->  -207.0\n",
      "Reward for Episode:  647  ->  -103.0\n",
      "Reward for Episode:  648  ->  -3.0\n",
      "Reward for Episode:  649  ->  -103.0\n",
      "Reward for Episode:  650  ->  -3.0\n",
      "Reward for Episode:  651  ->  -103.0\n",
      "Reward for Episode:  652  ->  -7.0\n",
      "Reward for Episode:  653  ->  -3.0\n",
      "Reward for Episode:  654  ->  -5.0\n",
      "Reward for Episode:  655  ->  -3.0\n",
      "Reward for Episode:  656  ->  -3.0\n",
      "Reward for Episode:  657  ->  -203.0\n",
      "Reward for Episode:  658  ->  -3.0\n",
      "Reward for Episode:  659  ->  -103.0\n",
      "Reward for Episode:  660  ->  -3.0\n",
      "Reward for Episode:  661  ->  -3.0\n",
      "Reward for Episode:  662  ->  -3.0\n",
      "Reward for Episode:  663  ->  -103.0\n",
      "Reward for Episode:  664  ->  -3.0\n",
      "Reward for Episode:  665  ->  -3.0\n",
      "Reward for Episode:  666  ->  -3.0\n",
      "Reward for Episode:  667  ->  -3.0\n",
      "Reward for Episode:  668  ->  -3.0\n",
      "Reward for Episode:  669  ->  -3.0\n",
      "Reward for Episode:  670  ->  -3.0\n",
      "Reward for Episode:  671  ->  -105.0\n",
      "Reward for Episode:  672  ->  -5.0\n",
      "Reward for Episode:  673  ->  -3.0\n",
      "Reward for Episode:  674  ->  -205.0\n",
      "Reward for Episode:  675  ->  -407.0\n",
      "Reward for Episode:  676  ->  -3.0\n",
      "Reward for Episode:  677  ->  -7.0\n",
      "Reward for Episode:  678  ->  -103.0\n",
      "Reward for Episode:  679  ->  -107.0\n",
      "Reward for Episode:  680  ->  -5.0\n",
      "Reward for Episode:  681  ->  -109.0\n",
      "Reward for Episode:  682  ->  -305.0\n",
      "Reward for Episode:  683  ->  -3.0\n",
      "Reward for Episode:  684  ->  -3.0\n",
      "Reward for Episode:  685  ->  -5.0\n",
      "Reward for Episode:  686  ->  -103.0\n",
      "Reward for Episode:  687  ->  -107.0\n",
      "Reward for Episode:  688  ->  -103.0\n",
      "Reward for Episode:  689  ->  -203.0\n",
      "Reward for Episode:  690  ->  -107.0\n",
      "Reward for Episode:  691  ->  -103.0\n",
      "Reward for Episode:  692  ->  -207.0\n",
      "Reward for Episode:  693  ->  -5.0\n",
      "Reward for Episode:  694  ->  -105.0\n",
      "Reward for Episode:  695  ->  -3.0\n",
      "Reward for Episode:  696  ->  -5.0\n",
      "Reward for Episode:  697  ->  -203.0\n",
      "Reward for Episode:  698  ->  -5.0\n",
      "Reward for Episode:  699  ->  -7.0\n",
      "Reward for Episode:  700  ->  -5.0\n",
      "Reward for Episode:  701  ->  -3.0\n",
      "Reward for Episode:  702  ->  -103.0\n",
      "Reward for Episode:  703  ->  -103.0\n",
      "Reward for Episode:  704  ->  -3.0\n",
      "Reward for Episode:  705  ->  -3.0\n",
      "Reward for Episode:  706  ->  -103.0\n",
      "Reward for Episode:  707  ->  -3.0\n",
      "Reward for Episode:  708  ->  -103.0\n",
      "Reward for Episode:  709  ->  -3.0\n",
      "Reward for Episode:  710  ->  -107.0\n",
      "Reward for Episode:  711  ->  -7.0\n",
      "Reward for Episode:  712  ->  -7.0\n",
      "Reward for Episode:  713  ->  -203.0\n",
      "Reward for Episode:  714  ->  -5.0\n",
      "Reward for Episode:  715  ->  -105.0\n",
      "Reward for Episode:  716  ->  -3.0\n",
      "Reward for Episode:  717  ->  -203.0\n",
      "Reward for Episode:  718  ->  -7.0\n",
      "Reward for Episode:  719  ->  -5.0\n",
      "Reward for Episode:  720  ->  -3.0\n",
      "Reward for Episode:  721  ->  -103.0\n",
      "Reward for Episode:  722  ->  -107.0\n",
      "Reward for Episode:  723  ->  -3.0\n",
      "Reward for Episode:  724  ->  -5.0\n",
      "Reward for Episode:  725  ->  -207.0\n",
      "Reward for Episode:  726  ->  -3.0\n",
      "Reward for Episode:  727  ->  -3.0\n",
      "Reward for Episode:  728  ->  -5.0\n",
      "Reward for Episode:  729  ->  -3.0\n",
      "Reward for Episode:  730  ->  -107.0\n",
      "Reward for Episode:  731  ->  -103.0\n",
      "Reward for Episode:  732  ->  -5.0\n",
      "Reward for Episode:  733  ->  -105.0\n",
      "Reward for Episode:  734  ->  -107.0\n",
      "Reward for Episode:  735  ->  -3.0\n",
      "Reward for Episode:  736  ->  -9.0\n",
      "Reward for Episode:  737  ->  -3.0\n",
      "Reward for Episode:  738  ->  -3.0\n",
      "Reward for Episode:  739  ->  -11.0\n",
      "Reward for Episode:  740  ->  -3.0\n",
      "Reward for Episode:  741  ->  -105.0\n",
      "Reward for Episode:  742  ->  -107.0\n",
      "Reward for Episode:  743  ->  -3.0\n",
      "Reward for Episode:  744  ->  -105.0\n",
      "Reward for Episode:  745  ->  -3.0\n",
      "Reward for Episode:  746  ->  -103.0\n",
      "Reward for Episode:  747  ->  -403.0\n",
      "Reward for Episode:  748  ->  -105.0\n",
      "Reward for Episode:  749  ->  -5.0\n",
      "Reward for Episode:  750  ->  -405.0\n",
      "Reward for Episode:  751  ->  -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for Episode:  752  ->  -113.0\n",
      "Reward for Episode:  753  ->  -103.0\n",
      "Reward for Episode:  754  ->  -105.0\n",
      "Reward for Episode:  755  ->  -103.0\n",
      "Reward for Episode:  756  ->  -3.0\n",
      "Reward for Episode:  757  ->  -3.0\n",
      "Reward for Episode:  758  ->  -103.0\n",
      "Reward for Episode:  759  ->  -3.0\n",
      "Reward for Episode:  760  ->  -105.0\n",
      "Reward for Episode:  761  ->  -303.0\n",
      "Reward for Episode:  762  ->  -307.0\n",
      "Reward for Episode:  763  ->  -3.0\n",
      "Reward for Episode:  764  ->  -107.0\n",
      "Reward for Episode:  765  ->  -203.0\n",
      "Reward for Episode:  766  ->  -103.0\n",
      "Reward for Episode:  767  ->  -3.0\n",
      "Reward for Episode:  768  ->  -3.0\n",
      "Reward for Episode:  769  ->  -5.0\n",
      "Reward for Episode:  770  ->  -5.0\n",
      "Reward for Episode:  771  ->  -3.0\n",
      "Reward for Episode:  772  ->  -3.0\n",
      "Reward for Episode:  773  ->  -3.0\n",
      "Reward for Episode:  774  ->  -103.0\n",
      "Reward for Episode:  775  ->  -3.0\n",
      "Reward for Episode:  776  ->  -103.0\n",
      "Reward for Episode:  777  ->  -3.0\n",
      "Reward for Episode:  778  ->  -205.0\n",
      "Reward for Episode:  779  ->  -5.0\n",
      "Reward for Episode:  780  ->  -3.0\n",
      "Reward for Episode:  781  ->  -105.0\n",
      "Reward for Episode:  782  ->  -105.0\n",
      "Reward for Episode:  783  ->  -5.0\n",
      "Reward for Episode:  784  ->  -103.0\n",
      "Reward for Episode:  785  ->  -109.0\n",
      "Reward for Episode:  786  ->  -3.0\n",
      "Reward for Episode:  787  ->  -3.0\n",
      "Reward for Episode:  788  ->  -405.0\n",
      "Reward for Episode:  789  ->  -105.0\n",
      "Reward for Episode:  790  ->  -203.0\n",
      "Reward for Episode:  791  ->  -3.0\n",
      "Reward for Episode:  792  ->  -5.0\n",
      "Reward for Episode:  793  ->  -205.0\n",
      "Reward for Episode:  794  ->  -3.0\n",
      "Reward for Episode:  795  ->  -103.0\n",
      "Reward for Episode:  796  ->  -7.0\n",
      "Reward for Episode:  797  ->  -105.0\n",
      "Reward for Episode:  798  ->  -415.0\n",
      "Reward for Episode:  799  ->  -305.0\n",
      "Reward for Episode:  800  ->  -205.0\n",
      "Reward for Episode:  801  ->  -3.0\n",
      "Reward for Episode:  802  ->  -3.0\n",
      "Reward for Episode:  803  ->  -3.0\n",
      "Reward for Episode:  804  ->  -5.0\n",
      "Reward for Episode:  805  ->  -3.0\n",
      "Reward for Episode:  806  ->  -403.0\n",
      "Reward for Episode:  807  ->  -3.0\n",
      "Reward for Episode:  808  ->  -105.0\n",
      "Reward for Episode:  809  ->  -5.0\n",
      "Reward for Episode:  810  ->  -3.0\n",
      "Reward for Episode:  811  ->  -3.0\n",
      "Reward for Episode:  812  ->  -7.0\n",
      "Reward for Episode:  813  ->  -7.0\n",
      "Reward for Episode:  814  ->  -207.0\n",
      "Reward for Episode:  815  ->  -3.0\n",
      "Reward for Episode:  816  ->  -3.0\n",
      "Reward for Episode:  817  ->  -9.0\n",
      "Reward for Episode:  818  ->  -103.0\n",
      "Reward for Episode:  819  ->  -5.0\n",
      "Reward for Episode:  820  ->  -5.0\n",
      "Reward for Episode:  821  ->  -5.0\n",
      "Reward for Episode:  822  ->  -5.0\n",
      "Reward for Episode:  823  ->  -3.0\n",
      "Reward for Episode:  824  ->  -103.0\n",
      "Reward for Episode:  825  ->  -103.0\n",
      "Reward for Episode:  826  ->  -3.0\n",
      "Reward for Episode:  827  ->  -103.0\n",
      "Reward for Episode:  828  ->  -303.0\n",
      "Reward for Episode:  829  ->  -3.0\n",
      "Reward for Episode:  830  ->  -3.0\n",
      "Reward for Episode:  831  ->  -103.0\n",
      "Reward for Episode:  832  ->  -503.0\n",
      "Reward for Episode:  833  ->  -109.0\n",
      "Reward for Episode:  834  ->  -9.0\n",
      "Reward for Episode:  835  ->  -3.0\n",
      "Reward for Episode:  836  ->  -107.0\n",
      "Reward for Episode:  837  ->  -3.0\n",
      "Reward for Episode:  838  ->  -5.0\n",
      "Reward for Episode:  839  ->  -103.0\n",
      "Reward for Episode:  840  ->  -3.0\n",
      "Reward for Episode:  841  ->  -305.0\n",
      "Reward for Episode:  842  ->  -105.0\n",
      "Reward for Episode:  843  ->  -3.0\n",
      "Reward for Episode:  844  ->  -3.0\n",
      "Reward for Episode:  845  ->  -3.0\n",
      "Reward for Episode:  846  ->  -5.0\n",
      "Reward for Episode:  847  ->  -203.0\n",
      "Reward for Episode:  848  ->  -7.0\n",
      "Reward for Episode:  849  ->  -3.0\n",
      "Reward for Episode:  850  ->  -103.0\n",
      "Reward for Episode:  851  ->  -5.0\n",
      "Reward for Episode:  852  ->  -105.0\n",
      "Reward for Episode:  853  ->  -103.0\n",
      "Reward for Episode:  854  ->  -405.0\n",
      "Reward for Episode:  855  ->  -5.0\n",
      "Reward for Episode:  856  ->  -103.0\n",
      "Reward for Episode:  857  ->  -7.0\n",
      "Reward for Episode:  858  ->  -105.0\n",
      "Reward for Episode:  859  ->  -5.0\n",
      "Reward for Episode:  860  ->  -205.0\n",
      "Reward for Episode:  861  ->  -7.0\n",
      "Reward for Episode:  862  ->  -3.0\n",
      "Reward for Episode:  863  ->  -5.0\n",
      "Reward for Episode:  864  ->  -105.0\n",
      "Reward for Episode:  865  ->  -3.0\n",
      "Reward for Episode:  866  ->  -3.0\n",
      "Reward for Episode:  867  ->  -207.0\n",
      "Reward for Episode:  868  ->  -105.0\n",
      "Reward for Episode:  869  ->  -109.0\n",
      "Reward for Episode:  870  ->  -103.0\n",
      "Reward for Episode:  871  ->  -3.0\n",
      "Reward for Episode:  872  ->  -5.0\n",
      "Reward for Episode:  873  ->  -3.0\n",
      "Reward for Episode:  874  ->  -3.0\n",
      "Reward for Episode:  875  ->  -3.0\n",
      "Reward for Episode:  876  ->  -3.0\n",
      "Reward for Episode:  877  ->  -7.0\n",
      "Reward for Episode:  878  ->  -3.0\n",
      "Reward for Episode:  879  ->  -5.0\n",
      "Reward for Episode:  880  ->  -207.0\n",
      "Reward for Episode:  881  ->  -103.0\n",
      "Reward for Episode:  882  ->  -5.0\n",
      "Reward for Episode:  883  ->  -3.0\n",
      "Reward for Episode:  884  ->  -103.0\n",
      "Reward for Episode:  885  ->  -3.0\n",
      "Reward for Episode:  886  ->  -103.0\n",
      "Reward for Episode:  887  ->  -211.0\n",
      "Reward for Episode:  888  ->  -209.0\n",
      "Reward for Episode:  889  ->  -203.0\n",
      "Reward for Episode:  890  ->  -3.0\n",
      "Reward for Episode:  891  ->  -103.0\n",
      "Reward for Episode:  892  ->  -3.0\n",
      "Reward for Episode:  893  ->  -3.0\n",
      "Reward for Episode:  894  ->  -5.0\n",
      "Reward for Episode:  895  ->  -3.0\n",
      "Reward for Episode:  896  ->  -105.0\n",
      "Reward for Episode:  897  ->  -5.0\n",
      "Reward for Episode:  898  ->  -5.0\n",
      "Reward for Episode:  899  ->  -5.0\n",
      "Reward for Episode:  900  ->  -109.0\n",
      "Reward for Episode:  901  ->  -105.0\n",
      "Reward for Episode:  902  ->  -5.0\n",
      "Reward for Episode:  903  ->  -107.0\n",
      "Reward for Episode:  904  ->  -3.0\n",
      "Reward for Episode:  905  ->  -3.0\n",
      "Reward for Episode:  906  ->  -5.0\n",
      "Reward for Episode:  907  ->  -203.0\n",
      "Reward for Episode:  908  ->  -305.0\n",
      "Reward for Episode:  909  ->  -105.0\n",
      "Reward for Episode:  910  ->  -103.0\n",
      "Reward for Episode:  911  ->  -105.0\n",
      "Reward for Episode:  912  ->  -3.0\n",
      "Reward for Episode:  913  ->  -5.0\n",
      "Reward for Episode:  914  ->  -9.0\n",
      "Reward for Episode:  915  ->  -3.0\n",
      "Reward for Episode:  916  ->  -105.0\n",
      "Reward for Episode:  917  ->  -203.0\n",
      "Reward for Episode:  918  ->  -103.0\n",
      "Reward for Episode:  919  ->  -103.0\n",
      "Reward for Episode:  920  ->  -103.0\n",
      "Reward for Episode:  921  ->  -3.0\n",
      "Reward for Episode:  922  ->  -5.0\n",
      "Reward for Episode:  923  ->  -5.0\n",
      "Reward for Episode:  924  ->  -5.0\n",
      "Reward for Episode:  925  ->  -3.0\n",
      "Reward for Episode:  926  ->  -3.0\n",
      "Reward for Episode:  927  ->  -303.0\n",
      "Reward for Episode:  928  ->  -105.0\n",
      "Reward for Episode:  929  ->  -7.0\n",
      "Reward for Episode:  930  ->  -209.0\n",
      "Reward for Episode:  931  ->  -3.0\n",
      "Reward for Episode:  932  ->  -103.0\n",
      "Reward for Episode:  933  ->  -3.0\n",
      "Reward for Episode:  934  ->  -3.0\n",
      "Reward for Episode:  935  ->  -5.0\n",
      "Reward for Episode:  936  ->  -3.0\n",
      "Reward for Episode:  937  ->  -103.0\n",
      "Reward for Episode:  938  ->  -103.0\n",
      "Reward for Episode:  939  ->  -3.0\n",
      "Reward for Episode:  940  ->  -3.0\n",
      "Reward for Episode:  941  ->  -7.0\n",
      "Reward for Episode:  942  ->  -5.0\n",
      "Reward for Episode:  943  ->  -9.0\n",
      "Reward for Episode:  944  ->  -203.0\n",
      "Reward for Episode:  945  ->  -105.0\n",
      "Reward for Episode:  946  ->  -3.0\n",
      "Reward for Episode:  947  ->  -315.0\n",
      "Reward for Episode:  948  ->  -5.0\n",
      "Reward for Episode:  949  ->  -103.0\n",
      "Reward for Episode:  950  ->  -103.0\n",
      "Reward for Episode:  951  ->  -3.0\n",
      "Reward for Episode:  952  ->  -3.0\n",
      "Reward for Episode:  953  ->  -103.0\n",
      "Reward for Episode:  954  ->  -203.0\n",
      "Reward for Episode:  955  ->  -3.0\n",
      "Reward for Episode:  956  ->  -3.0\n",
      "Reward for Episode:  957  ->  -105.0\n",
      "Reward for Episode:  958  ->  -3.0\n",
      "Reward for Episode:  959  ->  -3.0\n",
      "Reward for Episode:  960  ->  -5.0\n",
      "Reward for Episode:  961  ->  -3.0\n",
      "Reward for Episode:  962  ->  -3.0\n",
      "Reward for Episode:  963  ->  -3.0\n",
      "Reward for Episode:  964  ->  -3.0\n",
      "Reward for Episode:  965  ->  -3.0\n",
      "Reward for Episode:  966  ->  -109.0\n",
      "Reward for Episode:  967  ->  -3.0\n",
      "Reward for Episode:  968  ->  -5.0\n",
      "Reward for Episode:  969  ->  -509.0\n",
      "Reward for Episode:  970  ->  -3.0\n",
      "Reward for Episode:  971  ->  -7.0\n",
      "Reward for Episode:  972  ->  -3.0\n",
      "Reward for Episode:  973  ->  -3.0\n",
      "Reward for Episode:  974  ->  -207.0\n",
      "Reward for Episode:  975  ->  -5.0\n",
      "Reward for Episode:  976  ->  -303.0\n",
      "Reward for Episode:  977  ->  -307.0\n",
      "Reward for Episode:  978  ->  -3.0\n",
      "Reward for Episode:  979  ->  -3.0\n",
      "Reward for Episode:  980  ->  -103.0\n",
      "Reward for Episode:  981  ->  -103.0\n",
      "Reward for Episode:  982  ->  -3.0\n",
      "Reward for Episode:  983  ->  -5.0\n",
      "Reward for Episode:  984  ->  -105.0\n",
      "Reward for Episode:  985  ->  -3.0\n",
      "Reward for Episode:  986  ->  -3.0\n",
      "Reward for Episode:  987  ->  -7.0\n",
      "Reward for Episode:  988  ->  -313.0\n",
      "Reward for Episode:  989  ->  -107.0\n",
      "Reward for Episode:  990  ->  -105.0\n",
      "Reward for Episode:  991  ->  -103.0\n",
      "Reward for Episode:  992  ->  -3.0\n",
      "Reward for Episode:  993  ->  -3.0\n",
      "Reward for Episode:  994  ->  -3.0\n",
      "Reward for Episode:  995  ->  -3.0\n",
      "Reward for Episode:  996  ->  -107.0\n",
      "Reward for Episode:  997  ->  -3.0\n",
      "Reward for Episode:  998  ->  -203.0\n",
      "Reward for Episode:  999  ->  -5.0\n",
      "Player scores for every episode:  [-621.0, -6337.0, -223.0, -3.0, -103.0, -11.0, -305.0, -5.0, -521.0, -3.0, -5.0, -107.0, -115.0, -103.0, -103.0, -7.0, -103.0, -5.0, -103.0, -203.0, -3.0, -5.0, -105.0, -5.0, -3.0, -3.0, -3.0, -203.0, -3.0, -3.0, -103.0, -5.0, -107.0, -5.0, -103.0, -3.0, -109.0, -3.0, -3.0, -3.0, -103.0, -205.0, -203.0, -203.0, -9.0, -3.0, -203.0, -3.0, -3.0, -3.0, -203.0, -103.0, -405.0, -3.0, -3.0, -3.0, -9.0, -105.0, -3.0, -5.0, -3.0, -103.0, -103.0, -3.0, -103.0, -211.0, -3.0, -105.0, -309.0, -103.0, -3.0, -109.0, -3.0, -209.0, -5.0, -3.0, -5.0, -5.0, -105.0, -105.0, -107.0, -107.0, -203.0, -3.0, -103.0, -5.0, -3.0, -103.0, -3.0, -7.0, -3.0, -111.0, -211.0, -103.0, -103.0, -205.0, -3.0, -109.0, -105.0, -3.0, -3.0, -3.0, -5.0, -3.0, -3.0, -3.0, -5.0, -3.0, -3.0, -5.0, -3.0, -3.0, -103.0, -5.0, -103.0, -3.0, -313.0, -109.0, -103.0, -3.0, -9.0, -105.0, -105.0, -3.0, -105.0, -103.0, -105.0, -207.0, -103.0, -5.0, -305.0, -3.0, -5.0, -211.0, -111.0, -5.0, -105.0, -3.0, -7.0, -3.0, -111.0, -105.0, -303.0, -103.0, -109.0, -3.0, -109.0, -7.0, -103.0, -3.0, -105.0, -105.0, -3.0, -207.0, -103.0, -3.0, -5.0, -3.0, -9.0, -3.0, -103.0, -103.0, -105.0, -3.0, -203.0, -105.0, -3.0, -5.0, -3.0, -3.0, -3.0, -3.0, -111.0, -3.0, -103.0, -203.0, -3.0, -5.0, -3.0, -205.0, -103.0, -103.0, -205.0, -107.0, -103.0, -3.0, -3.0, -305.0, -3.0, -303.0, -103.0, -205.0, -5.0, -3.0, -3.0, -103.0, -3.0, -105.0, -5.0, -11.0, -3.0, -105.0, -103.0, -105.0, -5.0, -5.0, -3.0, -3.0, -209.0, -3.0, -207.0, -7.0, -3.0, -205.0, -107.0, -111.0, -107.0, -3.0, -3.0, -3.0, -3.0, -103.0, -203.0, -3.0, -5.0, -5.0, -5.0, -105.0, -3.0, -5.0, -3.0, -3.0, -103.0, -3.0, -205.0, -103.0, -105.0, -3.0, -3.0, -111.0, -5.0, -5.0, -3.0, -3.0, -303.0, -3.0, -103.0, -3.0, -3.0, -3.0, -3.0, -207.0, -103.0, -105.0, -205.0, -9.0, -205.0, -3.0, -203.0, -3.0, -3.0, -5.0, -109.0, -3.0, -3.0, -3.0, -3.0, -3.0, -5.0, -111.0, -103.0, -103.0, -109.0, -103.0, -3.0, -107.0, -103.0, -103.0, -103.0, -3.0, -5.0, -7.0, -207.0, -5.0, -7.0, -3.0, -103.0, -3.0, -3.0, -103.0, -3.0, -105.0, -3.0, -103.0, -209.0, -107.0, -5.0, -5.0, -103.0, -3.0, -103.0, -305.0, -203.0, -3.0, -5.0, -3.0, -103.0, -103.0, -3.0, -5.0, -3.0, -3.0, -3.0, -409.0, -9.0, -205.0, -103.0, -105.0, -103.0, -105.0, -103.0, -5.0, -109.0, -407.0, -107.0, -3.0, -107.0, -103.0, -3.0, -7.0, -103.0, -3.0, -3.0, -103.0, -7.0, -105.0, -5.0, -309.0, -203.0, -403.0, -405.0, -203.0, -3.0, -5.0, -11.0, -103.0, -3.0, -3.0, -3.0, -3.0, -3.0, -107.0, -105.0, -3.0, -3.0, -107.0, -107.0, -3.0, -203.0, -109.0, -103.0, -7.0, -3.0, -3.0, -103.0, -7.0, -3.0, -307.0, -3.0, -105.0, -3.0, -103.0, -3.0, -3.0, -205.0, -111.0, -305.0, -311.0, -3.0, -109.0, -5.0, -3.0, -103.0, -103.0, -5.0, -3.0, -3.0, -105.0, -103.0, -105.0, -5.0, -3.0, -209.0, -105.0, -3.0, -103.0, -103.0, -207.0, -7.0, -105.0, -103.0, -5.0, -205.0, -3.0, -3.0, -9.0, -3.0, -3.0, -205.0, -205.0, -3.0, -5.0, -3.0, -5.0, -209.0, -3.0, -3.0, -3.0, -5.0, -3.0, -3.0, -305.0, -3.0, -103.0, -5.0, -103.0, -303.0, -107.0, -103.0, -5.0, -203.0, -3.0, -3.0, -5.0, -205.0, -3.0, -103.0, -103.0, -3.0, -105.0, -211.0, -203.0, -7.0, -103.0, -205.0, -109.0, -305.0, -105.0, -203.0, -3.0, -3.0, -5.0, -3.0, -3.0, -5.0, -5.0, -307.0, -3.0, -3.0, -103.0, -3.0, -203.0, -3.0, -109.0, -105.0, -5.0, -103.0, -3.0, -3.0, -3.0, -3.0, -103.0, -5.0, -409.0, -203.0, -3.0, -5.0, -105.0, -5.0, -203.0, -103.0, -507.0, -3.0, -109.0, -5.0, -105.0, -203.0, -207.0, -103.0, -109.0, -5.0, -103.0, -103.0, -3.0, -205.0, -107.0, -3.0, -303.0, -103.0, -203.0, -107.0, -103.0, -5.0, -103.0, -103.0, -3.0, -3.0, -3.0, -3.0, -3.0, -211.0, -3.0, -105.0, -205.0, -5.0, -103.0, -311.0, -105.0, -103.0, -3.0, -103.0, -203.0, -5.0, -103.0, -3.0, -203.0, -205.0, -3.0, -103.0, -203.0, -209.0, -3.0, -3.0, -203.0, -5.0, -3.0, -105.0, -211.0, -3.0, -103.0, -105.0, -3.0, -3.0, -5.0, -7.0, -5.0, -103.0, -209.0, -5.0, -203.0, -3.0, -3.0, -5.0, -3.0, -7.0, -413.0, -205.0, -105.0, -3.0, -103.0, -203.0, -3.0, -205.0, -3.0, -109.0, -5.0, -5.0, -5.0, -207.0, -7.0, -103.0, -5.0, -7.0, -3.0, -107.0, -103.0, -3.0, -105.0, -103.0, -107.0, -3.0, -205.0, -107.0, -3.0, -309.0, -105.0, -205.0, -303.0, -3.0, -5.0, -103.0, -3.0, -205.0, -105.0, -3.0, -3.0, -3.0, -3.0, -3.0, -117.0, -103.0, -3.0, -813.0, -103.0, -103.0, -3.0, -205.0, -7.0, -3.0, -3.0, -9.0, -5.0, -3.0, -103.0, -3.0, -3.0, -107.0, -3.0, -5.0, -105.0, -103.0, -207.0, -3.0, -7.0, -303.0, -7.0, -3.0, -5.0, -3.0, -5.0, -15.0, -3.0, -3.0, -5.0, -3.0, -209.0, -207.0, -103.0, -3.0, -103.0, -103.0, -103.0, -3.0, -5.0, -3.0, -203.0, -207.0, -103.0, -3.0, -103.0, -3.0, -103.0, -7.0, -3.0, -5.0, -3.0, -3.0, -203.0, -3.0, -103.0, -3.0, -3.0, -3.0, -103.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0, -105.0, -5.0, -3.0, -205.0, -407.0, -3.0, -7.0, -103.0, -107.0, -5.0, -109.0, -305.0, -3.0, -3.0, -5.0, -103.0, -107.0, -103.0, -203.0, -107.0, -103.0, -207.0, -5.0, -105.0, -3.0, -5.0, -203.0, -5.0, -7.0, -5.0, -3.0, -103.0, -103.0, -3.0, -3.0, -103.0, -3.0, -103.0, -3.0, -107.0, -7.0, -7.0, -203.0, -5.0, -105.0, -3.0, -203.0, -7.0, -5.0, -3.0, -103.0, -107.0, -3.0, -5.0, -207.0, -3.0, -3.0, -5.0, -3.0, -107.0, -103.0, -5.0, -105.0, -107.0, -3.0, -9.0, -3.0, -3.0, -11.0, -3.0, -105.0, -107.0, -3.0, -105.0, -3.0, -103.0, -403.0, -105.0, -5.0, -405.0, -3.0, -113.0, -103.0, -105.0, -103.0, -3.0, -3.0, -103.0, -3.0, -105.0, -303.0, -307.0, -3.0, -107.0, -203.0, -103.0, -3.0, -3.0, -5.0, -5.0, -3.0, -3.0, -3.0, -103.0, -3.0, -103.0, -3.0, -205.0, -5.0, -3.0, -105.0, -105.0, -5.0, -103.0, -109.0, -3.0, -3.0, -405.0, -105.0, -203.0, -3.0, -5.0, -205.0, -3.0, -103.0, -7.0, -105.0, -415.0, -305.0, -205.0, -3.0, -3.0, -3.0, -5.0, -3.0, -403.0, -3.0, -105.0, -5.0, -3.0, -3.0, -7.0, -7.0, -207.0, -3.0, -3.0, -9.0, -103.0, -5.0, -5.0, -5.0, -5.0, -3.0, -103.0, -103.0, -3.0, -103.0, -303.0, -3.0, -3.0, -103.0, -503.0, -109.0, -9.0, -3.0, -107.0, -3.0, -5.0, -103.0, -3.0, -305.0, -105.0, -3.0, -3.0, -3.0, -5.0, -203.0, -7.0, -3.0, -103.0, -5.0, -105.0, -103.0, -405.0, -5.0, -103.0, -7.0, -105.0, -5.0, -205.0, -7.0, -3.0, -5.0, -105.0, -3.0, -3.0, -207.0, -105.0, -109.0, -103.0, -3.0, -5.0, -3.0, -3.0, -3.0, -3.0, -7.0, -3.0, -5.0, -207.0, -103.0, -5.0, -3.0, -103.0, -3.0, -103.0, -211.0, -209.0, -203.0, -3.0, -103.0, -3.0, -3.0, -5.0, -3.0, -105.0, -5.0, -5.0, -5.0, -109.0, -105.0, -5.0, -107.0, -3.0, -3.0, -5.0, -203.0, -305.0, -105.0, -103.0, -105.0, -3.0, -5.0, -9.0, -3.0, -105.0, -203.0, -103.0, -103.0, -103.0, -3.0, -5.0, -5.0, -5.0, -3.0, -3.0, -303.0, -105.0, -7.0, -209.0, -3.0, -103.0, -3.0, -3.0, -5.0, -3.0, -103.0, -103.0, -3.0, -3.0, -7.0, -5.0, -9.0, -203.0, -105.0, -3.0, -315.0, -5.0, -103.0, -103.0, -3.0, -3.0, -103.0, -203.0, -3.0, -3.0, -105.0, -3.0, -3.0, -5.0, -3.0, -3.0, -3.0, -3.0, -3.0, -109.0, -3.0, -5.0, -509.0, -3.0, -7.0, -3.0, -3.0, -207.0, -5.0, -303.0, -307.0, -3.0, -3.0, -103.0, -103.0, -3.0, -5.0, -105.0, -3.0, -3.0, -7.0, -313.0, -107.0, -105.0, -103.0, -3.0, -3.0, -3.0, -3.0, -107.0, -3.0, -203.0, -5.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7F92B37B8F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-123.50163582 -101.04966646   -6.04353059   -6.19993868]\n",
      " [-105.18935929  -25.9996934    -2.23999949  -25.9604004 ]\n",
      " [-124.70776866   -2.23019722   -1.2512672  -120.05005069]\n",
      " [ -25.20769384 -124.67705178   -5.408        -1.66280992]\n",
      " [  -6.19999747   -3.56753943  -21.06731901   -1.4476832 ]\n",
      " [  -1.40839997   -2.23841601    0.         -110.24236328]\n",
      " [ -22.04       -118.29327202 -124.01634656   -2.0433204 ]\n",
      " [  -5.2134638   -21.88129579 -100.41505037    0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "[[0.1  0.1  0.7  0.1 ]\n",
      " [0.1  0.1  0.7  0.1 ]\n",
      " [0.1  0.1  0.7  0.1 ]\n",
      " [0.1  0.1  0.1  0.7 ]\n",
      " [0.1  0.1  0.1  0.7 ]\n",
      " [0.1  0.1  0.7  0.1 ]\n",
      " [0.1  0.1  0.1  0.7 ]\n",
      " [0.1  0.1  0.1  0.7 ]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "reward_matrix = np.ones((3,3))*-1\n",
    "reward_matrix[(2,2)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (3,3), (0,0), (2,2), reward_matrix)\n",
    "\n",
    "off_policy_qtable = MC_OffPolicy_QTable(target_game, \\\n",
    "                                       alpha=0.1, epsilon=0.4, discount=0.2, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=1)\n",
    "\n",
    "agent = MCAgent(target_game, off_policy_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbLv1ALLE0uT"
   },
   "source": [
    "### TD Q-Learning Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DWffvmvx1E2x",
    "outputId": "47ffd5bd-5eb0-4921-aeb6-7a08e695af6c"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.ones((5,5))*-1\n",
    "reward_matrix[(4,4)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (5,5), (0,0), (4,4), reward_matrix)\n",
    "\n",
    "qlearning_qtable = QLearningQTable(target_game, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.95, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = TDAgent(target_game, qlearning_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ioq4Dljldf91"
   },
   "source": [
    "### n-step Sarsa Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CmkTo9Yfdo8J",
    "outputId": "a4f3dfe1-8887-4e14-97d3-aca1266681d4"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.ones((5,5))*-1\n",
    "reward_matrix[(3,3)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (5,5), (0,0), (3,3), reward_matrix)\n",
    "\n",
    "nstep_sarsa_qtable = nstepSarsaQTable(target_game, n = 3, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.2, \\\n",
    "                                       alpha_decay_rate = 0.99, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = nstepTDAgent(target_game, nstep_sarsa_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhlLFxDmEPwg"
   },
   "source": [
    "### n-step Tree Backup Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "97gaaQh6ETBc",
    "outputId": "b32e6a9a-bc93-46e9-cd33-17bd5d1aa69e"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.ones((5,5))*-1\n",
    "reward_matrix[(3,3)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (5,5), (0,0), (3,3), reward_matrix)\n",
    "\n",
    "nstep_backup_qtable = nstepBackupQTable(target_game, n = 1, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.2, \\\n",
    "                                       alpha_decay_rate = 0.99, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = nstepTDAgent(target_game, nstep_backup_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQxFXtzfdQwV"
   },
   "source": [
    "### DynaQ Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "__qBbAqUdT-k",
    "outputId": "81bfc99f-c847-4d88-d0ba-0669b9c1f9cf"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.ones((5,5))*-1\n",
    "reward_matrix[(3,3)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (5,5), (0,0), (3,3), reward_matrix)\n",
    "\n",
    "dynaq_qtable = DynaQTable(target_game, n = 1, \\\n",
    "                                       alpha=0.2, epsilon=0.5, discount=0.2, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.9)\n",
    "\n",
    "agent = DynaQAgent(target_game, dynaq_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5m7G-q141qA"
   },
   "source": [
    "## Collection Game Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xgxrkL081Jcj",
    "outputId": "2774775d-994d-4b33-cafe-dbb0a460e7b7"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.ones((5,5))*-1\n",
    "reward_matrix[(2,2)] = 0\n",
    "reward_matrix[(4,4)] = 0\n",
    "\n",
    "collection_game = Collection_Game( (5,5), (0,0), [(2,2),(4,4)],reward_matrix)\n",
    "\n",
    "qlearning_qtable = QLearningQTable(collection_game, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.95, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = TDAgent(collection_game, qlearning_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppax8VGOM4HM"
   },
   "source": [
    "## FindMax Game Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqnSsONsfaw4"
   },
   "outputs": [],
   "source": [
    "board_size, start_state, reward_matrix = make_maxgame_grid((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdQx9EJCx4Fa"
   },
   "outputs": [],
   "source": [
    "max_game = FindMax_Game(board_size, start_state, reward_matrix)\n",
    "\n",
    "qtable = DynaQTable(max_game, 0.2, 0.5)\n",
    "agent = DynaQAgent(max_game, qtable, 0.5, 50, 1)\n",
    "agent.play_game(200)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(agent.QTable.matrix)\n",
    "agent.get_target_policy()\n",
    "agent.print_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFPZ_fIfd0_0"
   },
   "outputs": [],
   "source": [
    "max_game = FindMax_Game(board_size, start_state, reward_matrix)\n",
    "\n",
    "qlearning_qtable = QLearningQTable(max_game, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.95, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = TDAgent(max_game, qlearning_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjrPteYf151q"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.asarray([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19,20],[21,22,23,24,25]])\n",
    "\n",
    "max_game = FindMax_Game( (5,5), (2,0), reward_matrix)\n",
    "\n",
    "qlearning_qtable = QLearningQTable(max_game, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.95, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = TDAgent(max_game, qlearning_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-grNWlAmNAEn"
   },
   "source": [
    "## MaxPath Game Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hwqV81GNDhG"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.asarray([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19,20],[21,22,23,24,25]])\n",
    "\n",
    "max_game = MaxPath_Game( (5,5), (0,0), reward_matrix)\n",
    "\n",
    "qlearning_qtable = QLearningQTable(max_game, \\\n",
    "                                       alpha=0.2, epsilon=0.8, discount=0.95, \\\n",
    "                                       alpha_decay_rate = 0.9, epsilon_decay_rate=0.99)\n",
    "\n",
    "agent = TDAgent(max_game, qlearning_qtable)\n",
    "agent.play_game(1000)\n",
    "agent.print_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmfLO_zy454_"
   },
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EKuyI_EeodN"
   },
   "outputs": [],
   "source": [
    "class DynaQAgentOLD:\n",
    "    \"\"\"\n",
    "    This class implements a temporal difference off-policy (qlearning) agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, QTable, epsilon, policy_update_steps=50, n_planning_steps = 50, epsilon_decay_rate=1.0):\n",
    "        self.Game = Game\n",
    "        self.QTable = QTable\n",
    "        self.planning_model = np.zeros_like(self.QTable.matrix)\n",
    "        self.behavior_policy = np.ones_like(self.QTable.matrix) / len(self.Game.moves)\n",
    "        self.target_policy = None\n",
    "        self.epsilon = epsilon\n",
    "        self.policy_update_steps = policy_update_steps\n",
    "        self.n_planning_steps = n_planning_steps\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate # \\in [0.5, 1]\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the deterministic (greedy) policy from the qtable\n",
    "        \"\"\"\n",
    "        self.QTable.get_target_policy()\n",
    "        self.target_policy = self.QTable.target_policy\n",
    "    \n",
    "    def update_models(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        state - a tuple point corresponding to a square in a grid game\n",
    "        action - a tuple move corresponding to an action made in a grid game\n",
    "        reward - the reward derived from the game's reward matrix for specific action taken in specific state\n",
    "        new_state - a tuple point corresponding to the square in the grid game that your agent moved to \n",
    "            from taking the action described above in the state described above\n",
    "        This function updates the qtable by calling update_qtable and passing the necessary information about the game environment\n",
    "        \"\"\"\n",
    "        new_state_idx = self.Game.point_to_index(new_state)\n",
    "        state_idx = self.Game.point_to_index(state)\n",
    "        action_idx = self.Game.moves.index(action)\n",
    "        self.QTable.update_qtable(state_idx, action_idx, reward, new_state_idx)\n",
    "        self.planning_model[state_idx, action_idx] = reward\n",
    "    \n",
    "    def update_planning(self):\n",
    "        for n in range(self.n_planning_steps):\n",
    "            random_state_idx = random.randint(0, self.Game.width*self.Game.height)\n",
    "            random_action_idx = random.randint(0,len(self.Game.moves))\n",
    "            reward = self.planning_model[random_state_idx, random_action_idx]\n",
    "            new_state_idx = self.Game.point_to_index(tuple(np.array(self.Game.agent_pos) + np.array(self.Game.moves[random_action_idx])))\n",
    "            self.QTable.update_qtable(random_state_idx, random_action_idx, reward, new_state_idx)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.behavior_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.Game.moves[random.randint(0,len(self.Game.moves))]\n",
    "\n",
    "\n",
    "    def epsilon_decay (self):\n",
    "      self.epsilon = self.epsilon*self.epsilon_decay_rate\n",
    "\n",
    "\n",
    "    def update_behavior_policy(self):\n",
    "        \"Updates behavior table with the probability values of a given state index, must update QTable first\"\n",
    "        for state_idx in range(len(self.behavior_policy)):\n",
    "            max_action_idx = np.argmax(self.QTable.matrix[state_idx])\n",
    "        \n",
    "            for x in range(0,len(self.Game.moves)):\n",
    "                self.behavior_policy[state_idx][x] = self.epsilon/len(self.Game.moves)\n",
    "            self.behavior_policy[state_idx][max_action_idx] = 1-self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "        \n",
    "        self.update_planning()\n",
    "        self.epsilon_decay()\n",
    "        self.print_policies()\n",
    "        print(self.QTable.matrix)\n",
    "\n",
    "        \n",
    "    def update_terminal_state_returns(self):\n",
    "        #learn max lookahead for actions in the terminal state\n",
    "        terminal_state = self.Game.agent_pos\n",
    "        for iters in range(5*len(self.Game.moves)):\n",
    "            self.Game.agent_pos = terminal_state\n",
    "\n",
    "            #get possible moves going up, left, down, and right\n",
    "            new_action = self.get_action(self.Game.agent_pos)\n",
    "            new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "\n",
    "            #if move is illegal (going off the board), set reward to very bad\n",
    "            if self.Game.illegal_move(new_state):\n",
    "                reward = -10000\n",
    "                self.update_models(self.Game.agent_pos, new_action, reward, self.Game.agent_pos)\n",
    "                self.Game.update_state(tuple(new_state), True)\n",
    "            else:\n",
    "            #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                reward = self.Game.get_reward(new_state)\n",
    "                self.update_models(self.Game.agent_pos, new_action, reward, new_state)\n",
    "                self.Game.update_state(tuple(new_state), False)\n",
    "\n",
    "        self.QTable.incidence = np.zeros_like(self.QTable.matrix)\n",
    "\n",
    "    def print_policies(self):\n",
    "        print(\"Behavior Policy\")\n",
    "        print(self.behavior_policy)\n",
    "        print(\"Target Policy\")\n",
    "        print(self.target_policy)\n",
    "        print(\"Planning Model\")\n",
    "        print(self.planning_model)\n",
    "\n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "\n",
    "            t = 0\n",
    "            episode_reward = 0\n",
    "            \n",
    "            if (i % self.policy_update_steps) + 1 == self.policy_update_steps:\n",
    "                print(\"update at step: \",i)\n",
    "                self.update_behavior_policy()\n",
    "\n",
    "            #while agent is not in a terminal state\n",
    "            while not self.Game.is_episode_terminal():\n",
    "                #Take action A, get reward R, step into s'\n",
    "                #Find a that is max(s',a)\n",
    "\n",
    "                if output:\n",
    "                    print(\"Player's position\",self.Game.agent_pos)\n",
    "\n",
    "                t += 1\n",
    "\n",
    "                #get possible moves going up, left, down, and right\n",
    "                new_action = self.get_action(self.Game.agent_pos)\n",
    "                new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "\n",
    "                #if move is illegal (going off the board), set reward to very bad\n",
    "                if self.Game.illegal_move(new_state):\n",
    "                    reward = -10000\n",
    "                    self.update_models(self.Game.agent_pos, new_action, reward, self.Game.agent_pos)\n",
    "                    self.Game.update_state(tuple(new_state), True)\n",
    "                else:\n",
    "                #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                    reward = self.Game.get_reward(new_state)\n",
    "                    self.update_models(self.Game.agent_pos, new_action, reward, new_state)\n",
    "                    self.Game.update_state(tuple(new_state), False)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                \n",
    "#                 print(\"Player's possible moves: \",self.Game.get_moves(self.Game.agent_pos))\n",
    "#                 print(\"Player's chosen move\",new_state)\n",
    "#                 print(\"Player's new position\",self.Game.agent_pos)\n",
    "#                 print(\"Player's reward for this move\",reward)\n",
    "#                 print(\"Number of moves made to perform task: \",t)\n",
    "\n",
    "                # if self.Game.terminal_state is not None and self.Game.agent_pos in self.Game.terminal_state:\n",
    "                #     print(\"Player found the target square: \",self.Game.terminal_state)\n",
    "\n",
    "#                 self.Game.draw()\n",
    "                \n",
    "            self.update_terminal_state_returns()\n",
    "\n",
    "            player_scores.append(episode_reward)\n",
    "\n",
    "            print(\"Player finished task in :\",t, \" moves\")\n",
    "            self.Game.refresh_game()\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AXGVTKYejCZ"
   },
   "outputs": [],
   "source": [
    "class DynaQTableOLD:\n",
    "    \"\"\"\n",
    "    This class implements the qtable object with qlearning update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha, discount, alpha_decay_rate = 0.6):\n",
    "        self.Game = Game\n",
    "        self.matrix = np.zeros([Game.height*Game.width, len(Game.moves)])\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.target_policy = np.ones_like(self.matrix) / len(Game.moves)\n",
    "        self.alpha_decay_rate = alpha_decay_rate\n",
    "\n",
    "    def alpha_decay(self):\n",
    "        self.alpha = self.alpha*self.alpha_decay_rate\n",
    "\n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function gets the deterministic (greedy) policy derived from the qtable\n",
    "        \"\"\"\n",
    "        for state_idx in range(len(self.target_policy)):\n",
    "            max_action_idx = np.argmax(self.matrix[state_idx])\n",
    "        \n",
    "            for x in range(0,len(self.Game.moves)):\n",
    "                self.target_policy[state_idx][x] = 0\n",
    "            self.target_policy[state_idx][max_action_idx] = 1\n",
    "        \n",
    "    def get_return(self, state, action):\n",
    "        \"\"\"\n",
    "        state - some integer index corresponding to a tuple point in the grid game\n",
    "        action - some integer index corresponding to a possible action taken in the grid game\n",
    "        This function returns the expected return from the qtable for a specific action made in a specific state\n",
    "        \"\"\"\n",
    "        return self.matrix[state, action]\n",
    "\n",
    "    def max_lookahead(self, state):\n",
    "        \"\"\"\n",
    "        state - this is s', a point in the grid where are agent is going to be when taking action a in state s\n",
    "        This function finds the best action which maximizes the next move made from s' (state)\n",
    "        \"\"\"\n",
    "        aprime = np.argmax(self.matrix[state, :])\n",
    "        return self.get_return(state,aprime)\n",
    "\n",
    "    def update_qtable(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        state - a point on the grid where your agent is performing an action from (converted to index in qtable)\n",
    "        action - this is an integer corresponding to a tuple that makes the agent move up, left, down, or right and represents \n",
    "          the selected move made by the agent in the state provided above\n",
    "        (state, action) pair provided above can be plugged directly into qtable\n",
    "        reward - this is an integer corresponding to the reward for the above (state, action) tuple\n",
    "        \"\"\"\n",
    "        # action equals 0, 1, 2, 3\n",
    "        current_reward = self.matrix[state,action]\n",
    "        self.matrix[state, action] = current_reward + self.alpha * (reward + self.discount * self.max_lookahead(new_state) - current_reward)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpMrcdUGJVq8"
   },
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    \"\"\"\n",
    "    This class implements the qtable object with qlearning update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha, discount, alpha_decay_rate = 0.6):\n",
    "        self.Game = Game\n",
    "        self.matrix = np.zeros([Game.height*Game.width, len(Game.moves)])\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.target_policy = np.ones_like(self.matrix) / len(Game.moves)\n",
    "        self.incidence = np.zeros_like(self.matrix)\n",
    "        self.alpha_decay_rate = alpha_decay_rate\n",
    "\n",
    "    def alpha_decay(self):\n",
    "        self.alpha = self.alpha*self.alpha_decay_rate\n",
    "\n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function gets the deterministic (greedy) policy derived from the qtable\n",
    "        \"\"\"\n",
    "        for state_idx in range(len(self.target_policy)):\n",
    "            max_action_idx = np.argmax(self.matrix[state_idx])\n",
    "        \n",
    "            for x in range(0,len(self.Game.moves)):\n",
    "                self.target_policy[state_idx][x] = 0\n",
    "            self.target_policy[state_idx][max_action_idx] = 1\n",
    "        \n",
    "    def get_return(self, state, action):\n",
    "        \"\"\"\n",
    "        state - some integer index corresponding to a tuple point in the grid game\n",
    "        action - some integer index corresponding to a possible action taken in the grid game\n",
    "        This function returns the expected return from the qtable for a specific action made in a specific state\n",
    "        \"\"\"\n",
    "        return self.matrix[state, action]\n",
    "\n",
    "    def max_lookahead(self, state):\n",
    "        \"\"\"\n",
    "        state - this is s', a point in the grid where are agent is going to be when taking action a in state s\n",
    "        This function finds the best action which maximizes the next move made from s' (state)\n",
    "        \"\"\"\n",
    "        aprime = np.argmax(self.matrix[state, :])\n",
    "        return self.get_return(state,aprime)\n",
    "\n",
    "    def update_qtable(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        state - a point on the grid where your agent is performing an action from (converted to index in qtable)\n",
    "        action - this is an integer corresponding to a tuple that makes the agent move up, left, down, or right and represents \n",
    "          the selected move made by the agent in the state provided above\n",
    "        (state, action) pair provided above can be plugged directly into qtable\n",
    "        reward - this is an integer corresponding to the reward for the above (state, action) tuple\n",
    "        \"\"\"\n",
    "        # action equals 0, 1, 2, 3\n",
    "        if self.incidence[state,action] == 0:\n",
    "            current_reward = self.matrix[state,action]\n",
    "            self.matrix[state, action] = current_reward + self.alpha * (reward + self.discount * self.max_lookahead(new_state) - current_reward)\n",
    "            self.incidence[state,action] == 1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LL_VuwTJEPW"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    This class implements a temporal difference off-policy (qlearning) agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, QTable, epsilon, policy_update_steps=50, epsilon_decay_rate=1.0):\n",
    "        self.Game = Game\n",
    "        self.QTable = QTable\n",
    "        self.behavior_policy = np.ones_like(self.QTable.matrix) / len(self.Game.moves)\n",
    "        self.target_policy = None\n",
    "        self.epsilon = epsilon\n",
    "        self.policy_update_steps = policy_update_steps\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate # \\in [0.5, 1]\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the deterministic (greedy) policy from the qtable\n",
    "        \"\"\"\n",
    "        self.QTable.get_target_policy()\n",
    "        self.target_policy = self.QTable.target_policy\n",
    "    \n",
    "    def update_qtable(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        state - a tuple point corresponding to a square in a grid game\n",
    "        action - a tuple move corresponding to an action made in a grid game\n",
    "        reward - the reward derived from the game's reward matrix for specific action taken in specific state\n",
    "        new_state - a tuple point corresponding to the square in the grid game that your agent moved to \n",
    "            from taking the action described above in the state described above\n",
    "        This function updates the qtable by calling update_qtable and passing the necessary information about the game environment\n",
    "        \"\"\"\n",
    "        new_state_idx = self.Game.point_to_index(new_state)\n",
    "        state_idx = self.Game.point_to_index(state)\n",
    "        action_idx = self.Game.moves.index(action)\n",
    "        self.QTable.update_qtable(state_idx, action_idx, reward, new_state_idx)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.behavior_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.moves[random.randint(0,len(self.moves))]\n",
    "\n",
    "\n",
    "    def epsilon_decay (self):\n",
    "      self.epsilon = self.epsilon*self.epsilon_decay_rate\n",
    "\n",
    "\n",
    "    def update_behavior_policy(self):\n",
    "        \"Updates behavior table with the probability values of a given state index, must update QTable first\"\n",
    "        for state_idx in range(len(self.behavior_policy)):\n",
    "            max_action_idx = np.argmax(self.QTable.matrix[state_idx])\n",
    "        \n",
    "            for x in range(0,len(self.Game.moves)):\n",
    "                self.behavior_policy[state_idx][x] = self.epsilon/len(self.Game.moves)\n",
    "            self.behavior_policy[state_idx][max_action_idx] = 1-self.epsilon + self.epsilon/len(self.Game.moves)\n",
    "        self.epsilon_decay()\n",
    "        self.QTable.alpha_decay()\n",
    "        self.print_policies()\n",
    "        print(self.QTable.matrix)\n",
    "\n",
    "        \n",
    "    def update_terminal_state_returns(self):\n",
    "        #learn max lookahead for actions in the terminal state\n",
    "        terminal_state = self.Game.agent_pos\n",
    "        for iters in range(5*len(self.Game.moves)):\n",
    "            self.Game.agent_pos = terminal_state\n",
    "\n",
    "            #get possible moves going up, left, down, and right\n",
    "            new_action = self.get_action(self.Game.agent_pos)\n",
    "            new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "\n",
    "            #if move is illegal (going off the board), set reward to very bad\n",
    "            if self.Game.illegal_move(new_state):\n",
    "                reward = -10000\n",
    "                self.update_qtable(self.Game.agent_pos, new_action, reward, self.Game.agent_pos)\n",
    "                self.Game.update_state(tuple(new_state), True)\n",
    "            else:\n",
    "            #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                reward = self.Game.get_reward(new_state)\n",
    "                self.update_qtable(self.Game.agent_pos, new_action, reward, new_state)\n",
    "                self.Game.update_state(tuple(new_state), False)\n",
    "\n",
    "        self.QTable.incidence = np.zeros_like(self.QTable.matrix)\n",
    "\n",
    "    def print_policies(self):\n",
    "        print(\"Behavior Policy\")\n",
    "        print(self.behavior_policy)\n",
    "        print(\"Target Policy\")\n",
    "        print(self.target_policy)\n",
    "\n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "\n",
    "            t = 0\n",
    "            episode_reward = 0\n",
    "            \n",
    "            if (i % self.policy_update_steps) + 1 == self.policy_update_steps:\n",
    "                print(\"update at step: \",i)\n",
    "                self.update_behavior_policy()\n",
    "\n",
    "            #while agent is not in a terminal state\n",
    "            while not self.Game.is_episode_terminal():\n",
    "                #Take action A, get reward R, step into s'\n",
    "                #Find a that is max(s',a)\n",
    "\n",
    "                if output:\n",
    "                    print(\"Player's position\",self.Game.agent_pos)\n",
    "\n",
    "                t += 1\n",
    "\n",
    "                #get possible moves going up, left, down, and right\n",
    "                new_action = self.get_action(self.Game.agent_pos)\n",
    "                new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "\n",
    "                #if move is illegal (going off the board), set reward to very bad\n",
    "                if self.Game.illegal_move(new_state):\n",
    "                    reward = -10000\n",
    "                    self.update_qtable(self.Game.agent_pos, new_action, reward, self.Game.agent_pos)\n",
    "                    self.Game.update_state(tuple(new_state), True)\n",
    "                else:\n",
    "                #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                    reward = self.Game.get_reward(new_state)\n",
    "                    self.update_qtable(self.Game.agent_pos, new_action, reward, new_state)\n",
    "                    self.Game.update_state(tuple(new_state), False)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                \n",
    "#                 print(\"Player's possible moves: \",self.Game.get_moves(self.Game.agent_pos))\n",
    "#                 print(\"Player's chosen move\",new_state)\n",
    "#                 print(\"Player's new position\",self.Game.agent_pos)\n",
    "#                 print(\"Player's reward for this move\",reward)\n",
    "#                 print(\"Number of moves made to perform task: \",t)\n",
    "\n",
    "                # if self.Game.terminal_state is not None and self.Game.agent_pos in self.Game.terminal_state:\n",
    "                #     print(\"Player found the target square: \",self.Game.terminal_state)\n",
    "\n",
    "#                 self.Game.draw()\n",
    "                \n",
    "            self.update_terminal_state_returns()\n",
    "\n",
    "            player_scores.append(episode_reward)\n",
    "\n",
    "            print(\"Player finished task in :\",t, \" moves\")\n",
    "            self.Game.refresh_game()\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLIVPtvXlmR-"
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.asarray([[7,8,9], [4,5,6], [1,2,3]])\n",
    "Game = Targeting_Game((3,3), (0,0), (2,2), reward_matrix)\n",
    "QLearnTable = QLearningTable(Game, 0.1, 0.25)\n",
    "QLearningA = QLearningAgent(Game, QLearnTable)\n",
    "OPBT = OffPolicyBehaviorTable(Game, QLearningA)\n",
    "OPBT.Game.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x-X-X1KlhvA"
   },
   "outputs": [],
   "source": [
    "import operator # not using\n",
    "class OffPolicyBehaviorTable:\n",
    "\n",
    "  def __init__(self, Game, QLearningAgent):\n",
    "    self.Game = Game\n",
    "    self.QLearnA = QLearningAgent\n",
    "    self.behavior_policy_table = np.ones_like(self.QLearnA.model.matrix) / len(self.Game.moves)\n",
    "    self.target_policy = self.QLearnA.get_target_policy()\n",
    "  \n",
    "  def update(self, stateidx):\n",
    "    \"Updates behavior table with the probability values of a given state index, must update QTable first\"\n",
    "    max_action_idx = np.argmax(self.QLearnA.model.matrix[stateidx])\n",
    "    for x in range(0,len(self.behavior_policy_table[0])):\n",
    "      self.behavior_policy_table[stateidx][x] = 0\n",
    "    self.behavior_policy_table[stateidx][max_action_idx] = 1-self.QLearnA.epsilon\n",
    "\n",
    "  def getAction(self, stateidx):\n",
    "    \"Returns an action index from a state index based on the behavior table\"\n",
    "    if 1-self.QLearnA.model.epsilon > random.random():\n",
    "      actionidx = np.argmax(self.behavior_policy_table[stateidx])\n",
    "    else:\n",
    "      actionidx = random.randint(0,len(self.behavior_policy_table[0])-1)\n",
    "    point = tuple(map(operator.add, self.Game.index_to_point(actionidx), self.Game.agent_pos))\n",
    "    while self.Game.illegal_move(point):\n",
    "      actionidx = random.randint(0,len(self.behavior_policy_table[0])-1)\n",
    "      point = tuple(map(operator.add, self.Game.index_to_point(actionidx), self.Game.agent_pos))\n",
    "    return actionidx\n",
    "  \n",
    "  #Note, if we change epsilon to equal zero at the end, getAction will return the\n",
    "  #actions that should be present in the target policy, or the optimal solution to the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6mbyjGm43NC"
   },
   "outputs": [],
   "source": [
    "def play_game_human(Game, epochs, output=False): #human player can play the game with this function.\n",
    "  #player = person, or agent\n",
    "\n",
    "  player_collection_scores = []\n",
    "  player_targeting_scores = []\n",
    "\n",
    "  #number of times player plays the game is epochs.\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    #in each epoch, the player needs to complete the tasks\n",
    "    #first complete collection task\n",
    "    #then complete target task\n",
    "\n",
    "    \"\"\"\n",
    "    COLLECTION TASK\n",
    "    \"\"\"\n",
    "\n",
    "    collection_moves_made = 0\n",
    "    Game.draw()\n",
    "    while len(Game.remaining_prizes) != 0:\n",
    "      if output:\n",
    "        print(\"Number of moves made to perform collection task: \",collection_moves_made)\n",
    "        print(\"Player's position\",game.agent_pos)\n",
    "            \n",
    "\n",
    "      collection_moves_made += 1\n",
    "\n",
    "      possible_moves = game.get_moves(game.agent_pos)\n",
    "      r = input(\"Play the game by entering w s a or d\")\n",
    "      new_pos=game.agent_pos\n",
    "\n",
    "      if r == \"w\":\n",
    "        new_pos=tuple((game.agent_pos[0]-1,game.agent_pos[1]))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "      \n",
    "      if r == \"s\":\n",
    "        new_pos=tuple((game.agent_pos[0]+1,game.agent_pos[1]))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "      \n",
    "      if r == \"d\":\n",
    "        new_pos=tuple((game.agent_pos[0],game.agent_pos[1]+1))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "\n",
    "      if r == \"a\":\n",
    "        new_pos=tuple((game.agent_pos[0],game.agent_pos[1]-1))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "      \n",
    "      if output:\n",
    "        print(\"Number of moves made to perform collection task: \",collection_moves_made)\n",
    "        print(\"Player's possible moves: \",possible_moves)\n",
    "        print(\"Player's chosen move\", new_pos)\n",
    "        print(\"Player's new position\",game.agent_pos)\n",
    "\n",
    "      if Game.agent_pos in Game.remaining_prizes:\n",
    "        print(\"Removing prize: \",Game.agent_pos)\n",
    "        Game.remove_prize(Game.agent_pos)\n",
    "        print(\"Remaining prizes: \", Game.remaining_prizes)\n",
    "\n",
    "      Game.draw()\n",
    "\n",
    "    player_collection_scores.append(collection_moves_made)\n",
    "\n",
    "    print(\"Player finished collection task in :\",collection_moves_made, \" moves\")\n",
    "\n",
    "    \"\"\"\n",
    "    TARGET TASK\n",
    "    \"\"\"\n",
    "\n",
    "    Game.update_agent_pos(Game.start)\n",
    "    targeting_moves_made = 0\n",
    "    Game.draw()\n",
    "    while Game.agent_pos != Game.stop:\n",
    "      if output:\n",
    "        print(\"Number of moves made to perform targeting task: \",targeting_moves_made)\n",
    "        print(\"Player's position\",game.agent_pos)\n",
    "            \n",
    "\n",
    "      targeting_moves_made += 1\n",
    "\n",
    "      possible_moves = game.get_moves(game.agent_pos)\n",
    "      r = input(\"Play the game by entering w s a or d\")\n",
    "      new_pos=game.agent_pos\n",
    "\n",
    "      if r == \"w\":\n",
    "        new_pos=tuple((game.agent_pos[0]-1,game.agent_pos[1]))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "      \n",
    "      if r == \"s\":\n",
    "        new_pos=tuple((game.agent_pos[0]+1,game.agent_pos[1]))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "      \n",
    "      if r == \"d\":\n",
    "        new_pos=tuple((game.agent_pos[0],game.agent_pos[1]+1))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "\n",
    "      if r == \"a\":\n",
    "        new_pos=tuple((game.agent_pos[0],game.agent_pos[1]-1))\n",
    "        if new_pos in possible_moves: \n",
    "          game.update_agent_pos(new_pos)\n",
    "\n",
    "      if Game.agent_pos in Game.stop:\n",
    "        print(\"Player found the target square: \",Game.stop)\n",
    "\n",
    "      Game.draw()\n",
    "\n",
    "    player_targeting_scores.append(targeting_moves_made)\n",
    "\n",
    "    print(\"Player finished targeting task in :\",targeting_moves_made, \" moves\")\n",
    "    Game.refresh_game()\n",
    "\n",
    "  Game.refresh_game()\n",
    "  Game.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3NyX4PS17Ed"
   },
   "outputs": [],
   "source": [
    "def epsilonFunc(epsilon, epsilonEnd, epochs, epochNumber):\n",
    "  #return epsilon * 0.9647\n",
    "  return (epsilon-epsilonEnd)*((epochs - epochNumber)/epochs) + epsilonEnd"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multi-TaskCode1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
